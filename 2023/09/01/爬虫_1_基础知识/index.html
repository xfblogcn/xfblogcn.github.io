<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="xfblog">
    
    <title>
        
            爬虫_1_基础知识 |
        
        XFBLOG.CN
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/favicon.webp">
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/fontawesome.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/regular.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/solid.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/brands.min.css">
    
        
            
                
<link rel="stylesheet" href="/css/custom-1.css">

            
        
    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"XFBLOG.CN","author":"xfblog","avatar":"/images/avatar.webp","logo":"/images/logo.webp","favicon":"/images/favicon.webp"},"menu":{"home":"/ || fa-solid fa-home","JumpTo":{"children":[{"JS逆向_3_扣算法专题":"/2024/05/01/JS逆向_3_扣算法专题/"},{"JS逆向_4_补环境专题":"/2024/07/01/JS逆向_4_补环境专题/"}]},"archives":"/archives || fa-solid fa-box-archive","tags":"/tags || fa-solid fa-tags","links":"/links || fa-solid fa-link","about":"/about || fa-solid fa-circle-info"},"first_screen":{"enable":true,"background_img":"/images/home_lights.webp","background_img_dark":"/images/home_dark.webp","description":null,"hitokoto":false},"social_contact":{"enable":true,"links":{"github":"https://github.com/xfblogcn","weixin":null,"qq":null,"weibo":null,"zhihu":null,"twitter":null,"x":null,"facebook":null,"email":null}},"scroll":{"progress_bar":false,"percent":true,"hide_header":true},"home":{"announcement":"版权声明：本站所有文章均为 xfblog.cn 小傅原创，请遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。","category":true,"tag":true,"post_datetime":"created","post_datetime_format":"YYYY-MM-DD"},"post":{"author_badge":{"enable":true,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":true,"min2read":true},"datetime_format":"YYYY-MM-DD","copyright_info":true,"share":true,"reward":{"enable":true,"img_link":"/images/zfb.webp","text":"创作不易，打赏随意，您的支持是我更新的动力💪！"}},"code_block":{"tools":{"enable":true,"style":"default"},"highlight_theme":"obsidian"},"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true,"layout":"right"},"website_count":{"busuanzi_count":{"enable":true,"site_uv":false,"site_pv":false,"page_pv":true}},"local_search":{"enable":true,"preload":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.36"},"waline":{"server_url":null,"reaction":false,"version":"3.2.1"},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":true},"lazyload":{"enable":true},"cdn":{"enable":true,"provider":"jsdelivr"},"pjax":{"enable":true},"footer":{"since":2021,"word_count":false,"site_deploy":{"enable":false,"provider":"github","url":null},"record":{"enable":false,"list":[{"code":"蜀ICP备2021021033号-1","link":"https://beian.miit.gov.cn"},{"code":"川公网安备 51138102000134号","link":"http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=51138102000134"}]}},"inject":{"enable":true,"css":["/css/custom-1.css"],"js":["/js/run_time.js","/js/code_click_copy.js","/js/copyrightpro.js"]},"root":"","source_data":{},"version":"4.2.5"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Keep" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>



<main class="page-container border-box">
    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left flex-start border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.webp">
                </a>
            
            <a class="site-name border-box" href="/">
               XFBLOG.CN
            </a>
        </div>

        <div class="right border-box">
            <div class="pc border-box">
                <ul class="menu-list border-box">
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-home"></i>
                                
                                首页
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box has-sub-menu">
                            <a class="menu-text-color border-box" href="javascript:void(0);">
                                
                                JUMPTO
                                
                                    <i class="menu-text-color collapse-icon fa-solid fa-angle-down"></i>
                                
                            </a>
                            
                                <ul class="sub-menu-list border-box">
                                    
                                        
                                        <li class="sub-menu-item border-box ">
                                            <a class="menu-text-color border-box flex-start" href="/2024/05/01/JS%E9%80%86%E5%90%91_3_%E6%89%A3%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98/">
                                                
                                                JS逆向_3_扣算法专题
                                            </a>
                                        </li>
                                    
                                        
                                        <li class="sub-menu-item border-box ">
                                            <a class="menu-text-color border-box flex-start" href="/2024/07/01/JS%E9%80%86%E5%90%91_4_%E8%A1%A5%E7%8E%AF%E5%A2%83%E4%B8%93%E9%A2%98/">
                                                
                                                JS逆向_4_补环境专题
                                            </a>
                                        </li>
                                    
                                </ul>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/archives">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                
                                归档
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/tags">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-tags"></i>
                                
                                标签
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/links">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-link"></i>
                                
                                友链
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/about">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-circle-info"></i>
                                
                                关于
                                
                            </a>
                            
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="menu-text-color fas search fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile border-box flex-start">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list border-box">
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-home"></i>
                                </span>
                            
                            首页
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box has-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="javascript:void(0);">
                            
                            JUMPTO
                        </a>
                        
                            <i class="right-side collapse-icon fa-solid fa-angle-left"></i>
                        
                    </label>
                    
                        <ul class="drawer-sub-menu-list border-box">
                            
                                
                                <li class="sub-menu-item border-box">
                                    <a class="drawer-menu-text-color border-box flex-start" href="/2024/05/01/js%E9%80%86%E5%90%91_3_%E6%89%A3%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98/">
                                        
                                        JS逆向_3_扣算法专题
                                    </a>
                                </li>
                            
                                
                                <li class="sub-menu-item border-box">
                                    <a class="drawer-menu-text-color border-box flex-start" href="/2024/07/01/js%E9%80%86%E5%90%91_4_%E8%A1%A5%E7%8E%AF%E5%A2%83%E4%B8%93%E9%A2%98/">
                                        
                                        JS逆向_4_补环境专题
                                    </a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/archives">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                </span>
                            
                            归档
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/tags">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-tags"></i>
                                </span>
                            
                            标签
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/links">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-link"></i>
                                </span>
                            
                            友链
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/about">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-circle-info"></i>
                                </span>
                            
                            关于
                        </a>
                        
                    </label>
                    
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        爬虫_1_基础知识
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.webp">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">xfblog</span>
                                
                                    <span class="author-badge">Lv3</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-calendar-plus"></i>&nbsp;
                <span class="datetime">2023-09-01</span>
            </span>

            
                <span class="meta-info-item post-update-date">
                    <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                    <span class="datetime" data-updated="Mon Dec 09 2024 10:26:09 GMT+0800">2024-12-09</span>
                </span>
            
        

        
            <span class="meta-info-item post-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="post-category-ul">
                    
                            <li class="category-item"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="post-tag meta-info-item border-box">
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Python/">Python</a></li>
                        
                    
                </ul>
            </span>
        

        
        
            <span class="meta-info-item post-wordcount">
                <i class="icon fas fa-file-word"></i>&nbsp;<span>25.3k 字</span>
            </span>
        
        
            <span class="meta-info-item post-min2read">
                <i class="icon fas fa-clock"></i>&nbsp;<span>111 分钟</span>
            </span>
        
        
            <span class="meta-info-item post-pv">
                <i class="icon fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
            </span>
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body ">
                    

                    
                         <h2 id="一、计算机网络"><a href="#一、计算机网络" class="headerlink" title="一、计算机网络"></a>一、计算机网络</h2><h3 id="1-B-S和C-S架构"><a href="#1-B-S和C-S架构" class="headerlink" title="1. B&#x2F;S和C&#x2F;S架构"></a>1. B&#x2F;S和C&#x2F;S架构</h3><ul>
<li><strong>B&#x2F;S架构（Browser&#x2F;Server）：浏览器&#x2F;服务器模式，是一种基于浏览器的客户端和服务器端的计算机架构</strong><ul>
<li>在B&#x2F;S架构中，浏览器作为客户端，负责处理用户交互和呈现数据，而服务器负责数据存储和处理。浏览器通过网络连接到服务器并请求数据，服务器将数据返回给浏览器，浏览器再将数据呈现给用户。</li>
</ul>
</li>
<li><strong>C&#x2F;S架构（Client&#x2F;Server）：客户端&#x2F;服务器模式，是一种常见的计算机系统架构，用于构建应用程序</strong><ul>
<li>在C&#x2F;S架构中，客户端和服务器端之间的通信是直接的，而不是通过浏览器中转。客户端通常是一款专门的软件应用程序，它向服务器请求数据，并将处理结果返回给用户</li>
</ul>
</li>
<li><strong>注意：B&#x2F;S架构也是C&#x2F;S架构的一种。</strong>当我们使用网络浏览器访问一个网站时，实际上是向该网站对应的服务器发出了请求，服务器响应后把网站的内容发送给浏览器，浏览器再将内容呈现给我们</li>
</ul>
<h3 id="2-OSI模型"><a href="#2-OSI模型" class="headerlink" title="2. OSI模型"></a>2. OSI模型</h3><ul>
<li>简介：<strong>OSI七层模型是计算机网络中的一种通信体系结构，它将计算机网络通信的过程划分为七个抽象层次</strong>。每一层都负责特定的任务，并与其它层进行交互，从而实现完整的数据通信<ol>
<li><strong>应用层</strong>：提供特定于应用程序的协议和服务，如HTTP、SMTP和FTP等</li>
<li><strong>表示层</strong>：将应用层的数据格式转换为网络标准格式，并提供加密和解密等数据转换服务</li>
<li><strong>会话层</strong>：建立、管理和终止会话，为应用程序提供端到端的连接</li>
<li><strong>传输层</strong>：在端到端的通信中负责数据的可靠传输，提供差错恢复和流量控制</li>
<li><strong>网络层</strong>：对数据包进行路由选择和转发，实现端到端的传输</li>
<li><strong>数据链路层</strong>：将比特流组装成数据帧，并提供错误检测和校验</li>
<li><strong>物理层</strong>：传输比特流，提供介质、机械和电气特性</li>
</ol>
</li>
<li><strong>TCP&#x2F;IP协议栈（Internet协议栈）：将会话层和表示层合并到了应用层，这是因为在实际应用中，会话层和表示层的功能已经被一些应用层协议所替代</strong><ol>
<li><strong>应用层</strong>：HTTP、FTP、SMTP、POP3、IMAP</li>
<li><strong>传输层</strong>：TCP、UDP</li>
<li><strong>网络层</strong>：IP、ICMP、IGMP</li>
<li><strong>数据链路层</strong>：ARP、RARP</li>
<li><strong>物理层</strong>：IEEE 802.3、EIA&#x2F;TIA-232、EIA&#x2F;TIA-449、V.35</li>
</ol>
</li>
</ul>
<img    
                       lazyload
                       alt="image"
                       data-src="https://xfblog.cn/images/1676623298-334a74bcfb5e64b.jpg"
                         style="width:80%;margin:0 auto;"
                 >

<h3 id="3-TCP-IP协议"><a href="#3-TCP-IP协议" class="headerlink" title="3. TCP&#x2F;IP协议"></a>3. TCP&#x2F;IP协议</h3><ul>
<li>简介：<strong>TCP&#x2F;IP是一组通信协议，其由多种协议组成，共同构成因特网的核心协议栈。通过TCP&#x2F;IP协议，不同类型的计算机和设备可以在全球范围内相互通信，并确保可靠和正确的数据传输</strong></li>
<li><strong>基本组成部分：</strong><ol>
<li><strong>IP协议</strong>：是TCP&#x2F;IP协议的基础，它用于在互联网上传输数据包。IP协议将每个数据包（也称为数据报）从源计算机传送到目标计算机，它使用IP地址来标识源和目标计算机</li>
<li><strong>TCP协议</strong>：是一个面向连接的协议，用于在计算机之间可靠地传输数据。TCP协议建立一个连接，使发送方和接收方能够以正确的顺序接收数据。如果某个数据包在传输过程中丢失，TCP会重新发送该数据包，以确保数据的完整性。此外，TCP还可以实现流量控制和拥塞控制，以避免网络拥塞和数据丢失</li>
<li><strong>UDP协议</strong>：是另一种在计算机之间传输数据的协议。与TCP协议不同，UDP协议不会建立连接或执行流量控制，因此传输速度更快。UDP协议适用于需要快速传输数据，但不要求数据完整性和可靠性的应用程序</li>
<li><strong>HTTP协议</strong>：是在互联网上传输超文本的协议。HTTP协议使用TCP协议作为其底层传输协议，它定义了客户端和服务器之间的通信格式。在使用HTTP协议时，客户端会向服务器发送请求，服务器会响应这些请求并返回请求的数据。HTTP协议支持多种不同的请求类型，如GET、POST、PUT和DELETE</li>
<li><strong>DNS协议</strong>：用于将域名转换为IP地址。DNS服务器负责维护域名和IP地址之间的映射，当您访问一个域名时，计算机会向DNS服务器发出请求，以获取与该域名对应的IP地址。然后计算机可以使用IP地址来连接该域名对应的服务器并获取网页或其他数据</li>
</ol>
</li>
</ul>
<h3 id="4-Socket编程"><a href="#4-Socket编程" class="headerlink" title="4. Socket编程"></a>4. Socket编程</h3><ul>
<li><p>简介：<strong>Socket是一种在计算机网络中用于通信的抽象概念，它充当了应用层与TCP&#x2F;IP协议族之间的中间层，使应用程序可以使用这些协议进行通信，而无需了解底层的网络细节，这种实现网络通信的方式被称为套接字（Socket）技术</strong></p>
</li>
<li><p><strong>套接字技术</strong>：套接字是一种在网络上运行的软件模块，可以使不同计算机之间的程序相互通信。它是一种接口，用于在应用层和传输层之间传递数据，通常被称为<strong>应用程序编程接口（API）</strong></p>
</li>
<li><p><strong>套接字应用</strong>：通过套接字技术，程序可以使用标准的TCP&#x2F;IP协议栈与其他程序进行通信。套接字提供了一个通信端点，可以通过一个网络协议和一个网络地址唯一的标识，通信的两端都需要创建一个套接字，并使用同一个网络协议和网络地址</p>
</li>
<li><p><strong>Socket编程（指定）</strong>：</p>
<ol>
<li><strong>IP地址</strong>：用于唯一标识一个计算机的地址</li>
<li><strong>端口号</strong>：用于标识一个应用程序的地址，一个端口号唯一对应一个应用程序</li>
</ol>
</li>
<li><p><strong>Socket编程（流程）</strong>：</p>
<ol>
<li>服务器端创建一个Socket，并绑定到一个IP地址和端口上</li>
<li>服务器端开始监听连接请求</li>
<li>客户端创建一个Socket，并连接到服务器端的IP地址和端口上</li>
<li>服务器端接受客户端的连接请求，并返回一个新的Socket对象，用于与客户端通信</li>
<li>服务器端和客户端可以通过新的Socket对象进行双向通信</li>
</ol>
</li>
</ul>
<img    
                       lazyload
                       alt="image"
                       data-src="https://xfblog.cn/images/1698062898-31d16a48b844a8e.jpeg"
                         style="width:80%;margin:0 auto;"
                 >

<ul>
<li><p><strong>代码实现（服务端）</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># socket服务端</span><br><span class="line"></span><br><span class="line">import socket</span><br><span class="line">import threading</span><br><span class="line"></span><br><span class="line">server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line"># 端口0-65535，其中0-1024给操作系统用</span><br><span class="line">server.bind((&#x27;127.0.0.1&#x27;, 8000))  # 绑定到127.0.0.1:8000端口上</span><br><span class="line">server.listen()  # 监听，最大挂起的链接数</span><br><span class="line"></span><br><span class="line">def handl_sock(conn, addr):</span><br><span class="line">    # 循环以完成和客户端的交互</span><br><span class="line">    while True:</span><br><span class="line">        tmp_data = conn.recv(1024)  # 单位：bytes，表示单次最大接受1024个bytes</span><br><span class="line">        print(&#x27;client_data：&#123;&#125;&#x27;.format(tmp_data.decode(&#x27;utf-8&#x27;)))</span><br><span class="line">        # 响应模版，即服务端发送给客户端的响应头</span><br><span class="line">        response_template = &#x27;&#x27;&#x27;HTTP/1.0 200 OK  </span><br><span class="line">Content-type: text/html  </span><br><span class="line">Set-Cookie: name=bobby</span><br><span class="line">Set-Cookie: course_id=78</span><br><span class="line">Set-Cookie: sessionid=abc123; Expires=Wed, 09 Jun 2023 10:18:14 GMT</span><br><span class="line"></span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">        # 服务端即将发送给客服端的消息，输入什么在客户端就显示什么</span><br><span class="line">        input_data = input()</span><br><span class="line">        # 服务端将数据包装在响应模板中，发送给客户端</span><br><span class="line">        conn.send(response_template.format(input_data).encode(&#x27;utf-8&#x27;))</span><br><span class="line">        # 如果是使用浏览器访问端口，则需要关闭链接，并退出循环；如果是自己的socket客户端，为了实现交互，则不需要关闭链接</span><br><span class="line">        conn.close()</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line"># 获取客户端连接并启动线程去处理</span><br><span class="line">while True:</span><br><span class="line">    # 阻塞等待连接，建立一个独立的通道，得到通道参数和地址</span><br><span class="line">    conn, client_addr = server.accept()</span><br><span class="line">    # 启动一个线程去处理新的用户连接</span><br><span class="line">    client_thread = threading.Thread(target=handl_sock, args=(conn, client_addr))</span><br><span class="line">    client_thread.start()</span><br><span class="line"></span><br><span class="line"># conn.close()</span><br><span class="line"># server.close()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>代码实现（客户端）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python3</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># socket客户端</span><br><span class="line"></span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line">client = socket.socket()</span><br><span class="line">client.connect((&#x27;127.0.0.1&#x27;, 8099))</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    input_data = input()</span><br><span class="line">    client.send(input_data.encode(&#x27;utf-8&#x27;))  # 不能直接发字符串，因为物理层是二进制流</span><br><span class="line">    server_data = client.recv(1024)</span><br><span class="line">    print(&#x27;server_data：&#123;&#125;&#x27;.format(server_data.decode(&#x27;utf-8&#x27;)))</span><br><span class="line"></span><br><span class="line"># client.close()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="5-HTTP协议"><a href="#5-HTTP协议" class="headerlink" title="5. HTTP协议"></a>5. HTTP协议</h3><ul>
<li>简介：<strong>HTTP（超文本传输协议）是一种应用层协议，主要用于Web浏览器和Web服务器之间的通信，是Web通信的基础</strong>。HTTP协议基于<strong>请求-响应模型</strong>，客户端发送HTTP请求，服务器端返回HTTP响应</li>
<li><strong>http和https的区别：</strong><ul>
<li><strong>端口：</strong>http的默认端口号为80，https的默认端口号为443</li>
<li><strong>安全：</strong>http不提供数据加密，因此数据以明文形式传输；而https使用TLS&#x2F;SSL加密协议来保护数据传输的安全性，使得数据在传输过程中难以被截取或窃听</li>
</ul>
</li>
<li><strong>HTTP协议特点：</strong><ol>
<li><strong>无状态协议</strong>：即每次请求和响应之间没有关联，服务器不会记录客户端的状态，每个请求都是独立的，它不知道请求和响应之间是否存在先后关系，也不知道它们是否来自同一个客户端</li>
<li><strong>请求方法</strong>：即常用的有GET、POST、PUT、DELETE等，它们分别用于获取资源、提交数据、更新资源和删除资源等操作</li>
<li><strong>缓存机制</strong>：即客户端可以缓存服务器返回的资源，避免重复请求，减轻服务器的负担</li>
<li><strong>传输方式</strong>：HTTP协议支持多种传输方式，包括TCP和UDP等，但TCP是最常用的传输方式</li>
</ol>
</li>
</ul>
<img    
                       lazyload
                       alt="image"
                       data-src="https://xfblog.cn/images/1676642514-e89120c0c6e0ba6.jpg"
                         style="width:80%;margin:0 auto;"
                 >

<ul>
<li><p><strong>Keep-Alive字段：HTTP Keep-Alive是一种HTTP协议的扩展，它允许一个单独的TCP连接在处理多个HTTP请求和响应时保持打开状态</strong>。在HTTP&#x2F;1.0中，每个HTTP请求和响应都需要打开一个新的TCP连接，这导致了额外的网络开销，降低了性能。而在HTTP&#x2F;1.1中，Keep-Alive成为了默认行为</p>
</li>
<li><p><strong>Keep-Alive作用：</strong>通过Keep-Alive字段，浏览器和Web服务器之间的TCP连接可以重用，以减少创建新连接的开销，提高了性能</p>
</li>
<li><p><strong>Keep-Alive原理：</strong></p>
<ul>
<li>当客户端请求到达服务器时，服务器告诉客户端是否支持Keep-Alive，如果支持，则在响应中添加一个Keep-Alive头，并在其后面指定超时时间。客户端可以利用此头保持连接打开，并在请求中添加一个Connection头，以告知服务器它的请求结束了</li>
<li>如果服务器没有在响应头中添加Keep-Alive头，客户端会自动关闭连接。默认情况下，服务器会在发送完响应后立即关闭连接。如果服务器发送Content-Length头，则可以不添加Keep-Alive头</li>
</ul>
</li>
<li><p><strong>模拟简单的HTTP服务器：创建一个Socket服务器，用于接收连接并返回HTTP响应，使得在客户端（浏览器）向Socket服务器发送HTTP请求后，Socket服务器可以返回正确的HTTP响应</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import socket</span><br><span class="line"></span><br><span class="line">HOST = &#x27;127.0.0.1&#x27;  # 监听的 IP 地址</span><br><span class="line">PORT = 8000  # 监听的端口号</span><br><span class="line"></span><br><span class="line"># 处理客户端请求并返回响应</span><br><span class="line">def handle_request(client):</span><br><span class="line">    # 1. 接收客户端请求数据</span><br><span class="line">    request_headers = client.recv(1024).decode()</span><br><span class="line">    print(request_headers)  # 打印请求头</span><br><span class="line">    # 2. 构造 HTTP 响应报文</span><br><span class="line">    response = &#x27;HTTP/1.1 200 OK\nContent-Type: text/html\n\nHello, World!&#x27;</span><br><span class="line">    # 3. 发送 HTTP 响应报文到客户端</span><br><span class="line">    client.sendall(response.encode(&#x27;utf-8&#x27;))</span><br><span class="line">    # 4. 最后关闭客户端连接</span><br><span class="line">    client.close()</span><br><span class="line"></span><br><span class="line"># 开启http服务器</span><br><span class="line">def start_server():</span><br><span class="line">    # 1. 创建一个套接字对象，使用 IPv4 和 TCP 协议</span><br><span class="line">    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    #  2. 允许地址重用</span><br><span class="line">    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)</span><br><span class="line">    # 3. 将套接字对象绑定到指定的地址和端口</span><br><span class="line">    sock.bind((HOST, PORT))</span><br><span class="line">    # 4. 监听来自客户端的连接请求</span><br><span class="line">    sock.listen(5)</span><br><span class="line">    print(&#x27;Listening at&#x27;, sock.getsockname())</span><br><span class="line"></span><br><span class="line">    # 5. 使用死循环模拟http的Keep-Alive功能，使服务器能够保持长连接并接受多个客户端请求</span><br><span class="line">    while True:</span><br><span class="line">        # 接受到客户端连接请求，返回一个客户端套接字对象和对应的客户端地址信息</span><br><span class="line">        client, address = sock.accept()</span><br><span class="line">        print(&#x27;Accepted connection from&#x27;, address)</span><br><span class="line"></span><br><span class="line">        # 处理客户端请求，通过client套接字对象与客户端进行数据交互</span><br><span class="line">        handle_request(client)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    start_server()</span><br></pre></td></tr></table></figure>

<h2 id="二、了解爬虫"><a href="#二、了解爬虫" class="headerlink" title="二、了解爬虫"></a>二、了解爬虫</h2><blockquote>
<p><strong>网络爬虫按照系统结构和实现技术，大致可以分为：</strong></p>
<ul>
<li><strong>通用网络爬虫（General Purpose Web Crawler）</strong></li>
<li><strong>聚焦网络爬虫（Focused Web Crawler）</strong></li>
<li><strong>增量式网络爬虫（Incremental Web Crawler）</strong></li>
<li><strong>深层网络爬虫（Deep Web Crawler）</strong></li>
</ul>
</blockquote>
<h3 id="1-请求头与响应头"><a href="#1-请求头与响应头" class="headerlink" title="1. 请求头与响应头"></a>1. 请求头与响应头</h3><ul>
<li><p><strong>请求头（Request Headers）：</strong>包含客户端向服务器发送的信息，以指示它们的需求和特定的请求参数</p>
<ul>
<li><p><strong>Host：</strong> 指示请求的主机名（域名），用于服务器根据不同主机名分配请求</p>
</li>
<li><p><strong>Accept：</strong> 指示客户端所能接受的响应媒体类型，例如文本&#x2F;html、图像、json等</p>
</li>
<li><p><strong>Accept-Encoding：</strong>指示客户端可以接受的内容编码方式，通常用于通知服务器客户端支持的压缩算法</p>
</li>
<li><p><strong>Authorization：</strong> 包含身份验证凭证，用于验证客户端的身份</p>
</li>
<li><p><strong>Cache-Control：</strong> 控制缓存行为，以便客户端和服务器可以更好地管理缓存</p>
</li>
<li><p><strong>Cookie：</strong> 包含客户端存储的会话信息，用于维持状态和跟踪用户</p>
</li>
<li><p><strong>Content-Type：</strong> 仅出现在POST请求中，指定请求正文的媒体类型</p>
</li>
<li><p><strong>Referer：</strong> 表示当前请求是通过哪个URL引发的，通常用于跟踪来源</p>
</li>
<li><p><strong>Upgrade-Insecure-Requests：</strong>用于指示服务器在可能的情况下将HTTP请求升级为HTTPS请求</p>
</li>
<li><p><strong>User-Agent：</strong> 表示客户端的类型和版本，服务器可以使用它来适应不同的客户端</p>
</li>
<li><p><code>X-Requested-With: XMLHttpRequest</code>：自定义参数，表示请求是通过JS使用XMLHttpRequest对象发起的（Ajax异步请求）</p>
</li>
</ul>
</li>
<li><p><strong>响应头（Response Headers）：</strong>包含服务器对客户端请求的响应信息，以告知客户端有关响应的元数据</p>
<ul>
<li><strong>Content-Type：</strong> 指示响应的媒体类型，如文本&#x2F;HTML、图像、JSON等</li>
<li><strong>Content-Length：</strong> 指示响应正文的长度，以便客户端可以正确接收和处理数据</li>
<li><strong>Location：</strong> 通常在重定向响应中使用，指示客户端跳转到另一个URL</li>
<li><strong>Set-Cookie：</strong> 用于在响应中设置新的Cookie，以便在客户端上维持状态和会话</li>
<li><strong>Cache-Control：</strong> 控制缓存行为，以便客户端和中间代理可以更好地管理缓存</li>
<li><strong>Server：</strong> 包含服务器的信息，用于告诉客户端服务器的类型和版本</li>
<li><strong>HTTP状态码：</strong> 表示响应的状态，如200成功、403无请求权限、404找不到页面等</li>
<li><strong>Date：</strong> 表示响应生成的日期和时间</li>
</ul>
</li>
</ul>
<h3 id="2-Robots协议"><a href="#2-Robots协议" class="headerlink" title="2. Robots协议"></a>2. Robots协议</h3><blockquote>
<p><strong>robots协议</strong>：也叫君子协议，是一种存放于网站根目录下的ASCII编码的文本文件，通常<strong>用来告诉网络搜索引擎的漫游器（网络蜘蛛）此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的</strong></p>
<ul>
<li>因为一些系统中的URL是大小写敏感的，所以<strong>robots.txt的文件名应统一为小写</strong></li>
<li>robots.txt应放置于网站的根目录下</li>
<li>如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据（Metadata，又称元数据）</li>
</ul>
<blockquote>
<p><strong>明确：robots协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私</strong></p>
</blockquote>
</blockquote>
<ul>
<li><strong>常用的robots协议语法：</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"># 1. 禁止所有搜索引擎抓取整个网站</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br><span class="line"></span><br><span class="line"># 2. 禁止所有搜索引擎抓取某一目录及其内容（禁止抓取的目录字符串可以出现在路径中的任何位置，因此Disallow: </span><br><span class="line">/junk/与https://example.com/junk/和https://example.com/for-sale/other/junk/均匹配）</span><br><span class="line"></span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /calendar/</span><br><span class="line">Disallow: /junk/</span><br><span class="line">Disallow: /books/fiction/contemporary/</span><br><span class="line"></span><br><span class="line"># 3. 只有 googlebot-news 可以抓取整个网站</span><br><span class="line">User-agent: Googlebot-news</span><br><span class="line">Allow: /</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br><span class="line"></span><br><span class="line"># 4. Unnecessarybot 不能抓取相应网站，所有其他漫游器都可以</span><br><span class="line">User-agent: Unnecessarybot</span><br><span class="line">Disallow: /</span><br><span class="line">User-agent: *</span><br><span class="line">Allow: /</span><br><span class="line"></span><br><span class="line"># 5. 禁止所有搜索引擎抓取 useless_file.html 网页</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /useless_file.html</span><br><span class="line"></span><br><span class="line"># 6. 禁止访问 dogs.jpg 图片</span><br><span class="line">User-agent: Googlebot-Image</span><br><span class="line">Disallow: /images/dogs.jpg</span><br><span class="line"></span><br><span class="line"># 7. 禁止 Google 图片访问您网站上的所有图片(如果无法抓取图片和视频，则 Google 无法将其编入索引)</span><br><span class="line">User-agent: Googlebot-Image</span><br><span class="line">Disallow: /</span><br><span class="line"></span><br><span class="line"># 8. 禁止谷歌抓取所有 .gif 文件</span><br><span class="line">User-agent: Googlebot</span><br><span class="line">Disallow: `/*.gif$`</span><br><span class="line"></span><br><span class="line"># 9. 禁止抓取整个网站，但允许 Mediapartners-Google 访问内容</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br><span class="line">User-agent: Mediapartners-Google</span><br><span class="line">Allow: /</span><br><span class="line"></span><br><span class="line"># 10. 禁止谷歌抓取所有 .xls 文件</span><br><span class="line">User-agent: Googlebot</span><br><span class="line">Disallow: `/*.xls$`</span><br></pre></td></tr></table></figure>

<h3 id="3-unicode和utf8编码"><a href="#3-unicode和utf8编码" class="headerlink" title="3. unicode和utf8编码"></a>3. unicode和utf8编码</h3><ul>
<li><p><strong>Unicode：Unicode是一个字符集，它定义了每个字符的唯一编号（即码点），并为这些字符提供了一个标准名称</strong></p>
</li>
<li><p><strong>UTF-8：UTF-8是一种可变长度字符编码方案，它将Unicode中的每个字符编码成1到4个字节序列，是一种广泛使用的字符编码格式</strong></p>
</li>
<li><p><strong>主要区别：</strong></p>
<ul>
<li><p><strong>Unicode是一个字符集，UTF-8是一种字符编码方式，它们共同构成了现代计算机系统中广泛使用的文本编码标准</strong></p>
</li>
<li><p>UTF-8是一种变长编码，ASCII字符在UTF-8中只占用一个字节，对于英文字母和数字等常见字符，也只占用一个字节，而对于汉字、日文、韩文等字符，则需要2到3个字节</p>
</li>
<li><p><strong>UTF-8最大的优点是它向后兼容ASCII码，也就是说，如果一段文本中只包含ASCII字符，那么使用UTF-8编码后，该文本仍然可以使用ASCII码进行解码，无需转换</strong></p>
</li>
</ul>
</li>
<li><p><strong>encode()方法：用于将字符串从Unicode编码转换为其他编码格式（例如UTF-8、ISO-8859-1等），返回一个字节对象，即bytes</strong></p>
</li>
<li><p><strong>decode()方法：用于将字节数据从其他编码格式（例如UTF-8、ISO-8859-1等）转换为Unicode编码的字符串，返回一个字符串</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Python 3中，默认的字符串类型是Unicode字符串</span><br><span class="line"># 从外部源接收的字节数据通常需要使用decode()来转换为字符串以进行处理</span><br><span class="line"></span><br><span class="line">str_code = &#x27;xfblog&#x27;</span><br><span class="line"></span><br><span class="line"># 使用encode()方法编码</span><br><span class="line">byte_code = str_code.encode(&#x27;utf-8&#x27;)</span><br><span class="line">print(byte_code)  # b&#x27;xfblog&#x27;</span><br><span class="line">print(type(byte_code))  # `&lt;class &#x27;bytes&#x27;&gt;`</span><br><span class="line"></span><br><span class="line"># 使用decode()方法解码</span><br><span class="line">str_c = byte_code.decode(&#x27;utf-8&#x27;)</span><br><span class="line">print(str_c)  # xfblog</span><br><span class="line">print(type(str_c))  # `&lt;class &#x27;str&#x27;&gt;`</span><br><span class="line">&quot;&quot;&quot;编码方式必须和解码方式必须一样，否则就会出现乱码问题&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<h3 id="4-网站分析"><a href="#4-网站分析" class="headerlink" title="4. 网站分析"></a>4. 网站分析</h3><blockquote>
<p><strong>注意：</strong></p>
<ul>
<li><strong>通过浏览器直接访问的方式是无法发送get以外的请求的</strong></li>
<li><strong>进行post方法请求时，参数data和headers尽量都使用关键字实参</strong></li>
<li><strong>进行port方法请求时，如果data中携带的某一字段值为空，则需使用空字符串””代替，而不是None！</strong></li>
<li><strong>请求移动端获取的数据接口时，请求头的user-agent也一定要用移动端的</strong></li>
</ul>
</blockquote>
<ul>
<li><p><strong>查询字符串参数：</strong></p>
<ul>
<li><p><strong>get请求[url链接中]：显现的，需要通过request.get()方法中的params参数传递，可以不完整</strong></p>
</li>
<li><p><strong>post请求[表单数据]：隐藏的，需要通过request.post()方法中的data参数传递，必须完整</strong></p>
</li>
<li><p><strong>post请求[请求载荷]：隐藏的，需要通过request.post()方法中的json参数传递，必须完整</strong></p>
</li>
</ul>
</li>
<li><p><strong>判断网站是否是动态网站：</strong></p>
<ul>
<li>观察当前网站页面中的翻页按钮，如果翻页后浏览器的url地址没有发生变化，则此网站数据大概率是动态数据，否则，大概率是静态数据</li>
<li><strong>如果一个页面有下一页，一定要分析一下，有些网站只在第一页使用静态数据，而后面使用动态数据，所以不要认为网页源代码中能搜索到数据，就一定是静态数据（要以api接口为主）</strong></li>
</ul>
</li>
<li><p><strong>数据抓包流程：</strong></p>
<ul>
<li>打开浏览器开发者工具，选择网路选项卡，使用放大镜搜索相关数据关键字，会在响应和预览中得到对应的json数据，分析json数据是否存在需要的数据（响应与预览中的数据一致）</li>
<li>第一次进入网站后先清除日志，再刷新网站以获取当前全部api。当数据过多时，一般不在响应中定位数据，而是直接在预览中配合ctrl+F搜索关键字直接定位到数据</li>
</ul>
</li>
</ul>
<h3 id="5-miniconda的使用"><a href="#5-miniconda的使用" class="headerlink" title="5. miniconda的使用"></a>5. miniconda的使用</h3><ul>
<li><p><strong>创建</strong>：conda create -n 虚拟环境名称 python&#x3D;版本号</p>
</li>
<li><p><strong>列表</strong>：conda env list</p>
</li>
<li><p><strong>激活：</strong>conda activate myenv</p>
</li>
<li><p><strong>退出</strong>：conda deactivate</p>
</li>
<li><p><strong>删除</strong>：conda remove -n “myenv” –all</p>
</li>
<li><p><strong>查看python版本：</strong><code>python -V</code></p>
</li>
</ul>
<h2 id="三、Requests模块"><a href="#三、Requests模块" class="headerlink" title="三、Requests模块"></a>三、Requests模块</h2><blockquote>
<p><strong><a class="link"   href="https://requests.readthedocs.io/projects/cn/zh_CN/latest/"  target="_blank">Requests官方文档<i class="fas fa-external-link-alt"></i></a><strong>：Requests 唯一的一个</strong>非转基因</strong>的 Python HTTP 库，人类可以安全享用</p>
<p><strong>警告</strong>：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡</p>
<p><strong>看吧，这就是 Requests 的威力：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;</span><br><span class="line">服务器：使用&quot;HTTP协议&quot;中通过socket模拟的简单HTTP服务器</span><br><span class="line">客户端：使用requests模拟浏览器向HTTP服务器发起请求</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;http://127.0.0.1:8000&quot;</span><br><span class="line"># 在服务器接受到的客户端请求头中，会出现很多并没有进行指定的其余字段，</span><br><span class="line"># 这是因为requests认为这些字段在此次http请求中是必不可少的，所以默认添加的</span><br><span class="line">header = &#123;</span><br><span class="line">    &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;</span><br><span class="line">&#125;</span><br><span class="line">params = &#123;</span><br><span class="line">    &quot;username&quot;: &#x27;wcf&#x27;,</span><br><span class="line">    &#x27;password&#x27;: &#x27;123456&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 1. 请求方法为get，使用params关键字指定参数</span><br><span class="line"># r = requests.get(url, headers=header, params=params)</span><br><span class="line"></span><br><span class="line"># 2. 请求方法为post，使用data关键字指定参数，发送表单数据或其他编码的数据</span><br><span class="line"># r = requests.post(url, headers=header, data=params)</span><br><span class="line"></span><br><span class="line"># 3. 请求方法为post，使用json关键字指定参数，发送JSON格式的数据</span><br><span class="line">r = requests.post(url, headers=header, json=params)</span><br><span class="line">&quot;&quot;&quot;携带不同类型的参数进行请求，请求头中Content-Type字段值和http内容实体都是不一样的&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="1-响应属性"><a href="#1-响应属性" class="headerlink" title="1. 响应属性"></a>1. 响应属性</h3><ul>
<li><p>常用响应属性：</p>
<ul>
<li><strong>response.status_code：</strong>响应的http状态码</li>
<li><strong>response.text：</strong>响应的文本内容（str类型）</li>
<li><strong>response.content：</strong>响应的二进制内容（bytes类型）</li>
<li><strong>response.json()：</strong>用于将json响应解析为python对象</li>
<li><strong>response.history：</strong>包含请求重定向历史，是一个包含Response对象的列表</li>
</ul>
</li>
<li><p>响应头：</p>
<ul>
<li><strong>response.headers：</strong>包含响应头信息的字典</li>
<li><strong>response.cookies：</strong>一个RequestsCookieJar对象，包含响应中的cookies，即Set-Cookie值</li>
</ul>
</li>
<li><p>请求头：</p>
<ul>
<li><strong>response.request：</strong>包含生成此响应的请求的相关信息，是一个PreparedRequest对象</li>
<li><strong>response.request.headers</strong>：包含响应对应请求头信息的字典</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;https://www.baidu.com&quot;</span><br><span class="line">response = requests.get(url)</span><br><span class="line"></span><br><span class="line"># 通常：通过response.encoding修改成网页源码的编码格式</span><br><span class="line">response.encoding = &quot;utf-8&quot;</span><br><span class="line"># 技巧：将字节数据转为字符串数据，python中decode默认解码方式为utf-8</span><br><span class="line">print(response.content.decode())  # 百度网页源代码</span><br><span class="line"></span><br><span class="line"># 获取响应中的cookies，即set-cookie信息</span><br><span class="line">print(response.cookies)  # `&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;`</span><br><span class="line">print(response.cookies.get_dict())  # &#123;&#x27;BDORZ&#x27;: &#x27;27315&#x27;&#125;</span><br><span class="line">print(requests.utils.dict_from_cookiejar(response.cookies))  # &#123;&#x27;BDORZ&#x27;: &#x27;27315&#x27;&#125;</span><br><span class="line"></span><br><span class="line"># 获取响应对应的请求头中的cookies</span><br><span class="line">print(response.request.headers.get(&#x27;cookies&#x27;))  # None</span><br></pre></td></tr></table></figure>

<h3 id="2-字节流下载"><a href="#2-字节流下载" class="headerlink" title="2. 字节流下载"></a>2. 字节流下载</h3><ul>
<li>简介：将stream参数设置为True时，Requests库返回的响应对象具有类似生成器的特性，即<strong>可迭代的流式响应</strong>。在这种流式响应对象下，<strong>可以使用iter_content()方法逐块处理响应内容</strong>，而不是一次性加载整个响应内容到内存中</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">video_url = &quot;&quot;</span><br><span class="line"></span><br><span class="line"># requests是同步的，设置了stream参数后，requests请求类似于生成器，是懒惰的</span><br><span class="line"># 只有调用iter_content方法时，才会真正发起请求，获取响应内容</span><br><span class="line">response = requests.get(video_url, stream=True)</span><br><span class="line"></span><br><span class="line"># 获取视频数据长度，通过响应头中的Content-Length字段</span><br><span class="line">data_length = int(response.headers.get(&quot;Content-Length&quot;))</span><br><span class="line"></span><br><span class="line"># 检查响应状态</span><br><span class="line">if response.status_code == 200:</span><br><span class="line">    with open(&quot;./video.mp4&quot;, &quot;wb&quot;) as f:</span><br><span class="line">        write_length = 0</span><br><span class="line">        # 逐块处理响应内容，使用iter_content方法，每次处理1024字节的数据</span><br><span class="line">        for chunk in response.iter_content(chunk_size=1024):</span><br><span class="line">            if chunk:</span><br><span class="line">                write_length += f.write(chunk)  # write方法返回写入的字节数</span><br><span class="line">                print(f&quot;下载进度：&#123;(write_length / data_length) * 100: .2f&#125;%&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="3-请求重定向"><a href="#3-请求重定向" class="headerlink" title="3. 请求重定向"></a>3. 请求重定向</h3><blockquote>
<p><strong>自动重定向：</strong>默认情况下，requests 会自动处理 HTTP 重定向。即当发起一个请求时，如果服务器返回了一个重定向响应（HTTP 状态码 3xx），requests 会自动跟踪并执行这个重定向，直到达到重定向的最终目标或达到最大允许重定向次数</p>
</blockquote>
<ul>
<li><p><strong>allow_redirects参数：</strong>可以控制是否允许自动重定向</p>
</li>
<li><p><strong>response.history属性：</strong>包含请求重定向历史，是一个包含Response对象的列表</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;https://www.360buy.com&quot;  # 自动重定向http://www.jd.com/</span><br><span class="line"></span><br><span class="line"># 设置allow_redirects为False，禁止自动重定向</span><br><span class="line">response = requests.get(url, allow_redirects=False)</span><br><span class="line">print(response.url)  # https://www.360buy.com</span><br><span class="line"></span><br><span class="line"># 获取这次请求中经历的所有重定向请求记录</span><br><span class="line">for resp in response.history:</span><br><span class="line">    print(resp.url)</span><br></pre></td></tr></table></figure>

<h3 id="4-SSL证书错误"><a href="#4-SSL证书错误" class="headerlink" title="4. SSL证书错误"></a>4. SSL证书错误</h3><ul>
<li><p><strong>默认验证证书：默认情况下，requests会验证 SSL 证书，确保通信是安全的</strong></p>
</li>
<li><p><strong>verify参数：</strong>是requests库中用于控制 SSL 证书验证的重要参数</p>
</li>
<li><p><strong>禁用证书验证：</strong>将 verify 参数设置为 False 来禁用证书验证，但由于不安全不建议在生产环境中使用</p>
</li>
<li><p><strong>指定自定义 CA 证书文件：</strong>可以使用 verify 参数指定自定义的 CA（Certificate Authority）证书文件，以便 requests 使用该 CA 来验证服务器的证书，可以用于内部服务器或特殊证书的情况</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;requests.exceptions.SSLError异常原因：</span><br><span class="line">1. 网站证书过期</span><br><span class="line">2. 网站链接编写错误</span><br><span class="line">3. urllib3版本过高，查看命令pip show urllib3，最好控制在1.26以下</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">url = &quot;https://chinasoftinc.com/owa&quot;</span><br><span class="line"></span><br><span class="line"># 禁用证书验证后，解决异常抛出</span><br><span class="line">response = requests.get(url, verify=False)</span><br></pre></td></tr></table></figure>

<h3 id="5-retrying模块"><a href="#5-retrying模块" class="headerlink" title="5. retrying模块"></a>5. retrying模块</h3><ul>
<li><strong>请求超时：timeout 参数用于设置请求的超时时间，即允许请求花费的最长时间</strong></li>
<li><strong>@retry装饰器：</strong>用于实现在执行某个函数或方法时进行自动重试的功能（默认决策是在失败后等待1秒并重试，最大尝试次数为3次）<ul>
<li><strong>wait_fixed</strong>：指定每次重试之间的等待时间（单位为毫秒）</li>
<li><strong>stop_max_attempt_number</strong>：指定最大尝试次数</li>
<li><strong>retry_on_exception</strong>：只指定某种异常会触发重试</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from retrying import retry</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">num = 1</span><br><span class="line"></span><br><span class="line">@retry(stop_max_attempt_number=3, wait_fixed=3000,</span><br><span class="line">       retry_on_exception=lambda x: isinstance(x, requests.exceptions.RequestException))</span><br><span class="line">def make_request():</span><br><span class="line">    global num</span><br><span class="line">    print(f&#x27;num：&#123;num&#125;&#x27;)</span><br><span class="line">    num += 1</span><br><span class="line">    # 超时会直接抛出异常并重试：requests.exceptions.ConnectTimeout</span><br><span class="line">    response = requests.get(&quot;https://example.com&quot;, timeout=2)</span><br><span class="line">    </span><br><span class="line">    # &quot;断言&quot;，如果assert后面的条件为True则继续运行，否则会抛出异常并重试</span><br><span class="line">    assert response.status_code == 200  # 状态码不是200，也会报错并重试</span><br><span class="line">    </span><br><span class="line">    # 检查响应的状态码是否表示请求成功，如果不是将引发一个异常</span><br><span class="line">    # response.raise_for_status()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    try:</span><br><span class="line">        make_request()</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(&quot;抛出异常:&quot;, e)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">num：1</span><br><span class="line">num：2</span><br><span class="line">num：3</span><br><span class="line">抛出异常: HTTPSConnectionPool(host=&#x27;example.com&#x27;, port=443): Read timed out. (read timeout=2)</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<h3 id="6-Session会话"><a href="#6-Session会话" class="headerlink" title="6. Session会话"></a>6. Session会话</h3><blockquote>
<p><strong>问题场景</strong>：在第一次请求后响应头中携带了一个set-cookie回来，需要在第二次发起请求的时候在请求头中设置第一次服务器响应中的set-cookie值</p>
<p><strong>场景解释</strong>：当在爬取某些页面的时候，服务器往往会需要cookie信息，而想要得到cookie就需要先访问某个url进行登录，服务器接收到请求之后验证用户名以及密码，在登录成功后会返回一个带有set-cookie信息的响应，这个set-cookie信息就是在下一次请求时需要发送给服务器的cookie信息</p>
<p><strong>问题解决：</strong>虽然可以通过response.cookies.get_dict()提取到此次响应的set-cookie信息，但在下一个请求中再携带这个数据的过程较为麻烦，所以requests库中的Session会话就派上用场了</p>
</blockquote>
<ul>
<li><strong>Session重要作用：</strong><ul>
<li><strong>状态保持</strong>：会话允许在多个HTTP请求之间保持状态。这对于模拟登录、保持用户会话或执行多步骤的操作非常重要</li>
<li><strong>Cookie 管理</strong>：会话自动处理cookies的存储和发送，这对于维护用户身份认证和跟踪用户会话非常有用</li>
<li><strong>全局参数共享</strong>：可以在会话对象中设置全局参数，如请求头、代理设置、认证信息等，以便在多个请求之间共享这些参数，从而减少重复代码</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 创建一个session对象</span><br><span class="line">session = requests.session()</span><br><span class="line"></span><br><span class="line"># 设置全剧参数</span><br><span class="line">session.headers.update(&#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;</span><br><span class="line">&#125;)</span><br><span class="line"># session.cookies.update(&#123;&#x27;key&#x27;: &#x27;value&#x27;&#125;)</span><br><span class="line"></span><br><span class="line"># 第一次请求</span><br><span class="line">response = session.get(&quot;https://www.baidu.com&quot;)</span><br><span class="line">print(&quot;第一次请求头的User-Agent信息：&quot;, response.request.headers[&#x27;User-Agent&#x27;])</span><br><span class="line"># 第一次请求返回的响应头中的set-cookie信息</span><br><span class="line">print(response.cookies.get_dict())</span><br><span class="line"></span><br><span class="line"># 第二次请求</span><br><span class="line">response = session.get(&quot;https://www.baidu.com&quot;)</span><br><span class="line">print(&quot;第二次请求头的User-Agent信息：&quot;, response.request.headers[&#x27;User-Agent&#x27;])</span><br><span class="line"># 第二次请求的请求头所携带的cookie</span><br><span class="line">print(response.request.headers[&#x27;cookie&#x27;])</span><br></pre></td></tr></table></figure>

<h3 id="7-代理ip"><a href="#7-代理ip" class="headerlink" title="7. 代理ip"></a>7. 代理ip</h3><ul>
<li><strong>什么是代理（Proxy）：</strong>代理是一个位于客户端和目标服务器之间的中介服务器。客户端的请求首先发送到代理服务器，然后由代理服务器将请求转发给目标服务器，最后将响应传回给客户端</li>
<li><strong>作用：</strong>代理允许隐藏自己的真实IP地址，绕过网站的访问限制，提高爬取数据的可靠性和隐私保护</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># 使用代理进行测试</span><br><span class="line">ip = &quot;127.0.0.1&quot;</span><br><span class="line">port = 7890</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    &quot;http&quot;: f&quot;http://&#123;ip&#125;:&#123;port&#125;&quot;,</span><br><span class="line">    &quot;https&quot;: f&quot;https://&#123;ip&#125;:&#123;port&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">header = &#123;</span><br><span class="line">    &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(&quot;http://httpbin.org/ip&quot;, proxies=proxies, headers=header)</span><br><span class="line">print(response.json()[&#x27;origin&#x27;])  # 代理ip</span><br></pre></td></tr></table></figure>

<h2 id="四、数据提取"><a href="#四、数据提取" class="headerlink" title="四、数据提取"></a>四、数据提取</h2><h3 id="1-关于JSON数据"><a href="#1-关于JSON数据" class="headerlink" title="1. 关于JSON数据"></a>1. 关于JSON数据</h3><ul>
<li><strong>JSON（JavaScript Object Notation）：</strong>是一种轻量级的数据交换格式，用于在不同应用程序之间传输和存储数据</li>
<li><strong>json数据：主要由对象（Object）和数组（Array）两种结构组成。</strong><ul>
<li>对象由一组键值对构成，使用大括号”{}”包围</li>
<li>数组由一个有序的值列表构成，使用方括号”[]”包围</li>
</ul>
</li>
<li><strong>Python处理JSON数据的方法：</strong><ul>
<li><strong>json.dumps()：用于将Python对象转换为JSON格式的字符串</strong></li>
<li><strong>json.loads()：用于将JSON字符串解析为Python对象</strong></li>
<li><strong>json()：用于便捷将响应的JSON数据直接转换为Python对象（通常是字典或列表）</strong></li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;http://www.cninfo.com.cn/new/disclosure&quot;</span><br><span class="line">header = &#123;</span><br><span class="line">    &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;</span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    &#x27;column&#x27;: &#x27;szse_latest&#x27;,</span><br><span class="line">    &#x27;pageNum&#x27;: 1,</span><br><span class="line">    &#x27;pageSize&#x27;: 30,</span><br><span class="line">    &#x27;sortName&#x27;: &#x27;&#x27;,</span><br><span class="line">    &#x27;sortType&#x27;: &#x27;&#x27;,</span><br><span class="line">    &#x27;clusterFlag&#x27;: &#x27;true&#x27;</span><br><span class="line">&#125;</span><br><span class="line">str_data = requests.post(url, headers=header, data=data).content.decode()</span><br><span class="line">print(type(str_data))  # `&lt;class &#x27;str&#x27;&gt;`</span><br><span class="line"></span><br><span class="line">json_dict = json.loads(str_data)</span><br><span class="line">print(type(json_dict))  # `&lt;class &#x27;dict&#x27;&gt;`</span><br><span class="line"></span><br><span class="line"># indent指定缩进级别，可以格式美化json字符串；ensure_ascii实现让中文写入的时候保持为中文</span><br><span class="line">json_str = json.dumps(json_dict, indent=2, ensure_ascii=False)</span><br><span class="line">print(type(json_str), json_str)  # &lt;class &#x27;str&#x27;&gt; &#123;......&#125;</span><br><span class="line"></span><br><span class="line"># json()方法：便捷将响应的JSON数据直接转换为Python对象</span><br><span class="line">json_data = requests.post(url, headers=header, data=data).json()</span><br><span class="line">print(type(json_data))  # `&lt;class &#x27;dict&#x27;&gt;`</span><br></pre></td></tr></table></figure>

<h3 id="2-xpath语法"><a href="#2-xpath语法" class="headerlink" title="2. xpath语法"></a>2. xpath语法</h3><blockquote>
<p>可以直接通过浏览器开发者工具选取对应元素，<strong>对元素对应的标签右键Copy –&gt; Copy XPath</strong>，即可提取对应的xpath路径表达式</p>
</blockquote>
<ul>
<li><strong>XPath（XML Path Language）：</strong>是一种用于在XML文档中定位和选择元素的查询语言。其语法非常强大，可以实现以非常精确的方式遍历和提取XML文档中的数据，<strong>返回值为列表</strong>（<a class="link"   href="https://www.w3school.com.cn/xpath/xpath_syntax.asp"  target="_blank" rel="noopener">W3chool官方文档<i class="fas fa-external-link-alt"></i></a>）</li>
<li><strong>选取节点：</strong><ul>
<li>nodename ：选取此节点的所有子节点</li>
<li>&#x2F; ：从根节点选取；或表示一个层级关系</li>
<li>&#x2F;&#x2F; ：从匹配选择的当前节点选择文档中的所有节点，而不考虑它们的位置；或表示多个层级关系</li>
<li>. ：选取当前节点</li>
<li>.. ：选取当前节点的父节点</li>
<li>@ ：选取属性</li>
<li>text() ：选取文本</li>
</ul>
</li>
<li><strong>谓语：</strong><ul>
<li>&#x2F;bookstore&#x2F;book[1] ：选取属于 bookstore 子元素的第一个 book 元素（**<span style="color:red;">索引定位，索引是从1开始的</span>**）</li>
<li>&#x2F;bookstore&#x2F;book[last()] ：选取属于 bookstore 子元素的最后一个 book 元素</li>
<li>&#x2F;bookstore&#x2F;book[last()-1] ：选取属于 bookstore 子元素的倒数第二个 book 元素</li>
<li>&#x2F;bookstore&#x2F;book[position()&lt;3] ：选取最前面的两个属于 bookstore 元素的子元素的 book 元素</li>
<li>&#x2F;&#x2F;title[@lang] ：选取所有拥有名为 lang 的属性的 title 元素</li>
<li>&#x2F;&#x2F;title[@lang&#x3D;’eng’] ：选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性</li>
<li>&#x2F;bookstore&#x2F;book[price&gt;35.00] ：选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00</li>
<li>&#x2F;bookstore&#x2F;book[price&gt;35.00]&#x2F;title ：选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00</li>
</ul>
</li>
<li><strong>选取未知节点：</strong><ul>
<li>* ：匹配任何元素节点</li>
<li>@* ：匹配任何属性节点</li>
<li>node() ：匹配任何类型的节点</li>
</ul>
</li>
<li><strong>选取若干路径：</strong>通过在路径表达式中使用“|”运算符，可以选取若干个路径</li>
<li><strong>contains包含</strong>：<ul>
<li><strong>tag[contains(text(),”item”)]：</strong>文本包含item的tag标签</li>
<li><strong>tag[contains(@class,”item-“)]：</strong>class属性值包含item-的tag标签</li>
</ul>
</li>
<li><strong>lxml解析数据：</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line">text = &#x27;&#x27;&#x27; &lt;div&gt; &lt;ul&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span><br><span class="line">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; </span><br><span class="line">        &lt;/ul&gt; &lt;/div&gt; &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"># 创建element对象来使用xpath语法（不能使用etree关键字定义变量！）</span><br><span class="line">tree = etree.HTML(text)  # `&lt;Element html at 0x10030abc0&gt;`</span><br><span class="line">li_list = tree.xpath(&#x27;//li[@class=&quot;item-1&quot;]&#x27;)  # 返回多个element对象的列表</span><br><span class="line">print(li_list)  # [&lt;Element li at 0x10312f5c0&gt;, &lt;Element li at 0x10312f540&gt;, &lt;Element li at 0x10312f640&gt;]</span><br><span class="line"></span><br><span class="line">info = []</span><br><span class="line">for li in li_list:  # 分组，将li和a标签分为一组，不会出现关系混乱的问题</span><br><span class="line">    # 只有element对象才有xpath方法，xpath方法返回的是列表</span><br><span class="line">    item_info = &#123;&#125;</span><br><span class="line">    item_info[&#x27;href&#x27;] = li.xpath(&#x27;./a/@href&#x27;)[0] if len(li.xpath(&#x27;./a/@href&#x27;)) &gt; 0 else None</span><br><span class="line">    item_info[&#x27;text&#x27;] = li.xpath(&#x27;./a/text()&#x27;)[0] if len(li.xpath(&#x27;./a/text()&#x27;)) &gt; 0 else None</span><br><span class="line">    info.append(item_info)</span><br><span class="line">print(info)</span><br></pre></td></tr></table></figure>

<h3 id="3-jsonpath语法"><a href="#3-jsonpath语法" class="headerlink" title="3. jsonpath语法"></a>3. jsonpath语法</h3><ul>
<li><strong>JSONPath：</strong>是一种用于从JSON数据结构中提取信息的查询语言和表达式，类似于XPath用于在XML数据中定位和选择节点，常用于解析多层嵌套的json数据，<strong>返回值为列表</strong>（<a class="link"   href="https://goessner.net/articles/JsonPath/"  target="_blank" rel="noopener">官方文档<i class="fas fa-external-link-alt"></i></a>）</li>
<li><strong>jsonpath语法元素：</strong><ul>
<li>$ ：根节点</li>
<li>@ ：现行节点</li>
<li>. 或 [] ：子节点</li>
<li>.. ：递归下降，从匹配选择的现行节点选择文档中的所有节点，而不考虑它们的位置</li>
<li>* ：通配符，所有节点</li>
<li>[] ：下标运算符</li>
<li>[, ] ：并运算符，产生节点集的组合（<strong>兼容切片操作，直接选取范围的节点集</strong>）</li>
<li>() ：支持表达式计算</li>
<li>?() ：应用过滤器表达式，包括过滤和判断（**与&amp;&amp;、或||、非!**）</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import jsonpath</span><br><span class="line"></span><br><span class="line"># 提取数据前，一定要先在开发者工具的预览中分析返回数据的数据结构</span><br><span class="line">json_data = &#123;</span><br><span class="line">    &quot;store&quot;: &#123;</span><br><span class="line">        &quot;book&quot;: [</span><br><span class="line">            &#123;&quot;category&quot;: &quot;reference&quot;, &quot;author&quot;: &quot;jack&quot;, &quot;title&quot;: &quot;First Book&quot;, &quot;price&quot;: 8.95&#125;,</span><br><span class="line">            &#123;&quot;category&quot;: &quot;fiction&quot;, &quot;author&quot;: &quot;alice&quot;, &quot;title&quot;: &quot;Second Book&quot;, &quot;price&quot;: 12.99&#125;,</span><br><span class="line">            &#123;&quot;category&quot;: &quot;fiction&quot;, &quot;author&quot;: &quot;tony&quot;, &quot;title&quot;: &quot;Thrid Book&quot;, &quot;isbn&quot;: &quot;0-519-15&quot;, &quot;price&quot;: 8.99&#125;,</span><br><span class="line">            &#123;&quot;category&quot;: &quot;fiction&quot;, &quot;author&quot;: &quot;lucy&quot;, &quot;title&quot;: &quot;Fourth Book&quot;, &quot;isbn&quot;: &quot;0-1225-15&quot;, &quot;price&quot;: 22.99&#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;bicycle&quot;: &#123;&quot;color&quot;: &quot;red&quot;, &quot;price&quot;: 19.95&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 1. 提取第一本书的title</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store.book[0].title&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 2. 提取第1和第3两本书的title</span><br><span class="line"># res = jsonpath.jsonpath(json_data, &#x27;$.store.book[0,2].title&#x27;)</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store.book[::2].title&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 3. 提取最后一本书的title</span><br><span class="line"># res = jsonpath.jsonpath(json_data, &#x27;$.store.book[(@.length-1)].title&#x27;)</span><br><span class="line"># 切片指定从倒数第一个元素开始一直到列表的末尾，因此它将提取最后一个元素</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store.book[-1:].title&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 4. 提取价格小于10的所有书本的title</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store.book[?(@.price&lt;10)].title&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 5. 提取价格小于等于20的所有商品的价格</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$..*[?(@.price&lt;=20)].price&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 6. 提取所有书本的作者信息</span><br><span class="line"># res = jsonpath.jsonpath(json_data, &#x27;$.store.book[::].author&#x27;)</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store.book[*].author&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 7. 提取store中所有商品的价格</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$.store..price&#x27;)</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line"># 8. 提取是否带有isbn键的书本信息，判断是否带有，所以用?()</span><br><span class="line">rest = jsonpath.jsonpath(json_data, &#x27;$..book[?(@.isbn)].title&#x27;)  # 带有</span><br><span class="line">resf = jsonpath.jsonpath(json_data, &#x27;$..book[?(!@.isbn)].title&#x27;)  # 不带有，非 !</span><br><span class="line">print(rest, resf)</span><br><span class="line"></span><br><span class="line"># 9. 提取价格在5-10之间的书本信息，提取价格不在5-10之间的书本信息</span><br><span class="line">res1 = jsonpath.jsonpath(json_data, &#x27;$..book[?(@.price&gt;=5 &amp;&amp; @.price&lt;=10)].price&#x27;)  # 与 &amp;&amp;</span><br><span class="line">res2 = jsonpath.jsonpath(json_data, &#x27;$..book[?(@.price&lt;5 || @.price&gt;10)].price&#x27;)  # 或 ||</span><br><span class="line">print(res1, res2)</span><br><span class="line"></span><br><span class="line"># 10. 获取所有元素</span><br><span class="line">res = jsonpath.jsonpath(json_data, &#x27;$..&#x27;)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>

<h3 id="4-bs4语法"><a href="#4-bs4语法" class="headerlink" title="4. bs4语法"></a>4. bs4语法</h3><ul>
<li><strong>Beautiful Soup：</strong>通常缩写为bs4，是Python中一个用于解析HTML和XML文档的第三方库。bs4是基于HTML DOM的，会载入整个文档并解析整个DOM树，因此性能要低于lxml模块（<a class="link"   href="https://beautifulsoup.readthedocs.io/"  target="_blank" rel="noopener">官方文档<i class="fas fa-external-link-alt"></i></a>）</li>
<li><strong>find(self, name&#x3D;None, attrs&#x3D;{}, recursive&#x3D;True, string&#x3D;None, **kwargs)</strong><ul>
<li>用于查找第一个匹配的标签或元素，返回匹配的标签或元素，没有匹配项则返回None</li>
</ul>
</li>
<li><strong>find_all(self, name&#x3D;None, attrs&#x3D;{}, recursive&#x3D;True, string&#x3D;None, limit&#x3D;None, **kwargs)</strong><ul>
<li>用于查找所有匹配的标签或元素，返回一个结果列表，没有匹配项则返回一个空列表</li>
<li><strong>name参数</strong>：用于指定要查找的标签名称或过滤条件</li>
<li><strong>attrs参数</strong>：根据标签的属性来过滤元素（或者直接使用class_|id|attr &#x3D; ‘’进行属性定位）</li>
<li><strong>recursive参数</strong>：用于指定是否递归查找子元素，默认为True</li>
<li><strong>string参数</strong>：用于搜索包含指定文本的元素</li>
<li><strong>limit参数</strong>：用于指定返回结果的数量上限</li>
</ul>
</li>
<li><strong>select(self, selector, namespaces&#x3D;None, limit&#x3D;None, **kwargs)</strong><ul>
<li>方法接受一个CSS选择器字符串作为参数，用于选择文档中的标签或元素。返回一个结果列表</li>
</ul>
</li>
<li><strong>提取标签中的文本和属性：</strong><ul>
<li>**text &#x2F; get_text()**：获取标签中所有的文本内容</li>
<li>**get(‘attr’)**：获取标签的指定属性值</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">html_doc = &quot;&quot;&quot;</span><br><span class="line">&lt;html&gt;&lt;head&gt;&lt;title&gt;讲故事&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1 class=&quot;sister&quot;&gt;&lt;b&gt;熊出没&lt;/b&gt;&lt;/h1&gt;</span><br><span class="line"></span><br><span class="line">&lt;p class=&quot;story&quot;&gt;两只狗熊和一个人：</span><br><span class="line">&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;</span><br><span class="line">&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;</span><br><span class="line">&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;</span><br><span class="line">&lt;/p&gt;</span><br><span class="line">&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 创建一个BeautifulSoup对象，并指定lxml解析器（该对象包含了解析后的文档树）</span><br><span class="line">soup = BeautifulSoup(html_doc, &#x27;lxml&#x27;)</span><br><span class="line"></span><br><span class="line"># 注意：指定name参数则在指定的标签中做属性定位，不指定则在文档所有标签中做属性定位</span><br><span class="line">&quot;&quot;&quot;find()方法、find_all()方法&quot;&quot;&quot;</span><br><span class="line"># res = soup.find(&#x27;a&#x27;, attrs=&#123;&#x27;class&#x27;: &#x27;sister&#x27;&#125;)  # find()指定class属性定位</span><br><span class="line"># res = soup.find_all(class_=&quot;sister&quot;)  # find_all()不指定name参数，所有元素中匹配</span><br><span class="line"># res = soup.find_all(&#x27;a&#x27;, class_=&quot;sister&quot;)  # 根据class属性定位</span><br><span class="line"># res = soup.find_all(&#x27;a&#x27;, id=&quot;link3&quot;)  # 根据id属性定位</span><br><span class="line">res = soup.find_all(&#x27;a&#x27;, string=&#x27;Tillie&#x27;)  # 根据包含文本定位</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;select()方法&quot;&quot;&quot;</span><br><span class="line"># res = soup.select(&#x27;.sister&#x27;)  # 匹配所有标签中class属性为sister的标签</span><br><span class="line">res = soup.select(&#x27;h1[class=sister]&#x27;)  # 匹配h1标签中class属性为sister的标签</span><br><span class="line">print(res)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;提取标签中的文本和属性&quot;&quot;&quot;</span><br><span class="line">print(res[0].text)  # 熊出没</span><br><span class="line">print(res[0].get_text())  # 熊出没</span><br><span class="line">print(res[0].get(&#x27;class&#x27;))  # [&#x27;sister&#x27;]</span><br></pre></td></tr></table></figure>

<h3 id="5-正则表达式"><a href="#5-正则表达式" class="headerlink" title="5. 正则表达式"></a>5. 正则表达式</h3><blockquote>
<p><strong>关于正则表达式的学习内容：<a class="link"   href="https://xfblog.cn/2023/07/01/Python_4_%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"  target="_blank" rel="noopener">04_Python_正则表达式<i class="fas fa-external-link-alt"></i></a></strong></p>
</blockquote>
<h2 id="五、数据存储"><a href="#五、数据存储" class="headerlink" title="五、数据存储"></a>五、数据存储</h2><h3 id="1-csv文件存储"><a href="#1-csv文件存储" class="headerlink" title="1. csv文件存储"></a>1. csv文件存储</h3><ul>
<li><p><strong>CSV（Comma-Separated Values）：</strong>csv文件是一种常见的文本文件格式，用于存储表格数据</p>
</li>
<li><p><strong>csv数据：数据通常由逗号分隔，每一行表示表格中的一行数据，每个逗号表示一列数据</strong></p>
</li>
<li><p><strong>读写列表数据：</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import csv</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">headers = [&#x27;班级&#x27;, &#x27;姓名&#x27;, &#x27;性别&#x27;, &#x27;手机号&#x27;, &#x27;QQ&#x27;]</span><br><span class="line">rows = [</span><br><span class="line">    [&quot;18级Python&quot;, &#x27;小王&#x27;, &#x27;男&#x27;, &#x27;13146060xx1&#x27;, &#x27;123456xx1&#x27;],</span><br><span class="line">    [&quot;18级Python&quot;, &#x27;小李&#x27;, &#x27;男&#x27;, &#x27;13146060xx2&#x27;, &#x27;123456xx2&#x27;],</span><br><span class="line">    [&quot;19级Python&quot;, &#x27;小赵&#x27;, &#x27;女&#x27;, &#x27;13146060xx3&#x27;, &#x27;123456xx3&#x27;],</span><br><span class="line">    [&quot;19级Python&quot;, &#x27;小红&#x27;, &#x27;女&#x27;, &#x27;13146060xx4&#x27;, &#x27;123456xx4&#x27;],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># 1. 写入列表数据到csv文件</span><br><span class="line">if not os.path.isfile(&#x27;example.csv&#x27;):</span><br><span class="line">    with open(&#x27;example.csv&#x27;, &#x27;w&#x27;, newline=&#x27;&#x27;) as file:</span><br><span class="line">        # 创建csv写入对象（写入器）</span><br><span class="line">        writer = csv.writer(file)</span><br><span class="line">        # 写入数据，一行用writerow，多行用writerows</span><br><span class="line">        writer.writerow(headers)</span><br><span class="line">        writer.writerows(rows)</span><br><span class="line"></span><br><span class="line"># 2. 读取csv文件数据</span><br><span class="line">with open(&#x27;example.csv&#x27;, &#x27;r&#x27;) as file:</span><br><span class="line">    # 创建csv读取对象（读取器）</span><br><span class="line">    reader = csv.reader(file)</span><br><span class="line">    # 遍历每一行数据</span><br><span class="line">    for row in reader:</span><br><span class="line">        print(row)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>读写字典数据：</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import csv</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 操作字典数据时，表头数据必须对应字典中的key</span><br><span class="line">headers = [&#x27;Name&#x27;, &#x27;Age&#x27;, &#x27;City&#x27;]</span><br><span class="line">rows = [</span><br><span class="line">    &#123;&#x27;Name&#x27;: &#x27;Alice&#x27;, &#x27;Age&#x27;: 25, &#x27;City&#x27;: &#x27;New York&#x27;&#125;,</span><br><span class="line">    &#123;&#x27;Name&#x27;: &#x27;Bob&#x27;, &#x27;Age&#x27;: 30, &#x27;City&#x27;: &#x27;San Francisco&#x27;&#125;,</span><br><span class="line">    &#123;&#x27;Name&#x27;: &#x27;Charlie&#x27;, &#x27;Age&#x27;: 22, &#x27;City&#x27;: &#x27;Los Angeles&#x27;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># 1. 写入字典数据到csv文件</span><br><span class="line">if not os.path.isfile(&#x27;example.csv&#x27;):</span><br><span class="line">    with open(&#x27;example.csv&#x27;, &#x27;w&#x27;, newline=&#x27;&#x27;) as file:</span><br><span class="line">        # 创建csv字典写入对象（字典写入器）</span><br><span class="line">        writer = csv.DictWriter(file, headers)</span><br><span class="line">        # **需要先将传递的表头信息写入**</span><br><span class="line">        writer.writeheader()</span><br><span class="line">        # 写入数据，一行用writerow，多行用writerows</span><br><span class="line">        writer.writerows(rows)</span><br><span class="line"></span><br><span class="line"># 2. 读取csv文件字典数据</span><br><span class="line">with open(&#x27;example.csv&#x27;, &#x27;r&#x27;) as file:</span><br><span class="line">    # 创建csv字典读取对象（字典读取器）</span><br><span class="line">    reader = csv.DictReader(file)</span><br><span class="line">    # 遍历每一行数据</span><br><span class="line">    for row in reader:</span><br><span class="line">        print(row)</span><br></pre></td></tr></table></figure>

<h3 id="2-json文件存储"><a href="#2-json文件存储" class="headerlink" title="2. json文件存储"></a>2. json文件存储</h3><blockquote>
<p><strong>常见的json数据格式：外层列表，内层字典（存储时一般以这种格式存储）</strong></p>
<ul>
<li><strong>[{“name”: “jack”, “age”: 20}, {“name”: “alice”, “age”: 18}]</strong></li>
</ul>
</blockquote>
<ul>
<li><strong>json模块常用方法：</strong><ul>
<li>**json.dumps()**：将 Python 对象转换为 JSON 格式的字符串</li>
<li>**json.dump()**：将 Python 对象转换为 JSON 格式的字符串，并将其写入文件对象</li>
<li>**json.loads()**：将 JSON 格式的字符串转换为 Python 对象</li>
<li>**json.load()**：从文件对象中读取 JSON 数据，并将其转换为 Python 对象</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"># 数据获取</span><br><span class="line">url = &#x27;https://www.4399.com/flash/&#x27;</span><br><span class="line">data = requests.get(url).content.decode(&#x27;gb2312&#x27;)</span><br><span class="line"></span><br><span class="line"># 数据解析</span><br><span class="line">tree = etree.HTML(data)</span><br><span class="line">li_list = tree.xpath(&#x27;//ul[@class=&quot;b-game cf&quot;]/li&#x27;)</span><br><span class="line">game_info = list()</span><br><span class="line">for li in li_list:</span><br><span class="line">    item = dict()</span><br><span class="line">    item[&#x27;href&#x27;] = url.replace(&#x27;/flash/&#x27;, &#x27;&#x27;) + li.xpath(&#x27;./a/@href&#x27;)[0]</span><br><span class="line">    item[&#x27;title&#x27;] = li.xpath(&#x27;./a/text()&#x27;)[0]</span><br><span class="line">    game_info.append(item)</span><br><span class="line"></span><br><span class="line"># 数据存储，两种方法等价</span><br><span class="line"># 1. 使用json.dumps()方法将python对象转换为字符串，通过fp.write()写入json文件</span><br><span class="line">with open(&quot;game_info.json_1&quot;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as fp:</span><br><span class="line">    fp.write(json.dumps(game_info, indent=4, ensure_ascii=False))</span><br><span class="line"></span><br><span class="line"># 2. 使用json.dump()方法将python对象转换为字符串，并直接写入json文件</span><br><span class="line">with open(&quot;game_info.json_2&quot;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as fp:</span><br><span class="line">    json.dump(game_info, fp, indent=4, ensure_ascii=False)</span><br></pre></td></tr></table></figure>

<h3 id="3-MySQL数据库存储"><a href="#3-MySQL数据库存储" class="headerlink" title="3. MySQL数据库存储"></a>3. MySQL数据库存储</h3><blockquote>
<p>创建一个数据库：<code>create database py_spider charset=utf8;</code></p>
</blockquote>
<ul>
<li><strong>代码逻辑：</strong><ol>
<li><strong>将url和headers定义为类属性</strong></li>
<li><strong>构建数据库链接对象</strong>，定义为实例属性，保证每一个实例对象都独有一份</li>
<li><strong>获取数据</strong>，定义为类方法，因为方法中所使用的都是类对象</li>
<li><strong>创建表</strong>，定义为实例方法，因为使用的mysql链接对象和游标都是实例属性</li>
<li><strong>数据录入</strong>，因为execute执行插入sql语句时，是需要传入参数的，所以函数通过*args接受参数</li>
</ol>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">import pymysql</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">class TxWork:</span><br><span class="line">    # 将url和headers定义为类属性</span><br><span class="line">    url = &quot;https://careers.tencent.com/tencentcareer/api/post/Query?timestamp=1699680197162&amp;countryId=&amp;cityId=&amp;bgIds=&amp;productId=&amp;categoryId=&amp;parentCategoryId=&amp;attrId=1&amp;keyword=python&amp;pageIndex=&#123;&#125;&amp;pageSize=10&amp;language=zh-cn&amp;area=cn&quot;</span><br><span class="line">    header = &#123;</span><br><span class="line">        &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # 构建数据库链接对象，定义为实例属性，保证每一个实例对象都独有一份</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.db = pymysql.connect(host=&#x27;localhost&#x27;, user=&#x27;root&#x27;, password=&#x27;123456&#x27;, db=&#x27;py_spider&#x27;)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line"></span><br><span class="line">    # 当这个类的生命周期为del退出时，会触发__del__方法</span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.db.close()</span><br><span class="line"></span><br><span class="line">    # 1. 获取数据，定义为类方法（代码优化），因为方法中所使用的都是类对象</span><br><span class="line">    # 类方法的内存占用空间更小，虽然可以将所有方法定义为实例方法，但不建议</span><br><span class="line">    @classmethod</span><br><span class="line">    def get_data(cls):</span><br><span class="line">        for page in range(1, 11):</span><br><span class="line">            info = requests.get(cls.url.format(page), headers=cls.header).json()</span><br><span class="line">            print(f&quot;正在获取第&#123;page&#125;页数据...&quot;)</span><br><span class="line">            data_list = info[&#x27;Data&#x27;][&#x27;Posts&#x27;]</span><br><span class="line">            # 1) 将所有数据都append到列表中，然后return，但数据量过大时列表占用内存空间太大</span><br><span class="line">            # 2) 定义为生成器，因为生成器是惰性加载的，只有用到的时候才去加载，这能有效地利用内存</span><br><span class="line">            yield data_list</span><br><span class="line"></span><br><span class="line">    # 2. 创建表，定义为实例方法，因为使用的mysql链接对象和游标都是实例属性</span><br><span class="line">    def create_table(self):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            create table if not exists tx_work(</span><br><span class="line">                id int primary key auto_increment,</span><br><span class="line">                work_name varchar(100),</span><br><span class="line">                country_name varchar(50),</span><br><span class="line">                city_name varchar(50),</span><br><span class="line">                work_desc text</span><br><span class="line">            );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            self.cursor.execute(&#x27;truncate table tx_work&#x27;)  # 保证可以正常多次调试</span><br><span class="line">            print(&quot;表创建成功&quot;)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(f&quot;创建表失败：&#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # 3. 数据录入，因为execute执行插入sql语句时，是需要传入参数的，所以函数通过*args接受参数</span><br><span class="line">    def insert_mysql(self, *args):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param ags: 对应插入时需要的数据库中的字段参数</span><br><span class="line">            id  work_name  country_name  city_name  work_desc</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            insert into tx_work values(%s, %s, %s, %s, %s);</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql, args)  # args参数是可迭代对象，所以不能用*拆包</span><br><span class="line">            self.db.commit()</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(f&quot;此页中有数据插入失败：&#123;e&#125;&quot;)</span><br><span class="line">            self.db.rollback()</span><br><span class="line">        &quot;&quot;&quot;此处不能关闭游标和链接，因为是循环插入多页数据，第一页关了第二页就不能存了</span><br><span class="line">        finally:</span><br><span class="line">            self.cursor.close()</span><br><span class="line">            self.db.close()</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        self.create_table()</span><br><span class="line">        data_generator = self.get_data()  # 返回一个生成器对象</span><br><span class="line">        work_id = 0  # 使用0或None占位，id为主键，占位可以实现自动自增长</span><br><span class="line">        for data_list in data_generator:</span><br><span class="line">            for data in data_list:</span><br><span class="line">                work_name = data[&#x27;RecruitPostName&#x27;]</span><br><span class="line">                country_name = data[&#x27;CountryName&#x27;]</span><br><span class="line">                city_name = data[&#x27;LocationName&#x27;]</span><br><span class="line">                work_desc = data[&#x27;Responsibility&#x27;]</span><br><span class="line">                self.insert_mysql(work_id, work_name, country_name, city_name, work_desc)</span><br><span class="line">            print(&quot;数据录入成功！&quot;)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    tx_work = TxWork()</span><br><span class="line">    tx_work.main()</span><br></pre></td></tr></table></figure>

<h3 id="4-Peewee的使用"><a href="#4-Peewee的使用" class="headerlink" title="4. Peewee的使用"></a>4. Peewee的使用</h3><ul>
<li>Peewee简介：Peewee 是一个轻量级的 Python ORM 库，它支持 SQLite、MySQL、PostgreSQL 和 Oracle 数据库等多种数据库，并且提供了简单易用的 API</li>
<li>ORM简介：<strong>ORM（Object-Relational Mapping）是一种将关系型数据库中的表和数据映射为对象和属性的方法，从而通过操作对象来完成数据库的增删改查等操作。</strong>使用 ORM 可以让我们更加专注于业务逻辑的实现，而不用过多关注 SQL 语句的编写和数据库的细节操作</li>
<li><strong>数据的增删改查：</strong><ol>
<li><strong>增：</strong>1、使用**create()<strong>方法创建一个对象，这个对象会自动调用</strong>execute()<strong>方法保存到数据库中。2、使用实例化对象并调用</strong>save()**方法保存到数据库中，那么在调用save()方法之前，对象并没有被保存到数据库中；如果要保存的对象已经存在于数据库中，则save()方法会更新该对象，而不是新增一个对象</li>
<li><strong>删：使用update()修改数据，返回被删除的行数</strong></li>
<li><strong>改：使用delete()删除数据，返回被删除的行数</strong></li>
<li><strong>查：使用selete()查询数据，返回SelectQuery对象，需要通过execute()方法执行查询并获取结果</strong></li>
<li><strong>execute()：是peewee中的一个方法，用于执行SQL语句并返回受影响的行数</strong></li>
</ol>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">from peewee import *</span><br><span class="line"></span><br><span class="line"># 连接MySQL数据库</span><br><span class="line">db = MySQLDatabase(&#x27;spider&#x27;, user=&#x27;root&#x27;, password=&#x27;123456&#x27;, host=&#x27;localhost&#x27;, port=3306)</span><br><span class="line"></span><br><span class="line"># 定义模型类</span><br><span class="line">class User(Model):</span><br><span class="line">    username = CharField()</span><br><span class="line">    password = CharField()</span><br><span class="line"></span><br><span class="line">    class Meta:</span><br><span class="line">        database = db  # 指定使用的数据库</span><br><span class="line">        table_name = &#x27;user&#x27;  # 指定对应的表名</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # 创建表</span><br><span class="line">    db.create_tables([User])</span><br><span class="line"></span><br><span class="line">    # 插入数据</span><br><span class="line">    user = User.create(username=&#x27;wcf&#x27;, password=&#x27;123456&#x27;)</span><br><span class="line">    print(user)</span><br><span class="line"></span><br><span class="line">    # 查询所有用户</span><br><span class="line">    users = User.select().where(User.username == &#x27;00jack&#x27;)</span><br><span class="line">    print(users)</span><br><span class="line"></span><br><span class="line">    # 查询用户名为&#x27;Alice&#x27;的用户</span><br><span class="line">    users = User.select().where(User.username == &#x27;Alice&#x27;)</span><br><span class="line"></span><br><span class="line">    # 更新用户名为&#x27;Alice&#x27;的用户的密码</span><br><span class="line">    User.update(password=&#x27;654321&#x27;).where(User.username == &#x27;Alice&#x27;).execute()</span><br><span class="line"></span><br><span class="line">    # 删除用户名为&#x27;Alice&#x27;的用户</span><br><span class="line">    User.delete().where(User.username == &#x27;Alice&#x27;).execute()</span><br></pre></td></tr></table></figure>

<h3 id="5-MongDB数据库存储"><a href="#5-MongDB数据库存储" class="headerlink" title="5. MongDB数据库存储"></a>5. MongDB数据库存储</h3><blockquote>
<p><strong>在pyhton中，可以使用PyMongo直接将字典数据插入到MongoDB数据库中</strong></p>
<p><strong>原理：</strong>MongoDB文档是一种使用类似 JSON 的 BSON（Binary JSON）格式表示的数据结构，而字典在Python中与JSON结构非常相似，因此字典数据可以直接映射为MongoDB文档</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">class AiQiYi:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.url = &quot;https://pcw-api.iqiyi.com/search/recommend/list&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line">        self.mongo_cli = pymongo.MongoClient(host=&#x27;localhost&#x27;, port=27017)</span><br><span class="line"></span><br><span class="line">    # 获取数据</span><br><span class="line">    def get_data(self, params):</span><br><span class="line">        # 这里不使用yield返回数据是因为没有循环，构建params参数的实现放在了main方法中</span><br><span class="line">        data = requests.get(url=self.url, headers=self.headers, params=params).json()</span><br><span class="line">        return data</span><br><span class="line"></span><br><span class="line">    # 解析数据</span><br><span class="line">    def parse_data(self, data):</span><br><span class="line">        for item in data[&#x27;data&#x27;][&#x27;list&#x27;]:</span><br><span class="line">            info = dict()</span><br><span class="line">            info[&#x27;title&#x27;] = item[&#x27;title&#x27;]</span><br><span class="line">            info[&#x27;score&#x27;] = item[&#x27;score&#x27;]</span><br><span class="line">            info[&#x27;description&#x27;] = item[&#x27;description&#x27;]</span><br><span class="line"></span><br><span class="line">            # 将这条数据保存到mongdb中</span><br><span class="line">            try:</span><br><span class="line">                self.save_data(info)</span><br><span class="line">                print(&quot;保存成功...&quot;)</span><br><span class="line">            except Exception as e:</span><br><span class="line">                print(f&quot;保存失败：&#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    # 保存数据</span><br><span class="line">    def save_data(self, info):</span><br><span class="line">        self.mongo_cli[&#x27;AiQiYi&#x27;][&#x27;movie_info&#x27;].insert_one(info)</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        for page in range(1, 6):</span><br><span class="line">            params = &#123;</span><br><span class="line">                &quot;channel_id&quot;: &quot;2&quot;,</span><br><span class="line">                &quot;data_type&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;mode&quot;: &quot;11&quot;,</span><br><span class="line">                &quot;page_id&quot;: page,</span><br><span class="line">                &quot;ret_num&quot;: &quot;48&quot;,</span><br><span class="line">                &quot;session&quot;: &quot;2ead98f78a643719307ff1643f591f59&quot;,</span><br><span class="line">                &quot;three_category_id&quot;: &quot;15;must&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            data = self.get_data(params)</span><br><span class="line">            self.parse_data(data)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    aqy = AiQiYi()</span><br><span class="line">    aqy.main()</span><br></pre></td></tr></table></figure>

<h3 id="6-md5数据去重"><a href="#6-md5数据去重" class="headerlink" title="6. md5数据去重"></a>6. md5数据去重</h3><blockquote>
<p><strong>MD5哈希函数</strong>：是一种不可逆的哈希函数，用于产生固定长度的散列值，通常是128位的16进制数（对于相同的源数据，无论何时计算，其产生的 MD5 哈希值都是固定的、不变的）</p>
<p><strong>不可逆特性：</strong>不可逆特性是指通过 MD5 算法产生的摘要（哈希值）不能被逆向还原为原始输入数据</p>
<p><strong>数据完整性验证：</strong>在某些应用中，需要确保存储的数据在后续读取时没有被篡改。计算数据的MD5哈希并将其存储在数据库中，然后在读取时再次计算哈希并与存储的哈希进行比较，以验证数据的完整性</p>
</blockquote>
<ul>
<li><strong>思路分析：</strong>将获取的数据存储到 mongodb 数据库中，在调用保存方法之前，先调用md5哈希方法将数据转为哈希值并保存到 redis 中，再判断当前获取的数据的哈希值是否已存在于 redis 数据库，不存在则保存</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">import redis</span><br><span class="line">import pymongo</span><br><span class="line">import hashlib</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">class MovieInfo:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.url = &quot;https://pianku.api.mgtv.com/rider/list/pcweb/v3&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line">        self.mongo_cli = pymongo.MongoClient()</span><br><span class="line">        self.redis_cli = redis.Redis()</span><br><span class="line"></span><br><span class="line">    # 请求数据</span><br><span class="line">    def get_data(self, params):</span><br><span class="line">        response = requests.get(self.url, headers=self.headers, params=params).json()</span><br><span class="line">        return response</span><br><span class="line"></span><br><span class="line">    # 数据清洗以及数据结构调整</span><br><span class="line">    def parse_data(self, response):</span><br><span class="line">        for data in response[&#x27;data&#x27;][&#x27;hitDocs&#x27;]:</span><br><span class="line">            info = dict()</span><br><span class="line">            info[&#x27;title&#x27;] = data[&quot;title&quot;]</span><br><span class="line">            info[&#x27;subtitle&#x27;] = data[&quot;subtitle&quot;]</span><br><span class="line">            info[&#x27;story&#x27;] = data[&quot;story&quot;]</span><br><span class="line"></span><br><span class="line">            # 在数据清洗之后保存数据</span><br><span class="line">            self.save_data(info)</span><br><span class="line"></span><br><span class="line">    # 数据去重，主要是为了多线程或协程服务的，当线程对象过多时可能会导致保存数据重复</span><br><span class="line">    @staticmethod</span><br><span class="line">    def get_md5(item):</span><br><span class="line">        # md5方法只能接收字节数据，计算哈希值, 哈希值是唯一的</span><br><span class="line">        return hashlib.md5(str(item).encode(&#x27;utf-8&#x27;)).hexdigest()</span><br><span class="line"></span><br><span class="line">    # 数据保存，在保存数据前进行数据重复判断，即数据去重</span><br><span class="line">    def save_data(self, data):</span><br><span class="line">        md5_data = self.get_md5(data)</span><br><span class="line">        # 当前集合的key如果有冒号，redis会在内部创建一个文件夹</span><br><span class="line">        result = self.redis_cli.sadd(&#x27;movie:folder&#x27;, md5_data)  # 返回1或0</span><br><span class="line">        if result:</span><br><span class="line">            self.mongo_cli[&#x27;MGTV&#x27;][&#x27;movie&#x27;].insert_one(data)</span><br><span class="line">            print(&quot;数据保存成功&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;数据重复...&quot;)</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        for page in range(1, 5):</span><br><span class="line">            params = &#123;</span><br><span class="line">                &quot;allowedRC&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;platform&quot;: &quot;pcweb&quot;,</span><br><span class="line">                &quot;channelId&quot;: &quot;2&quot;,</span><br><span class="line">                &quot;pn&quot;: page,</span><br><span class="line">                &quot;pc&quot;: &quot;80&quot;,</span><br><span class="line">                &quot;hudong&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;_support&quot;: &quot;10000000&quot;,</span><br><span class="line">                &quot;kind&quot;: &quot;19&quot;,</span><br><span class="line">                &quot;area&quot;: &quot;10&quot;,</span><br><span class="line">                &quot;year&quot;: &quot;all&quot;,</span><br><span class="line">                &quot;chargeInfo&quot;: &quot;a1&quot;,</span><br><span class="line">                &quot;sort&quot;: &quot;c2&quot;,</span><br><span class="line">                &quot;feature&quot;: &quot;all&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            data = self.get_data(params)</span><br><span class="line">            self.parse_data(data)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    mgtv = MovieInfo()</span><br><span class="line">    mgtv.main()</span><br></pre></td></tr></table></figure>

<h2 id="六、并发爬虫"><a href="#六、并发爬虫" class="headerlink" title="六、并发爬虫"></a>六、并发爬虫</h2><blockquote>
<p>当使用异步关键词 async&#x2F;await 进行开发时，需要利用各种异步操作来执行不同类型的任务：</p>
<ul>
<li><strong>异步网络请求：</strong>使用 <strong>aiohttp</strong> 库代替同步的 requests 库来执行异步的网络请求操作</li>
<li><strong>异步MySQL数据库操作：</strong>使用 <strong>aiomysql</strong> 库代替同步的 pymysql 库来操作 mysql 数据库</li>
<li><strong>异步MongoDB数据库操作：</strong>使用 <strong>motor</strong> 库代替同步的 pymongo 库来操作 mongodb 数据库</li>
<li><strong>Redis数据库操作：</strong>Redis 本身是异步的</li>
</ul>
</blockquote>
<h3 id="1-协程配合requests"><a href="#1-协程配合requests" class="headerlink" title="1. 协程配合requests"></a>1. 协程配合requests</h3><ul>
<li>回顾并利用之前学习的协程来完成爬虫并发任务</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import asyncio</span><br><span class="line">import requests</span><br><span class="line">from functools import partial</span><br><span class="line"></span><br><span class="line">url = &#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">async def get_movie_info(page):</span><br><span class="line">    loop = asyncio.get_running_loop()</span><br><span class="line">    # run_in_executor不支持关键字参数传递headers, 可以使用lambda包装，也可以使用偏函数传递</span><br><span class="line">    # lambda函数在这里的作用是将请求操作包装成一个可调用对象，相当于直接return这个requests.get方法</span><br><span class="line">    # response = await loop.run_in_executor(None, lambda: requests.get(url.format(page * 25), headers=headers))</span><br><span class="line">    response = await loop.run_in_executor(None, partial(requests.get, url.format(page * 25), headers=headers))</span><br><span class="line">    print(response.text)</span><br><span class="line"></span><br><span class="line">async def main():</span><br><span class="line">    tasks = [asyncio.create_task(get_movie_info(page)) for page in range(5)]</span><br><span class="line">    await asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # 异步随机调度会导致输出的数据不是顺序输出</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure>

<h3 id="2-aiohttp代替requests"><a href="#2-aiohttp代替requests" class="headerlink" title="2. aiohttp代替requests"></a>2. aiohttp代替requests</h3><blockquote>
<p><strong>问题：</strong>由于 requests 爬虫库本身不支持异步，在 asyncio 中需要 run_in_executor 开启线程池才能异步</p>
<p><strong>优化：使用 aiohttp 库的【客户端】代替同步的 requests 库来执行异步的网络请求操作</strong></p>
</blockquote>
<ul>
<li><strong>aiohttp：</strong>是一个基于异步IO的 HTTP 客户端和服务器框架，适用于 Python 的 asyncio 库（<a class="link"   href="https://docs.aiohttp.org/en/stable/"  target="_blank" rel="noopener">官方文档<i class="fas fa-external-link-alt"></i></a>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import aiohttp</span><br><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">url = &quot;https://www.baidu.com&quot;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">async def get_data():</span><br><span class="line">    # 创建一个异步http客户端会话（session），通过async with异步上下文管理器管理会话资源</span><br><span class="line">    async with aiohttp.ClientSession() as session:</span><br><span class="line">        # 使用session会话发起get请求，同样请求和响应也是需要关闭的</span><br><span class="line">        async with session.get(url, headers=headers, ssl=False) as resp:</span><br><span class="line">            print(resp.status)</span><br><span class="line">            # text()被定义为协程函数：async def text()，所以需要await等待</span><br><span class="line">            print(await resp.text())</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    asyncio.run(get_data())</span><br></pre></td></tr></table></figure>

<h3 id="3-aiomysql的使用"><a href="#3-aiomysql的使用" class="headerlink" title="3. aiomysql的使用"></a>3. aiomysql的使用</h3><ul>
<li><strong>aiomysql：</strong>是一个基于 asyncio 的 MySQL 异步驱动，用于在异步应用程序中执行 MySQL 数据库操作</li>
<li><strong>异步连接池的作用：</strong><ul>
<li><strong>资源复用：</strong>连接池通过提供一个池化的连接资源集合，减少了每次数据库操作都需要新建连接的开销。每个协程可以从连接池中获取一个可用的连接，使用完毕后将其放回池中，以便其他协程继续使用。这有助于降低连接的创建和销毁成本</li>
<li><strong>并发性：</strong>异步连接池允许多个协程同时从池中获取连接，执行数据库操作，而无需等待</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import aiomysql</span><br><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">async def conn_mysql():</span><br><span class="line">    # 创建一个异步连接池，可用于管理多个异步连接，从池中获取连接执行查询，最后关闭连接池</span><br><span class="line">    async with aiomysql.create_pool(user=&#x27;root&#x27;, password=&#x27;123456&#x27;, db=&#x27;py_spider&#x27;) as pool:</span><br><span class="line">        # 从连接池中获取连接</span><br><span class="line">        async with pool.acquire() as conn:</span><br><span class="line">            # 使用连接创建异步游标，用于执行sql查询</span><br><span class="line">            async with conn.cursor() as cursor:</span><br><span class="line">                # 执行sql语句</span><br><span class="line">                await cursor.execute(&#x27;select * from tx_work;&#x27;)</span><br><span class="line">                # 获取查询结果的第一行</span><br><span class="line">                result = await cursor.fetchone()  # 返回的是一个Future对象</span><br><span class="line">                print(result)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    asyncio.run(conn_mysql())</span><br></pre></td></tr></table></figure>

<h3 id="4-汽车之家数据采集"><a href="#4-汽车之家数据采集" class="headerlink" title="4. 汽车之家数据采集"></a>4. 汽车之家数据采集</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">import redis</span><br><span class="line">import re</span><br><span class="line">import hashlib</span><br><span class="line">import chardet</span><br><span class="line">import asyncio</span><br><span class="line">import aiohttp</span><br><span class="line">import aiomysql</span><br><span class="line"></span><br><span class="line">class CarSpider:</span><br><span class="line">    redis_cli = redis.Redis()</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.url = &quot;https://www.che168.com/china/a0_0msdgscncgpi1ltocsp&#123;&#125;exf4x0/?pvareaid=102179#currengpostion&quot;</span><br><span class="line">        self.api_url = &quot;https://cacheapigo.che168.com/CarProduct/GetParam.ashx?specid=&#123;&#125;&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    # 获取页面中汽车的id</span><br><span class="line">    async def get_car_id(self, page, session, pool):</span><br><span class="line">        async with session.get(self.url.format(page), headers=self.headers, ssl=False) as response:</span><br><span class="line">            # aiohttp中，需要使用read()而不是content，获得响应的二进制内容（bytes类型）</span><br><span class="line">            content = await response.read()</span><br><span class="line">            # 由于汽车之家每一次请求返回的页面编码格式都可能不一样，并且错误的页面还无法获取到需要的数据</span><br><span class="line">            # 所以需要使用第三方库chardet识别页面的编码格式，因为自己去获取出的可能是别人伪装过的编码格式</span><br><span class="line">            encoding = chardet.detect(content)[&quot;encoding&quot;]</span><br><span class="line">            if encoding == &quot;GB2312&quot;:</span><br><span class="line">                data = content.decode(&quot;gbk&quot;)  # 编码格式为 gb2312 或者 iso-8859-1，均可用gbk解码</span><br><span class="line">                print(f&quot;第&#123;page&#125;次数据获取成功...&quot;)</span><br><span class="line">                # 使用正则表达式匹配文档中所有car_id，返回一个列表</span><br><span class="line">                car_id_list = re.findall(r&#x27;&lt;li.*?specid=&quot;(.*?)&quot;.*?&lt;/li&gt;&#x27;, data)</span><br><span class="line">                # 通过每一个汽车id获取其对应的api数据</span><br><span class="line">                tasks = [asyncio.create_task(self.get_car_info(car_id, session, pool)) for car_id in car_id_list]</span><br><span class="line">                await asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line">    # 通过汽车id获取api接口的数据，并清洗数据</span><br><span class="line">    async def get_car_info(self, spec_id, session, pool):</span><br><span class="line">        async with session.get(self.api_url.format(spec_id), headers=self.headers, ssl=False) as resp:</span><br><span class="line">            result = await resp.json()</span><br><span class="line">            # 返回的json数据中存在paramtypeitems这个key的值不存在的情况</span><br><span class="line">            if result[&#x27;result&#x27;].get(&#x27;paramtypeitems&#x27;):</span><br><span class="line">                info = dict()</span><br><span class="line">                info[&#x27;name&#x27;] = result[&#x27;result&#x27;][&quot;paramtypeitems&quot;][0][&quot;paramitems&quot;][0][&#x27;value&#x27;]</span><br><span class="line">                info[&#x27;price&#x27;] = result[&#x27;result&#x27;][&quot;paramtypeitems&quot;][0][&quot;paramitems&quot;][1][&#x27;value&#x27;]</span><br><span class="line">                info[&#x27;brand&#x27;] = result[&#x27;result&#x27;][&quot;paramtypeitems&quot;][0][&quot;paramitems&quot;][2][&#x27;value&#x27;]</span><br><span class="line"></span><br><span class="line">                # 调度保存任务，由于此处只有一个任务，所以是无法并发执行的，可以直接使用await直接将任务提交到事件循环</span><br><span class="line">                await self.save_car_info(info, pool)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def get_md5(info):</span><br><span class="line">        return hashlib.md5(str(info).encode(&quot;utf-8&quot;)).hexdigest()</span><br><span class="line"></span><br><span class="line">    # 将数据保存到mysql中，保存前使用redis配合md5完成数据去重</span><br><span class="line">    async def save_car_info(self, info, pool):</span><br><span class="line">        # 这里体现传递pool连接池的作用，因为有多个协程都需要保存数据，所以每个协程可以从连接池中获取一个可用的连接，</span><br><span class="line">        # 使用完毕后将其放回池中，以便其他协程继续使用。这减少了每次数据库操作都需要新建连接的开销</span><br><span class="line">        async with pool.acquire() as conn:</span><br><span class="line">            async with conn.cursor() as cursor:</span><br><span class="line">                sql = &quot;&quot;&quot;</span><br><span class="line">                    insert into car_info values(%s, %s, %s, %s);</span><br><span class="line">                &quot;&quot;&quot;</span><br><span class="line">                info_md5 = self.get_md5(info)</span><br><span class="line">                # 判断能否存入redis中，利用集合的不可重复的，成功则返回1，失败返回0</span><br><span class="line">                if self.redis_cli.sadd(&quot;car_info&quot;, info_md5):</span><br><span class="line">                    try:</span><br><span class="line">                        # cursor.execute()传参数时，第二个参数需要为元组或列表，所以使用tuple(info.values())构造元组</span><br><span class="line">                        await cursor.execute(sql, (0, *tuple(info.values())))</span><br><span class="line">                        await conn.commit()</span><br><span class="line">                        print(&quot;数据保存成功...&quot;)</span><br><span class="line">                    except Exception as e:</span><br><span class="line">                        await conn.rollback()</span><br><span class="line">                        print(&quot;数据保存失败：&quot;, e)</span><br><span class="line">                else:</span><br><span class="line">                    print(&quot;数据重复!&quot;)</span><br><span class="line"></span><br><span class="line">    async def main(self):</span><br><span class="line">        async with aiomysql.create_pool(user=&#x27;root&#x27;, password=&#x27;123456&#x27;, db=&#x27;py_spider&#x27;) as pool:</span><br><span class="line">            async with pool.acquire() as conn:</span><br><span class="line">                async with conn.cursor() as cursor:</span><br><span class="line">                    # 创建表</span><br><span class="line">                    sql = &quot;&quot;&quot;</span><br><span class="line">                        create table car_info(</span><br><span class="line">                            id int primary key auto_increment,</span><br><span class="line">                            name varchar(100),</span><br><span class="line">                            price varchar(100),</span><br><span class="line">                            brand varchar(100)</span><br><span class="line">                        );</span><br><span class="line">                    &quot;&quot;&quot;</span><br><span class="line">                    # 检查表是否存在，cursor.execute()方法存在则返回1，不存在则返回0</span><br><span class="line">                    if not await cursor.execute(&#x27;show tables like &quot;car_info&quot;;&#x27;):</span><br><span class="line">                        await cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">            async with aiohttp.ClientSession() as session:</span><br><span class="line">                # 通过分析发现，不管第几页，页面每次刷新所返回的数据都不一样，所以不同的page相当于只是区分又发起了一次请求而已</span><br><span class="line">                # 异步发起15次请求，获取汽车的id数据，其中可能只能成功几次，所以这种网站一般是需要24*7不间断爬取的</span><br><span class="line">                tasks = [asyncio.create_task(self.get_car_id(page, session, pool)) for page in range(1, 16)]</span><br><span class="line">                await asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    car = CarSpider()</span><br><span class="line">    asyncio.run(car.main())</span><br></pre></td></tr></table></figure>

<h3 id="5-JoinableQueue队列"><a href="#5-JoinableQueue队列" class="headerlink" title="5. JoinableQueue队列"></a>5. JoinableQueue队列</h3><ul>
<li><p><strong>JoinableQueue：</strong>是 multiprocessing 模块中的一种队列类型，<strong>它是 Queue 的子类</strong>，其主要用于在<strong>多进程</strong>编程中进行任务的协同。它允许一个进程放入任务，而另一个进程则从队列中取出任务。</p>
</li>
<li><p><strong>put(item[, block[, timeout]])：将项目放入队列</strong></p>
<ul>
<li><strong>参数item</strong>：即要放入队列的任务</li>
<li><strong>参数block</strong>：控制在队列已满时的行为，为布尔值。为True时，在队列有空间之前会阻塞；为False时，会立即引发 Full 异常</li>
<li><strong>参数timeout</strong>：表示阻塞的最长时间，如果在指定时间内队列仍然是满的，将引发 Full 异常</li>
</ul>
</li>
<li><p><strong>get([block[, timeout]])：从队列中取出一个项目</strong></p>
<ul>
<li><strong>参数block</strong>：控制在队列为空时的行为，为布尔值。为True时，在队列不为空之前会阻塞；为False时，会立即引发 Empty 异常</li>
<li><strong>参数timeout</strong>：表示阻塞的最长时间，如果在指定时间内队列仍然是空的，将引发 Empty 异常</li>
</ul>
</li>
<li><p><strong>task_done()：用于表示之前放入队列的任务已经完成</strong></p>
<ul>
<li><strong>解释：</strong>JoinableQueue中存在一个队列计数器，每调用 put() 后，则计数器 +1，每调用 get() 后，表示对应的任务已经被处理完毕，则需要调用 task_done() 表示此任务已经完成，且计数器 -1</li>
</ul>
</li>
<li><p><strong>join()：阻塞，直到队列中的所有任务都被处理并调用了 task_done()</strong></p>
<ul>
<li><strong>解释：</strong>阻塞主进程，等待工作进程执行完所有任务再继续执行。即当队列中所有任务都调用了 get() 和 task_done() 后，表示队列中的所有任务都已经完成了，此时队列计数器为零，主进程则不再阻塞</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># https://careers.tencent.com/search.html?keyword=python</span><br><span class="line">from multiprocessing import Process</span><br><span class="line">from multiprocessing import JoinableQueue</span><br><span class="line">import requests</span><br><span class="line">import time</span><br><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">url = &quot;https://careers.tencent.com/tencentcareer/api/post/Query&quot;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def get_data(page, queue):</span><br><span class="line">    params = &#123;</span><br><span class="line">        &quot;timestamp&quot;: &quot;1700122896844&quot;,</span><br><span class="line">        &quot;countryId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;cityId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;bgIds&quot;: &quot;&quot;,</span><br><span class="line">        &quot;productId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;categoryId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;parentCategoryId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;attrId&quot;: &quot;&quot;,</span><br><span class="line">        &quot;keyword&quot;: &quot;python&quot;,</span><br><span class="line">        &quot;pageIndex&quot;: page,</span><br><span class="line">        &quot;pageSize&quot;: &quot;10&quot;,</span><br><span class="line">        &quot;language&quot;: &quot;zh-cn&quot;,</span><br><span class="line">        &quot;area&quot;: &quot;cn&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, headers=headers, params=params).json()</span><br><span class="line">    for item in response[&quot;Data&quot;][&quot;Posts&quot;]:</span><br><span class="line">        info = dict()</span><br><span class="line">        info[&quot;CountryName&quot;] = item[&quot;CountryName&quot;]</span><br><span class="line">        info[&quot;RecruitPostName&quot;] = item[&quot;RecruitPostName&quot;]</span><br><span class="line">        info[&quot;Responsibility&quot;] = item[&quot;Responsibility&quot;]</span><br><span class="line">        queue.put(info)</span><br><span class="line"></span><br><span class="line">def save_data(queue):</span><br><span class="line">    mongo_cli = pymongo.MongoClient()</span><br><span class="line">    db = mongo_cli[&quot;tencent&quot;][&quot;tx_work&quot;]</span><br><span class="line">    while True:</span><br><span class="line">        info = queue.get()</span><br><span class="line">        db.insert_one(info)</span><br><span class="line">        queue.task_done()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    queue = JoinableQueue()</span><br><span class="line">    # 创建并添加子进程到列表，所有子进程都已经启动后，主进程再等待它们完成，实现并行</span><br><span class="line">    processes = []</span><br><span class="line">    # 使用两个进程，一个用于获取数据，另一个用于保存数据到MongoDB</span><br><span class="line">    # 这样在获取数据的同时，可以在另一个进程中保存数据，从而实现并行处理</span><br><span class="line">    for page in range(1, 11):</span><br><span class="line">        process = Process(target=get_data, args=(page, queue))</span><br><span class="line">        processes.append(process)</span><br><span class="line"></span><br><span class="line">    processes.append(Process(target=save_data, args=(queue,)))</span><br><span class="line"></span><br><span class="line">    for item in processes:</span><br><span class="line">        item.daemon = True  # 设置守护进程</span><br><span class="line">        item.start()</span><br><span class="line"></span><br><span class="line">    time.sleep(1)  # 让操作系统有足够的时间来启动进程</span><br><span class="line">    # 由于队列计数器默认为0，所以需要给启动进程一点时间，否则计数器还没来得及调用put方法+1，</span><br><span class="line">    # queue.join()中也就没有需要完成的任务，也就不会阻塞主进程，这样整个程序就直接结束了</span><br><span class="line"></span><br><span class="line">    queue.join()  # 阻塞主进程，直到队列中的所有任务完成后解堵塞</span><br><span class="line">    # 解堵塞后，整个程序按理说就应该执行完毕了，但是因为子进程save_data方法中是一个死循环，</span><br><span class="line">    # 它不能自己退出，所以需要设置守护进程，当主进程解堵塞后，子进程就随着主进程的退出而退出</span><br></pre></td></tr></table></figure>

<h2 id="七、综合并发爬虫案例"><a href="#七、综合并发爬虫案例" class="headerlink" title="七、综合并发爬虫案例"></a>七、综合并发爬虫案例</h2><blockquote>
<p><strong>队列在多线程编程中传输数据的好处：</strong></p>
<ol>
<li><strong>线程安全：</strong> <strong>队列提供了线程安全的数据结构，确保在多线程环境中数据传输的安全性</strong>。队列的实现通常会使用锁机制，以防止多个线程同时修改队列数据导致的竞态条件</li>
<li><strong>解耦和协作：</strong> <strong>队列允许不同的线程在不直接依赖于对方的内部实现的情况下进行协作</strong>。通过将数据放入队列，一个线程可以向其他线程传递信息，而无需了解接收线程的具体实现细节</li>
<li><strong>异步处理：</strong> <strong>队列可以用于实现异步处理</strong>。一个线程可以将任务放入队列，而不必等待任务完成，从而允许线程继续执行其他任务。接收任务的线程可以异步处理队列中的任务</li>
<li><strong>防止资源竞争：</strong> 在多线程环境中，如果多个线程同时访问和修改共享的数据结构，可能会导致竞争条件。<strong>通过使用队列，可以减少对共享资源的直接访问，降低了出现竞争条件的可能性</strong></li>
<li><strong>平衡生产者和消费者速度：</strong> 如果一个线程生成数据的速度快于另一个线程处理数据的速度，<strong>使用队列可以平衡生产者和消费者之间的速度差异</strong>。队列充当了缓冲区，使得生产者和消费者可以在各自的速度下独立操作</li>
<li><strong>可扩展性：</strong> 使用队列可以方便地扩展程序，因为你可以很容易地添加或删除处理特定任务的线程，而不必修改其他部分的代码</li>
</ol>
</blockquote>
<h3 id="1-多线程实现并发爬虫"><a href="#1-多线程实现并发爬虫" class="headerlink" title="1. 多线程实现并发爬虫"></a><strong>1. 多线程实现并发爬虫</strong></h3><ul>
<li>案例目标：使用多线程完成并发爬虫，通过队列进行数据传递，最后将数据存入mongodb数据库</li>
</ul>
<ul>
<li><strong>独立线程：</strong>每个函数 <strong>get_api_url</strong>、<strong>get_api_json</strong>、<strong>parse_movie_info</strong> 和 <strong>save_movie_info</strong> 在独立的线程中运行</li>
<li><strong>数据传递：</strong>使用队列 <strong>url_queue</strong>、<strong>json_queue</strong> 和 <strong>data_queue</strong> 进行数据传递，明确控制线程之间的通信</li>
<li><strong>线程控制：</strong>提供更细粒度的线程控制，可以手动管理线程的启动和关闭，适用于需要对每个线程进行精细调整的场景</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"># https://list.iqiyi.com/www/2/15-------------11-1-1-iqiyi--.html?s_source=PCW_SC</span><br><span class="line">import requests</span><br><span class="line">import pymongo</span><br><span class="line">from threading import Thread</span><br><span class="line">from queue import Queue</span><br><span class="line"></span><br><span class="line">class AiQiYi:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.api_url = &quot;https://pcw-api.iqiyi.com/search/recommend/list?channel_id=2&amp;data_type=1&amp;mode=11&amp;page_id=&#123;&#125;&amp;ret_num=48&amp;session=fcfd98ff68e5d561762262c1ece3fb7e&amp;three_category_id=15;must&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">            &#x27;Referer&#x27;: &#x27;https://list.iqiyi.com/www/2/15-------------11-1-1-iqiyi--.html?s_source=PCW_SC&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">        self.mongo_cli = pymongo.MongoClient()</span><br><span class="line">        self.collection = self.mongo_cli[&quot;AiQiYi&quot;][&quot;movie_info&quot;]</span><br><span class="line"></span><br><span class="line">        # 创建几个用于线程间传递数据的队列</span><br><span class="line">        self.url_queue = Queue()</span><br><span class="line">        self.json_queue = Queue()</span><br><span class="line">        self.data_queue = Queue()</span><br><span class="line"></span><br><span class="line">    def get_api_url(self):</span><br><span class="line">        for i in range(1, 6):</span><br><span class="line">            self.url_queue.put(self.api_url.format(i))</span><br><span class="line"></span><br><span class="line">    def get_api_json(self):</span><br><span class="line">        while True:</span><br><span class="line">            url = self.url_queue.get()</span><br><span class="line">            response = requests.get(url, headers=self.headers)</span><br><span class="line">            self.json_queue.put(response.json())</span><br><span class="line">            self.url_queue.task_done()</span><br><span class="line"></span><br><span class="line">    def parse_movie_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            json = self.json_queue.get()</span><br><span class="line">            for item in json[&quot;data&quot;][&quot;list&quot;]:</span><br><span class="line">                info = dict()  # 字典要定义在循环内，因为每循环一次都要存入一条新数据</span><br><span class="line">                info[&quot;name&quot;] = item[&quot;name&quot;]</span><br><span class="line">                info[&quot;playUrl&quot;] = item[&quot;playUrl&quot;]</span><br><span class="line">                info[&quot;description&quot;] = item[&quot;description&quot;]</span><br><span class="line">                self.data_queue.put(info)</span><br><span class="line">            self.json_queue.task_done()</span><br><span class="line"></span><br><span class="line">    def save_movie_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            data = self.data_queue.get()</span><br><span class="line">            self.collection.insert_one(data)</span><br><span class="line">            self.data_queue.task_done()</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        # 初始化线程对象列表</span><br><span class="line">        thread_list = list()</span><br><span class="line"></span><br><span class="line">        # 创建获取url的线程对象并加入到线程对象列表中，1个线程</span><br><span class="line">        thread_list.append(Thread(target=self.get_api_url))</span><br><span class="line"></span><br><span class="line">        # 创建发送请求的线程对象并加入到线程对象列表中，3个线程</span><br><span class="line">        for _ in range(3):</span><br><span class="line">            thread_list.append(Thread(target=self.get_api_json))</span><br><span class="line"></span><br><span class="line">        # 创建数据清洗的线程对象并加入到线程对象列表中，1个线程</span><br><span class="line">        thread_list.append(Thread(target=self.parse_movie_info))</span><br><span class="line"></span><br><span class="line">        # 创建保存数据的线程对象并加入到线程对象列表中，1个线程</span><br><span class="line">        thread_list.append(Thread(target=self.save_movie_info))</span><br><span class="line"></span><br><span class="line">        # 循环线程列表设置线程对象为守护线程并启动</span><br><span class="line">        for t_obj in thread_list:</span><br><span class="line">            t_obj.daemon = True  # 设置守护线程</span><br><span class="line">            t_obj.start()</span><br><span class="line"></span><br><span class="line">        # 阻塞主进程，直到队列中的所有任务完成后解堵塞，即等待所有队列中的计数器减为零</span><br><span class="line">        for q_obj in [self.url_queue, self.json_queue, self.data_queue]:</span><br><span class="line">            q_obj.join()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    aiqiyi = AiQiYi()</span><br><span class="line">    aiqiyi.main()</span><br></pre></td></tr></table></figure>

<h3 id="2-线程池实现并发爬虫"><a href="#2-线程池实现并发爬虫" class="headerlink" title="2. 线程池实现并发爬虫"></a>2. <strong>线程池实现并发爬虫</strong></h3><ul>
<li>案例目标：使用线程池完成并发爬虫，通过 Future 对象异步获取任务的结果，最后将数据存入mysql数据库</li>
</ul>
<ul>
<li><strong>线程控制：</strong>提供更高级的线程管理，可以更好地管理线程的生命周期，减少了线程创建和销毁的开销，适用于大量短时任务的场景</li>
<li><strong>数据传递：</strong>通过 Future 对象异步获取任务的结果，线程间的数据传递更为抽象，不直接使用队列</li>
<li><strong>API抽象程度：</strong>提供更高级别、更抽象的API，减少了对线程和队列的直接管理，使得程序员更专注于业务逻辑，降低了操作线程和队列的复杂性</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"># https://talent.baidu.com/jobs/social-list?search=python</span><br><span class="line">import requests</span><br><span class="line">import pymysql</span><br><span class="line">from concurrent.futures import ThreadPoolExecutor, as_completed</span><br><span class="line"></span><br><span class="line">class BaiduSpider:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.api_url = &quot;https://talent.baidu.com/httservice/getPostListNew&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">            &#x27;cookie&#x27;: &#x27;BIDUPSID=60FBCA8DBF4F3E49D29E6D84287CBA53; PSTM=1688559896; BAIDUID=60FBCA8DBF4F3E499AAD874D85AABF9F:FG=1; H_WISE_SIDS=110085_268592_269832_269904_271169_271269_271812_272280_272461_273118_273300_273460_275940_269610_271120_276250_274502_275171_276419_276441_276572_276600_275589_276831_276985_276987_276964_270102_277313_277356_276264_277633_275188_275733; BDUSS=ItSERNVUVCSkE2ODRYbG5idEJEVm5kUjNFSDJvRy03SWlPYzhLUG16UzZvVTFsSVFBQUFBJCQAAAAAAAAAAAEAAAAfu5VDd2NmMTUzMjUyNzc0MzUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALoUJmW6FCZlY3; BDUSS_BFESS=ItSERNVUVCSkE2ODRYbG5idEJEVm5kUjNFSDJvRy03SWlPYzhLUG16UzZvVTFsSVFBQUFBJCQAAAAAAAAAAAEAAAAfu5VDd2NmMTUzMjUyNzc0MzUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALoUJmW6FCZlY3; H_WISE_SIDS_BFESS=110085_268592_269832_269904_271169_271269_271812_272280_272461_273118_273300_273460_275940_269610_271120_276250_274502_275171_276419_276441_276572_276600_275589_276831_276985_276987_276964_270102_277313_277356_276264_277633_275188_275733; BAIDUID_BFESS=60FBCA8DBF4F3E499AAD874D85AABF9F:FG=1; BDRCVFR[bLbo9QmdyQn]=mk3SLVN4HKm; delPer=0; PSINO=5; H_PS_PSSID=39634_39648_39671_39664_39682_39676_39678_39712_39733_39740_39758; ZFY=C1sztMdYGAwYdUxZ5HRR6vVXN3v8TQAhQMgFnDurHfA:C; Hm_lvt_50e85ccdd6c1e538eb1290bc92327926=1700214719; Hm_lpvt_50e85ccdd6c1e538eb1290bc92327926=1700215474; RT=&quot;z=1&amp;dm=baidu.com&amp;si=d38ba4f2-7dbd-44e1-b182-33ddf62dd746&amp;ss=lp2fy0ex&amp;sl=1&amp;tt=1a2&amp;bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&quot;&#x27;,</span><br><span class="line">            &quot;Referer&quot;: &quot;https://talent.baidu.com/jobs/social-list?search=python&quot;,</span><br><span class="line">        &#125;</span><br><span class="line">        self.db = pymysql.connect(user=&#x27;root&#x27;, password=&quot;123456&quot;, db=&quot;py_spider&quot;)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line"></span><br><span class="line">    def get_data(self, page):</span><br><span class="line">        data = &#123;</span><br><span class="line">            &quot;recruitType&quot;: &quot;SOCIAL&quot;,</span><br><span class="line">            &quot;pageSize&quot;: &quot;10&quot;,</span><br><span class="line">            &quot;keyWord&quot;: &quot;python&quot;,</span><br><span class="line">            &quot;curPage&quot;: page,</span><br><span class="line">            &quot;projectType&quot;: &quot;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        return requests.post(self.api_url, headers=self.headers, data=data).json()</span><br><span class="line"></span><br><span class="line">    def parse_date(self, json):</span><br><span class="line">        for item in json[&#x27;data&#x27;][&#x27;list&#x27;]:</span><br><span class="line">            name = item[&#x27;name&#x27;]</span><br><span class="line">            publish_date = item[&#x27;publishDate&#x27;]</span><br><span class="line">            service_condition = item[&#x27;serviceCondition&#x27;]</span><br><span class="line"></span><br><span class="line">            # 保存数据</span><br><span class="line">            self.save_data(name, publish_date, service_condition)</span><br><span class="line"></span><br><span class="line">    def create_table(self):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            create table if not exists job_info(</span><br><span class="line">                id int primary key auto_increment,</span><br><span class="line">                name varchar(100),</span><br><span class="line">                publish_date date,</span><br><span class="line">                service_condition text</span><br><span class="line">            );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            print(&#x27;表创建成功...&#x27;)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(&#x27;表创建失败:&#x27;, e)</span><br><span class="line"></span><br><span class="line">    def save_data(self, *args):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            insert into job_info values (%s, %s, %s, %s);</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql, (0, *args))</span><br><span class="line">            self.db.commit()</span><br><span class="line">            print(&quot;数据保存成功...&quot;)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(&quot;数据保存失败:&quot;, e)</span><br><span class="line">            self.db.rollback()</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        self.create_table()</span><br><span class="line">        with ThreadPoolExecutor(max_workers=5) as pool:</span><br><span class="line">            futures = [pool.submit(self.get_data, page) for page in range(1, 6)]</span><br><span class="line"></span><br><span class="line">            for item in as_completed(futures):</span><br><span class="line">                json = item.result()</span><br><span class="line">                self.parse_date(json)</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.cursor.close()</span><br><span class="line">        self.db.close()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    baidu_work = BaiduSpider()</span><br><span class="line">    baidu_work.main()</span><br></pre></td></tr></table></figure>

<h3 id="3-多进程实现并发爬虫"><a href="#3-多进程实现并发爬虫" class="headerlink" title="3. 多进程实现并发爬虫"></a>3. 多进程实现并发爬虫</h3><ul>
<li>案例目标：使用多进程完成并发爬虫，通过 JoinableQueue 队列进行数据传递，对数据去重并将数据存入 mongodb 数据库</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"># https://www.mgtv.com/lib/</span><br><span class="line">import redis</span><br><span class="line">import hashlib</span><br><span class="line">import pymongo</span><br><span class="line">import requests</span><br><span class="line">from multiprocessing import JoinableQueue, Process</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">class MgTv:</span><br><span class="line">    # 在多进程中，对于数据库初始化操作必须设置为类属性</span><br><span class="line">    # 目的是确保每个子进程都共享同一个数据库连接，而不是创建各自独立的数据库连接</span><br><span class="line">    redis_cli = redis.Redis()</span><br><span class="line">    mongo_cli = pymongo.MongoClient()</span><br><span class="line">    collection = mongo_cli[&#x27;py_spider&#x27;][&#x27;mg_movie&#x27;]</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.api_url = &quot;https://pianku.api.mgtv.com/rider/list/pcweb/v3&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        self.params_queue = JoinableQueue()</span><br><span class="line">        self.json_queue = JoinableQueue()</span><br><span class="line">        self.data_queue = JoinableQueue()</span><br><span class="line"></span><br><span class="line">    def get_params(self):</span><br><span class="line">        for page in range(1, 6):</span><br><span class="line">            params = &#123;</span><br><span class="line">                &quot;allowedRC&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;platform&quot;: &quot;pcweb&quot;,</span><br><span class="line">                &quot;channelId&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;pn&quot;: page,</span><br><span class="line">                &quot;pc&quot;: &quot;80&quot;,</span><br><span class="line">                &quot;hudong&quot;: &quot;1&quot;,</span><br><span class="line">                &quot;_support&quot;: &quot;10000000&quot;,</span><br><span class="line">                &quot;kind&quot;: &quot;a1&quot;,</span><br><span class="line">                &quot;chargeInfo&quot;: &quot;a1&quot;,</span><br><span class="line">                &quot;sort&quot;: &quot;c2&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            self.params_queue.put(params)</span><br><span class="line"></span><br><span class="line">    def get_api_json(self):</span><br><span class="line">        while True:</span><br><span class="line">            params = self.params_queue.get()</span><br><span class="line">            response = requests.get(self.api_url, headers=self.headers, params=params)</span><br><span class="line">            self.json_queue.put(response.json())</span><br><span class="line">            self.params_queue.task_done()</span><br><span class="line"></span><br><span class="line">    def parse_movie_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            json = self.json_queue.get()</span><br><span class="line">            for item in json[&#x27;data&#x27;][&#x27;hitDocs&#x27;]:</span><br><span class="line">                info = dict()</span><br><span class="line">                info[&#x27;title&#x27;] = item[&#x27;title&#x27;]</span><br><span class="line">                info[&#x27;updateInfo&#x27;] = item[&#x27;updateInfo&#x27;]</span><br><span class="line">                info[&#x27;views&#x27;] = item[&#x27;views&#x27;]</span><br><span class="line">                self.data_queue.put(info)</span><br><span class="line">            self.json_queue.task_done()</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def get_md5(info):</span><br><span class="line">        return hashlib.md5(str(info).encode(&#x27;utf-8&#x27;)).hexdigest()</span><br><span class="line"></span><br><span class="line">    def save_movie_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            data = self.data_queue.get()</span><br><span class="line">            data_md5 = self.get_md5(data)</span><br><span class="line">            if self.redis_cli.sadd(&#x27;movie:MgTv&#x27;, data_md5):</span><br><span class="line">                self.collection.insert_one(data)</span><br><span class="line">                print(&quot;数据保存成功&quot;)</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;数据重复...&quot;)</span><br><span class="line">            self.data_queue.task_done()</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        # 创建进程对象列表</span><br><span class="line">        processes = list()</span><br><span class="line"></span><br><span class="line">        # 创建一个进程对象用于构造请求参数并添加到进程对象列表中</span><br><span class="line">        processes.append(Process(target=self.get_params))</span><br><span class="line"></span><br><span class="line">        # 创建五个进程对象用于发送请求并添加到进程对象列表中</span><br><span class="line">        for _ in range(5):</span><br><span class="line">            processes.append(Process(target=self.get_api_json))</span><br><span class="line"></span><br><span class="line">        # 创建一个进程对象用于数据清洗并添加到进程对象列表中</span><br><span class="line">        processes.append(Process(target=self.parse_movie_info))</span><br><span class="line"></span><br><span class="line">        # 创建一个进程对象用户数据保存并添加到进程对象列表中</span><br><span class="line">        processes.append(Process(target=self.save_movie_info))</span><br><span class="line"></span><br><span class="line">        for item in processes:</span><br><span class="line">            item.daemon = True  # 设置守护进程</span><br><span class="line">            item.start()</span><br><span class="line">            time.sleep(0.2)  # 进程的启动是需要时间的</span><br><span class="line"></span><br><span class="line">        # 阻塞主进程，直到所有队列中的所有任务完成后解堵塞</span><br><span class="line">        for que in [self.params_queue, self.json_queue, self.data_queue]:</span><br><span class="line">            que.join()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    mg_tv = MgTv()</span><br><span class="line">    mg_tv.main()</span><br></pre></td></tr></table></figure>

<h3 id="4-用协程实现并发爬虫"><a href="#4-用协程实现并发爬虫" class="headerlink" title="4. 用协程实现并发爬虫"></a>4. 用协程实现并发爬虫</h3><ul>
<li><p>案例目标：使用协程完成并发爬虫下载图片，并使用 aiofile 模块异步读写文件，将图片保存到本地文件夹</p>
</li>
<li><p><strong>aiofile：是一个用于异步文件操作的 Python 模块，专门设计用于异步 I&#x2F;O 程序。它建立在 asyncio 库的基础上，提供了异步文件读写的功能。</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import aiofile</span><br><span class="line">import aiohttp</span><br><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">class WzryHero:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.json_url = &quot;https://pvp.qq.com/web201605/js/herolist.json&quot;</span><br><span class="line">        self.img_url = &quot;https://game.gtimg.cn/images/yxzj/img201606/skin/hero-info/&#123;&#125;/&#123;&#125;-bigskin-&#123;&#125;.jpg&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    async def main(self):</span><br><span class="line">        async with aiohttp.ClientSession() as session:</span><br><span class="line">            async with session.get(self.json_url, headers=self.headers, ssl=False) as resp:</span><br><span class="line">                # 通常情况下，aiohttp 会根据响应的 Content-Type 头部字段自动选择解析方式，</span><br><span class="line">                # 将其设置为None，就是告诉 aiohttp 不要尝试解析响应的 Content-Type</span><br><span class="line">                # 即在解析响应内容时不要依赖于 Content-Type 头部字段的值，而是尝试使用 JSON 解析器解析响应的整个内容</span><br><span class="line">                json = await resp.json(content_type=None)</span><br><span class="line">                tasks = list()</span><br><span class="line">                for item in json:</span><br><span class="line">                    ename = item[&#x27;ename&#x27;]</span><br><span class="line">                    cname = item[&#x27;cname&#x27;]</span><br><span class="line">                    coro_obj = self.get_img_data(session, ename, cname)</span><br><span class="line">                    tasks.append(asyncio.create_task(coro_obj))</span><br><span class="line">                await asyncio.wait(tasks)</span><br><span class="line"></span><br><span class="line">    async def get_img_data(self, session, ename, cname):</span><br><span class="line">        for skin_id in range(1, 20):</span><br><span class="line">            async with session.get(self.img_url.format(ename, ename, skin_id), headers=self.headers, ssl=False) as resp:</span><br><span class="line">                # aiohttp中的响应状态码是使用status，而不是requests模块的status_code</span><br><span class="line">                if resp.status == 200:</span><br><span class="line">                    content = await resp.read()</span><br><span class="line">                    async with aiofile.async_open(&#x27;./images/&#x27; + cname + &#x27;-&#x27; + str(skin_id) + &#x27;.jpg&#x27;, &#x27;wb&#x27;) as afp:</span><br><span class="line">                        await afp.write(content)</span><br><span class="line">                    print(&quot;下载成功&quot;)</span><br><span class="line">                else:</span><br><span class="line">                    break</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    if not os.path.exists(&#x27;./images&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./images&#x27;)</span><br><span class="line">    hero = WzryHero()</span><br><span class="line">    asyncio.run(hero.main())</span><br></pre></td></tr></table></figure>

<h2 id="八、selenium测试框架"><a href="#八、selenium测试框架" class="headerlink" title="八、selenium测试框架"></a>八、selenium测试框架</h2><blockquote>
<p><strong>安装selenium限定框架版本为4.9.1：<code>pip install selenium==4.9.1</code></strong></p>
<ul>
<li><p>驱动下载地址（关闭代理才能下载）：</p>
<ul>
<li><a class="link"   href="https://googlechromelabs.github.io/chrome-for-testing/"  target="_blank" rel="noopener">谷歌驱动下载地址<i class="fas fa-external-link-alt"></i></a></li>
<li><a class="link"   href="https://github.com/mozilla/geckodriver/releases"  target="_blank" rel="noopener">火狐驱动下载地址<i class="fas fa-external-link-alt"></i></a></li>
</ul>
</li>
<li><p>驱动下载完成后将文件移动到系统环境变量中：</p>
<ul>
<li>MacOS：将文件移动到 &#x2F;use&#x2F;local&#x2F;bin 目录</li>
<li>Windows：将文件移动到 miniconda3 安装目录</li>
</ul>
</li>
<li><p>在 macOS 上禁止 Chrome 自动更新（<strong>以下方法全部在第二天失效</strong>）：</p>
<ul>
<li>停止了检查更新的频率：<code>defaults write com.google.Keystone.Agent checkInterval 0</code></li>
<li>删除谷歌更新代理的执行文件：在 &#x2F;Library&#x2F;Google&#x2F; 中直接删除文件夹 GoogleSoftwareUpdate</li>
<li>在 CleanMyMac 的优化中移除或禁用谷歌更新相关的代理</li>
<li>在任何情况下，不要点击<code>关于Google Chrome</code>的按钮，否则直接自动更新</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="1-基本网页使用"><a href="#1-基本网页使用" class="headerlink" title="1. 基本网页使用"></a>1. 基本网页使用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line"># 创建一个Chrome浏览器实例，并打开一个页面</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(&quot;https://www.baidu.com&quot;)</span><br><span class="line"></span><br><span class="line"># 1. 截取当前页面的屏幕并保存为&quot;截屏图片.png&quot;，最好等页面加载完成后截屏</span><br><span class="line">time.sleep(1)</span><br><span class="line">browser.save_screenshot(&quot;截屏图片.png&quot;)</span><br><span class="line"></span><br><span class="line"># 2. 在原有的标签页中重新访问一个新的网址</span><br><span class="line">browser.get(&quot;https://www.jd.com&quot;)</span><br><span class="line"></span><br><span class="line"># 3. 页面前进与页面后退</span><br><span class="line">browser.back()  # 页面后退</span><br><span class="line">browser.forward()  # 页面前进</span><br><span class="line"></span><br><span class="line"># 4. 执行js脚本在新标签页中访问一个新的网址</span><br><span class="line">browser.execute_script(&quot;window.open(&#x27;https://xfblog.cn&#x27;)&quot;)</span><br><span class="line"></span><br><span class="line"># 5. 切换标签页，先切到标签页1，再切回标签页2</span><br><span class="line">browser.switch_to.window(browser.window_handles[0])</span><br><span class="line">browser.switch_to.window(browser.window_handles[1])</span><br><span class="line"># browser.switch_to.frame(iframe)  # 切入到iframe子页面中</span><br><span class="line"></span><br><span class="line"># 6. 页面滚动，大部分网站数据是动态数据，需要触发 ajax 请求后才能在页面中进行数据渲染</span><br><span class="line">for _ in range(1, 11):</span><br><span class="line">    browser.execute_script(&quot;window.scrollBy(0, 200)&quot;)</span><br><span class="line">    time.sleep(0.5)</span><br><span class="line"></span><br><span class="line"># 7. 关闭当前所在的标签页，当只有一个标签页时就是退出浏览器</span><br><span class="line">browser.close()</span><br><span class="line"></span><br><span class="line"># 8. 退出浏览器</span><br><span class="line">browser.quit()</span><br></pre></td></tr></table></figure>

<h3 id="2-获取请求相关信息"><a href="#2-获取请求相关信息" class="headerlink" title="2. 获取请求相关信息"></a>2. 获取请求相关信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(&quot;https://www.baidu.com&quot;)</span><br><span class="line"></span><br><span class="line"># 获取当前页面的源代码（selenium获取到的是动态数据渲染好的页面）</span><br><span class="line">html = browser.page_source</span><br><span class="line">print(type(html))  # `&lt;class &#x27;str&#x27;&gt;`</span><br><span class="line"></span><br><span class="line"># 获取cookies</span><br><span class="line">cookies = browser.get_cookies()</span><br><span class="line">print(cookies)</span><br><span class="line"></span><br><span class="line"># 获取经过处理之后，页面最后显示的url（如果有302的话，那么就是302之后的url）</span><br><span class="line">time.sleep(2)</span><br><span class="line">browser.get(&quot;https://www.360buy.com&quot;)  # 在原有标签页中重新访问新网址</span><br><span class="line">url = browser.current_url</span><br><span class="line">print(url)  # https://www.jd.com/</span><br></pre></td></tr></table></figure>

<h3 id="3-元素定位方法"><a href="#3-元素定位方法" class="headerlink" title="3. 元素定位方法"></a>3. 元素定位方法</h3><ul>
<li>**find_element()**：用于定位匹配给定选择器表达式的第一个元素，返回一个WebElements对象</li>
<li>**find_elements()**：用于定位匹配给定选择器表达式的所有元素，返回一个WebElements对象的列表<ul>
<li>获取文本：<strong>element.text</strong></li>
<li>获取属性值：<strong>element.get_attribute(“href”)</strong></li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(&quot;https://news.baidu.com/&quot;)</span><br><span class="line"></span><br><span class="line"># 定位单个对象，返回一个WebElement对象</span><br><span class="line">element_obj1 = browser.find_element(By.ID, &quot;ww&quot;)</span><br><span class="line">element_obj2 = browser.find_element(By.CSS_SELECTOR, &quot;.word&quot;)</span><br><span class="line">element_obj3 = browser.find_element(By.XPATH, &quot;//input[@id=&#x27;ww&#x27;]&quot;)</span><br><span class="line">print(element_obj1 == element_obj2 == element_obj3)  # True</span><br><span class="line"># LINK_TEXT要求链接文本完全匹配，而PARTIAL_LINK_TEXT只需要部分匹配（查找对象必须是超链接）</span><br><span class="line">element_obj4 = browser.find_element(By.LINK_TEXT, &quot;热点要闻&quot;)</span><br><span class="line">element_obj5 = browser.find_element(By.PARTIAL_LINK_TEXT, &quot;热点要&quot;)</span><br><span class="line">print(element_obj4 == element_obj5)  # True</span><br><span class="line"></span><br><span class="line"># 定位热点要闻（多个），返回包含多个WebElement对象的列表</span><br><span class="line">element_objs = browser.find_elements(By.XPATH, &quot;//div[@class=&#x27;hotnews&#x27;]//li&quot;)</span><br><span class="line">for item in element_objs:</span><br><span class="line">    print(item.get_attribute(&#x27;class&#x27;))</span><br><span class="line">    print(item.text)</span><br></pre></td></tr></table></figure>

<h3 id="4-cookie信息处理"><a href="#4-cookie信息处理" class="headerlink" title="4. cookie信息处理"></a>4. cookie信息处理</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(&quot;https://www.baidu.com/&quot;)</span><br><span class="line"></span><br><span class="line"># 通过selenium控制浏览器请求获得的是服务器响应设置的完整cookies信息（包含多个字段），</span><br><span class="line"># 这与requests模块中 response.cookies.get_dict() 所获取的set-cookie信息不同，</span><br><span class="line"># 并且浏览器在发送新请求时携带的cookie也只需要name和value字段，</span><br><span class="line"># 所以这里需要将cookies整理为requests等请求需要的只包含 name 和 value 字段的字典格式</span><br><span class="line">cookies = browser.get_cookies()</span><br><span class="line"></span><br><span class="line"># 通过字典生成式直接生成</span><br><span class="line">cookie_dict = &#123;item[&#x27;name&#x27;]: item[&#x27;value&#x27;] for item in cookies&#125;</span><br><span class="line">print(cookie_dict)</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">cookie设置：browser.add_cookie(&#123;name: &#x27;xfblog&#x27;&#125;)</span><br><span class="line">cookie删除：browser.delete_cookie(&#x27;name&#x27;)，browser.delete_all_cookies()</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<h3 id="5-WebDriverWait等待"><a href="#5-WebDriverWait等待" class="headerlink" title="5. WebDriverWait等待"></a>5. WebDriverWait等待</h3><ul>
<li><strong>WebDriverWait：</strong>是Selenium库中的一个类，用于实现<strong>显式等待</strong>。显式等待是指在查找元素或执行某些操作之前，显式地等待某个特定条件的发生。与隐式等待不同，显式等待仅适用于特定的操作，而不是整个页面</li>
<li><strong>使用：</strong>使用 WebDriverWait 通常需要配合 expected_conditions 模块，它包含了一系列可以用于等待的条件，例如元素可见、元素存在、元素可点击等（<strong>以下方法参数接受一个元组</strong>）<ul>
<li><strong>等待元素可见：<code>EC.visibility_of_element_located()</code></strong></li>
<li><strong>等待元素存在：<code>EC.presence_of_element_located()</code></strong></li>
<li><strong>等待元素可点击：&#96;&#96;EC.element_to_be_clickable()&#96;</strong></li>
<li><strong>等待文本出现在元素中：<code>EC.text_to_be_present_in_element()</code></strong></li>
</ul>
</li>
<li><strong>隐式等待：</strong>是Selenium提供的一种等待机制，适用于 WebDriver 实例的整个生命周期  **<code>browser.implicitly_wait(10)</code>**。一旦设置了隐式等待，它将在后续的所有查找操作中生效，直到WebDriver实例关闭。这使得代码中不必在每个查找操作前都显式地等待元素出现，但隐式等待并不是对所有操作都有效，它主要影响 find_element 方法。对于一些异步加载的元素或 JavaScript 动态渲染的内容，隐式等待可能不如显式等待精确和灵活</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line"></span><br><span class="line">class LoginEmail:</span><br><span class="line">    def __init__(self, email, password):</span><br><span class="line">        self.browser = webdriver.Chrome()</span><br><span class="line">        self.wait_ob = WebDriverWait(self.browser, timeout=5)</span><br><span class="line">        self.url = &quot;https://mail.163.com/&quot;</span><br><span class="line">        self.email = email</span><br><span class="line">        self.password = password</span><br><span class="line"></span><br><span class="line">    def open_email(self):</span><br><span class="line">        self.browser.get(self.url)</span><br><span class="line"></span><br><span class="line">    def login_email(self):</span><br><span class="line">        # 使用WebDriverWait等待iframe可用并切换到它，等价于不用等待的以下用法：</span><br><span class="line">        # iframe = self.browser.find_element(By.XPATH, &quot;//div[@id=&#x27;loginDiv&#x27;]/iframe&quot;)</span><br><span class="line">        # self.browser.switch_to.frame(iframe)  # 切入到iframe子页面中</span><br><span class="line">        self.wait_ob.until(</span><br><span class="line">            EC.frame_to_be_available_and_switch_to_it((By.XPATH, &quot;//div[@id=&#x27;loginDiv&#x27;]/iframe&quot;)))</span><br><span class="line"></span><br><span class="line">        input_user = self.wait_ob.until(EC.presence_of_element_located((By.XPATH, &quot;//input[@name=&#x27;email&#x27;]&quot;)))</span><br><span class="line">        input_pass = self.wait_ob.until(EC.presence_of_element_located((By.XPATH, &quot;//input[@name=&#x27;password&#x27;]&quot;)))</span><br><span class="line">        login_button = self.wait_ob.until(EC.presence_of_element_located((By.ID, &quot;dologin&quot;)))</span><br><span class="line"></span><br><span class="line">        input_user.send_keys(self.email)</span><br><span class="line">        input_pass.send_keys(self.password)</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        login_button.click()</span><br><span class="line"></span><br><span class="line">    def close(self):</span><br><span class="line">        time.sleep(6)  # 成功登录后，等待跳转至邮箱页面</span><br><span class="line">        self.browser.quit()</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        self.open_email()</span><br><span class="line">        self.login_email()</span><br><span class="line">        self.close()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    login = LoginEmail(&quot;&quot;, &quot;&quot;)</span><br><span class="line">    login.main()</span><br></pre></td></tr></table></figure>

<h3 id="6-绕过检测配置"><a href="#6-绕过检测配置" class="headerlink" title="6. 绕过检测配置"></a>6. 绕过检测配置</h3><blockquote>
<p><strong>问题：</strong>默认情况下Selenium在加载页面时，根据正常的加载策略，会将页面及其所有静态资源都下载完（如css、图片、js等）</p>
<p><strong>解决：修改WebDriver的页面加载策略</strong></p>
<ul>
<li><strong>normal （默认）：</strong>等待所有内容加载完成，包括静态资源文件、css、js等</li>
<li><strong>eager：</strong>等待初始HTML文档完全加载和解析，并放弃css、图像和子框架等的加载</li>
<li><strong>none：</strong>仅等待初始页面下载，即进行操作</li>
</ul>
</blockquote>
<ul>
<li><strong>绕过检测：</strong>通常是指网站使用一些手段来检测自动化工具（Selenium）并采取一些措施阻止自动化脚本的执行。在使用Selenium时，有一些配置项可以尽量隐藏浏览器驱动的信息，以减少被检测的概率</li>
<li><strong>无头浏览器运行：<code>options.add_argument(&#39;--headless&#39;)</code></strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">from selenium import webdriver</span><br><span class="line"></span><br><span class="line">options = webdriver.ChromeOptions()</span><br><span class="line"># 1. 禁用Chrome的Blink渲染引擎中相关的自动化特性，目的是减少被自动化工具检测到的概率</span><br><span class="line">options.add_argument(&#x27;--disable-blink-features=AutomationControlled&#x27;)</span><br><span class="line"></span><br><span class="line"># 2. 更改User-Agent</span><br><span class="line">options.add_argument(</span><br><span class="line">    &quot;user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&quot;)</span><br><span class="line"></span><br><span class="line"># 3. 禁用浏览器扩展，减少了浏览器的标识，使其更难以被网站检测到</span><br><span class="line">options.add_argument(&quot;--disable-extensions&quot;)</span><br><span class="line"></span><br><span class="line"># 4. 禁用图片加载，有些检测机制可能通过检查图片加载行为来判断是否使用了自动化工具</span><br><span class="line">options.add_argument(&quot;--blink-settings=imagesEnabled=false&quot;)</span><br><span class="line">&quot;&quot;&quot;以下也可以禁用图片加载：</span><br><span class="line">prefs = &#123;&quot;profile.managed_default_content_settings.images&quot;: 2&#125;</span><br><span class="line">options.add_experimental_option(&#x27;prefs&#x27;, prefs)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 5. 禁用浏览器提示，如是否保存密码、是否允许通知等，这些提示可能包含了一些浏览器的信息</span><br><span class="line">options.add_argument(&quot;--disable-infobars&quot;)</span><br><span class="line"></span><br><span class="line"># 6. 禁用自动化标志和扩展（隐藏开发者警告）</span><br><span class="line">options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;])</span><br><span class="line">options.add_experimental_option(&#x27;useAutomationExtension&#x27;, False)</span><br><span class="line"></span><br><span class="line"># 7. 使用代理IP，切换IP地址可能有助于绕过一些检测机制</span><br><span class="line">options.add_argument(&#x27;--proxy-server=http://127.0.0.1:7890&#x27;)</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome(options=options)</span><br><span class="line">browser.get(&quot;https://bot.sannysoft.com/&quot;)</span><br><span class="line">time.sleep(10)</span><br></pre></td></tr></table></figure>

<h3 id="7-ActionChains动作链"><a href="#7-ActionChains动作链" class="headerlink" title="7. ActionChains动作链"></a>7. ActionChains动作链</h3><ul>
<li><p><strong>ActionChains：</strong>是 Selenium 中的一个类，它提供了一种用于处理一系列连续的、复杂的用户输入操作的方式，例如鼠标移动、点击、拖拽、键盘输入等，可以模拟用户在浏览器中的真实行为，执行多个动作的组合</p>
</li>
<li><p><strong>常用动作方法：</strong></p>
<ol>
<li>**click_and_hold()**：鼠标单击并按住不放</li>
<li>**pause()**：暂停一段时间，单位为秒</li>
<li>**wait()**：等待某个条件成立，比如等待元素出现或消失，或者等待某个属性变化</li>
<li>**drag_and_drop()**：将源元素拖拽到目标元素的位置</li>
<li>**drag_and_drop_by_offset()**：将元素拖拽到指定的坐标位置</li>
<li>**move_by_offset()**：将鼠标移动到指定的坐标位置</li>
<li>**move_to_element_with_offset()**：将鼠标移动到指定元素的偏移位置</li>
<li>**release()**：释放鼠标操作</li>
<li>**perform()**：执行ActionChains中所有动作，通常在最后调用</li>
</ol>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line">from selenium.webdriver import ActionChains</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(&quot;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&quot;)</span><br><span class="line"></span><br><span class="line">wait_ob = WebDriverWait(browser, timeout=5)</span><br><span class="line">wait_ob.until(EC.frame_to_be_available_and_switch_to_it((By.XPATH, &quot;//iframe[@id=&#x27;iframeResult&#x27;]&quot;)))</span><br><span class="line">div_1 = wait_ob.until(EC.element_to_be_clickable((By.XPATH, &quot;//div[@id=&#x27;draggable&#x27;]&quot;)))</span><br><span class="line">div_2 = wait_ob.until(EC.element_to_be_clickable((By.XPATH, &quot;//div[@id=&#x27;droppable&#x27;]&quot;)))</span><br><span class="line"></span><br><span class="line">action = ActionChains(browser)</span><br><span class="line">action.drag_and_drop(div_1, div_2)</span><br><span class="line">action.perform()  # 执行动作链</span><br><span class="line"></span><br><span class="line">time.sleep(3)</span><br></pre></td></tr></table></figure>

<h3 id="8-selenium异常处理"><a href="#8-selenium异常处理" class="headerlink" title="8. selenium异常处理"></a>8. selenium异常处理</h3><ul>
<li><strong>selenium异常处理：</strong>虽然 Selenium 中的异常是继承自 Python 中的 Exception 类的，但是它们不仅仅是 Python 标准异常的简单子类，还包含了一些 Selenium 自定义的特定异常类（例如 <code>NoSuchElementException</code>、<code>TimeoutException</code> 等）。因此，<strong>不能使用 Python 中的通用异常类 Exception 来捕获 Selenium 中的某些特定异常，需要导入 Selenium 中特定的异常类进行捕获</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.common.exceptions import TimeoutException, NoSuchElementException</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"></span><br><span class="line"># 设置页面加载超时时间为3秒</span><br><span class="line">browser.set_page_load_timeout(3)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    browser.get(&quot;https://www.google.com&quot;)</span><br><span class="line">except TimeoutException as e:</span><br><span class="line">    print(f&quot;页面加载超时：&#123;e&#125;&quot;)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">页面加载超时：Message: timeout: Timed out receiving message from renderer: -0.005</span><br><span class="line">  (Session info: chrome=119.0.6045.159)</span><br><span class="line">Stacktrace:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    browser.find_element(By.ID, &quot;abc&quot;)</span><br><span class="line">except NoSuchElementException as e:</span><br><span class="line">    print(f&quot;元素没找到：&#123;e&#125;&quot;)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">元素没找到：Message: no such element: Unable to locate element: &#123;&quot;method&quot;:&quot;css selector&quot;,&quot;selector&quot;:&quot;[id=&quot;abc&quot;]&quot;&#125;</span><br><span class="line">  (Session info: chrome=119.0.6045.159)</span><br><span class="line">Stacktrace:</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<h3 id="9-唯品会数据采集"><a href="#9-唯品会数据采集" class="headerlink" title="9. 唯品会数据采集"></a>9. 唯品会数据采集</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import random</span><br><span class="line">from pymongo import MongoClient</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line">from selenium.common.exceptions import NoSuchElementException</span><br><span class="line"></span><br><span class="line">class Wpshop:</span><br><span class="line">    # MongoDB 客户端和集合初始化</span><br><span class="line">    mongo_cli = MongoClient()</span><br><span class="line">    collection = mongo_cli[&#x27;py_spider&#x27;][&#x27;wp_shop&#x27;]</span><br><span class="line"></span><br><span class="line">    # 配置 Chrome 浏览器选项</span><br><span class="line">    options = webdriver.ChromeOptions()</span><br><span class="line">    options.add_argument(&quot;--blink-settings=imagesEnabled=false&quot;)  # 禁止图片加载</span><br><span class="line"></span><br><span class="line">    # 初始化 Chrome 浏览器和等待对象</span><br><span class="line">    browser = webdriver.Chrome(options=options)</span><br><span class="line">    wait_ob = WebDriverWait(browser, timeout=5)</span><br><span class="line"></span><br><span class="line">    def index_html(self):</span><br><span class="line">        # 打开首页，搜索输入框输入关键字 &quot;口红&quot;，点击搜索按钮</span><br><span class="line">        self.browser.get(&quot;https://www.vip.com/&quot;)</span><br><span class="line">        search_input = self.wait_ob.until(EC.presence_of_element_located((By.XPATH, &quot;//div[@id=&#x27;J-search&#x27;]/div/input&quot;)))</span><br><span class="line">        search_input.send_keys(&quot;口红&quot;)</span><br><span class="line">        time.sleep(2)</span><br><span class="line">        search_btn = self.wait_ob.until(EC.element_to_be_clickable((By.XPATH, &quot;//div[@id=&#x27;J-search&#x27;]/div/a&quot;)))</span><br><span class="line">        search_btn.click()</span><br><span class="line"></span><br><span class="line">        # 当前页面成功获取之后建议延迟两秒，让浏览器滚动</span><br><span class="line">        time.sleep(2)</span><br><span class="line"></span><br><span class="line">    # 页面滚动</span><br><span class="line">    def drop_down(self):</span><br><span class="line">        # 控制滚动12次，差一点到底部，直接滑动到底部可能被反爬</span><br><span class="line">        for i in range(1, 12):</span><br><span class="line">            self.browser.execute_script(f&#x27;document.documentElement.scrollTop = &#123;i * 1000&#125;&#x27;)</span><br><span class="line">            time.sleep(1.5 + random.random())  # 每次滑动等待一定时间，因为数据可能通过ajax继续加载更多出来</span><br><span class="line"></span><br><span class="line">    # 商品数据解析</span><br><span class="line">    def parse_data(self):</span><br><span class="line">        self.drop_down()</span><br><span class="line">        # xpath解析出包含data-product-id属性的div标签</span><br><span class="line">        div_list = self.browser.find_elements(By.XPATH, &quot;//section[@id=&#x27;J_searchCatList&#x27;]/div[@data-product-id]&quot;)</span><br><span class="line">        for item in div_list:</span><br><span class="line">            # xpath解析出class属性contains包含部分值的div标签</span><br><span class="line">            title = item.find_element(By.XPATH, &#x27;.//div[contains(@class, &quot;c-goods-item__name  c-goods-item__name&quot;)]&#x27;)</span><br><span class="line">            price = item.find_element(By.XPATH, &#x27;.//div[@class=&quot;c-goods-item__sale-price J-goods-item__sale-price&quot;]&#x27;)</span><br><span class="line">            data_item = &#123;</span><br><span class="line">                &#x27;title&#x27;: title.text,</span><br><span class="line">                &#x27;price&#x27;: price.text</span><br><span class="line">            &#125;</span><br><span class="line">            print(data_item)</span><br><span class="line">            self.save_mongo(data_item)</span><br><span class="line">        self.next_page()</span><br><span class="line"></span><br><span class="line">    # 翻页控制</span><br><span class="line">    def next_page(self):</span><br><span class="line">        try:</span><br><span class="line">            # 等待下一页按钮可点击</span><br><span class="line">            next_button = self.browser.find_element(By.XPATH, &#x27;//a[@id=&quot;J_nextPage_link&quot;]&#x27;)</span><br><span class="line">            next_button.click()</span><br><span class="line">            self.parse_data()</span><br><span class="line">        except NoSuchElementException as e:  # 最后一页没有下一页的按钮</span><br><span class="line">            print(f&quot;最后一页了：&#123;e&#125;&quot;)</span><br><span class="line">            self.browser.quit()</span><br><span class="line"></span><br><span class="line">    # 将商品信息存入 MongoDB 数据库</span><br><span class="line">    def save_mongo(self, data):</span><br><span class="line">        self.collection.insert_one(data)</span><br><span class="line"></span><br><span class="line">    # 主程序入口</span><br><span class="line">    def main(self):</span><br><span class="line">        self.index_html()</span><br><span class="line">        self.parse_data()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    wp_shop = Wpshop()</span><br><span class="line">    wp_shop.main()</span><br></pre></td></tr></table></figure>

<h3 id="10-pyppeteer框架"><a href="#10-pyppeteer框架" class="headerlink" title="10. pyppeteer框架"></a>10. pyppeteer框架</h3><blockquote>
<p><strong>注意：无论使用什么测试框架，只要涉及到浏览器，就已经与速度告别了，永远不可能比直接请求协议快</strong></p>
</blockquote>
<ul>
<li><strong>pyppeteer：</strong>是 Python 中的一个库，用于使用 Headless Chrome 浏览器进行无头浏览器测试和网页爬取。它是 Puppeteer 的 Python 版本，Puppeteer 是一个由 Google 提供的 Node.js 库，专门用于控制无头 Chrome 或 Chromium 浏览器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> pyppeteer <span class="keyword">import</span> launch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要设置显示内容的宽度、高度</span></span><br><span class="line">width, height = <span class="number">1366</span>, <span class="number">768</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pypeteer不用配置浏览器驱动，它会自动下载Chromium用于测试</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    `headless=False`：开启浏览器界面</span></span><br><span class="line"><span class="string">    `devtools=True`：开启浏览器开发工具</span></span><br><span class="line"><span class="string">    `userDataDir=&quot;./userdata&quot;`：数据持久化</span></span><br><span class="line"><span class="string">    `args=[&#x27;--disable-infobars&#x27;]`：禁用提示条</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    browser = <span class="keyword">await</span> launch(</span><br><span class="line">        headless=<span class="literal">False</span>,</span><br><span class="line">        devtools=<span class="literal">False</span>,</span><br><span class="line">        userDataDir=<span class="string">&quot;./userdata&quot;</span>,</span><br><span class="line">        <span class="comment"># 禁用提示条，同时绕过检测</span></span><br><span class="line">        args=[<span class="string">&#x27;--disable-infobars&#x27;</span>, <span class="string">&#x27;--disable-blink-features=AutomationControlled&#x27;</span>],</span><br><span class="line">    )</span><br><span class="line">    page = <span class="keyword">await</span> browser.newPage()</span><br><span class="line">    <span class="comment"># 设置显示界面与浏览器窗口大小一致</span></span><br><span class="line">    <span class="keyword">await</span> page.setViewport(&#123;<span class="string">&#x27;width&#x27;</span>: width, <span class="string">&#x27;height&#x27;</span>: height&#125;)</span><br><span class="line">    <span class="keyword">await</span> page.goto(<span class="string">&#x27;https://bot.sannysoft.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 屏幕截图</span></span><br><span class="line">    <span class="keyword">await</span> page.screenshot(&#123;<span class="string">&#x27;path&#x27;</span>: <span class="string">&#x27;example.png&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">await</span> browser.close()</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure>

<h2 id="九、IP代理池"><a href="#九、IP代理池" class="headerlink" title="九、IP代理池"></a>九、IP代理池</h2><blockquote>
<p><strong>开放代理（Public Proxy）：</strong>这类代理服务器是公开的，任何人都可以免费或者付费使用，可以快速获取。通常，开放代理服务器的 IP 地址和端口号是公开的，并且不限制访问。由于是公开的，被滥用的可能性较大。速度和稳定性大不如私密代理或独享代理。</p>
<p><strong>私密代理（Shared Proxy）：</strong>私密代理是由多个用户共享的，但是相对于开放代理，使用者需要经过认证，并且通常会提供更高的速度和稳定性。拥有相对较好的速度和稳定性，适用于一些对质量要求较高的场景。</p>
<p><strong>独享代理（Dedicated Proxy）：</strong>独享代理是由单个用户独享的，用户可以完全控制代理的使用。通常，这类代理的 IP 提供更高的隐私和稳定性，适用于需要保密性和高可用性的场景。但通常费用较高，相对于开放代理和私密代理来说，成本更高。</p>
</blockquote>
<h3 id="1-免费代理采集脚本"><a href="#1-免费代理采集脚本" class="headerlink" title="1. 免费代理采集脚本"></a>1. 免费代理采集脚本</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line">import pymongo</span><br><span class="line">from concurrent.futures import ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line">class FreeProxy:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.url = &quot;https://proxy.ip3366.net/free/?action=china&amp;page=&#123;&#125;&quot;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">        &#125;</span><br><span class="line">        self.mongo_cli = pymongo.MongoClient()</span><br><span class="line">        self.collection = self.mongo_cli[&#x27;py_spider&#x27;][&#x27;ip_port&#x27;]</span><br><span class="line"></span><br><span class="line">    def get_proxy(self, page):</span><br><span class="line">        response = requests.get(self.url.format(page), headers=self.headers).text</span><br><span class="line">        proxy_list = re.findall(r&#x27;.*?IP&quot;&gt;(.*?)&lt;/td&gt;.*?PORT&quot;&gt;(.*?)&lt;/td&gt;&#x27;, response, re.S)</span><br><span class="line">        for item in proxy_list:</span><br><span class="line">            proxy_info = dict()</span><br><span class="line">            proxy_info[&#x27;http&#x27;] = f&quot;http://&#123;item[0]&#125;:&#123;item[1]&#125;&quot;</span><br><span class="line">            proxy_info[&#x27;https&#x27;] = f&quot;https://&#123;item[0]&#125;:&#123;item[1]&#125;&quot;</span><br><span class="line"></span><br><span class="line">            self.test_ip(proxy_info)</span><br><span class="line"></span><br><span class="line">    def test_ip(self, proxy_info):</span><br><span class="line">        try:</span><br><span class="line">            response = requests.get(&quot;http://httpbin.org/ip&quot;, proxies=proxy_info, timeout=3)</span><br><span class="line">            if response.status_code == 200:</span><br><span class="line">                self.collection.insert_one(proxy_info)</span><br><span class="line">            print(f&quot;代理 &#123;proxy_info&#125; 可用&quot;)</span><br><span class="line">        except:</span><br><span class="line">            pass</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        with ThreadPoolExecutor(max_workers=10) as pool:</span><br><span class="line">            futures = [pool.submit(self.get_proxy, page) for page in range(1, 11)]</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    fp = FreeProxy()</span><br><span class="line">    fp.main()</span><br></pre></td></tr></table></figure>

<h3 id="2-付费代理的使用"><a href="#2-付费代理的使用" class="headerlink" title="2. 付费代理的使用"></a>2. 付费代理的使用</h3><ul>
<li>快代理：<a class="link"   href="https://www.kuaidaili.com/"  target="_blank" rel="noopener">https://www.kuaidaili.com/<i class="fas fa-external-link-alt"></i></a></li>
</ul>
<h3 id="3-亚马逊数据采集"><a href="#3-亚马逊数据采集" class="headerlink" title="3. 亚马逊数据采集"></a>3. 亚马逊数据采集</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line">import threading</span><br><span class="line">import time</span><br><span class="line">import pymysql</span><br><span class="line">import requests</span><br><span class="line">import retrying</span><br><span class="line">from queue import Queue</span><br><span class="line">from loguru import logger</span><br><span class="line">from feapder.network.user_agent import get  # User-Agent池</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">class Amazon:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.db = pymysql.connect(user=&quot;root&quot;, password=&#x27;123456&#x27;, db=&#x27;py_spider&#x27;)</span><br><span class="line">        self.cursor = self.db.cursor()</span><br><span class="line">        self.menu_url = &quot;https://www.amazon.cn/nav/ajax/hamburgerMainContent?ajaxTemplate=hamburgerMainContent&amp;pageType=Gateway&amp;hmDataAjaxHint=1&amp;navDeviceType=desktop&amp;isSmile=0&amp;isPrime=0&amp;isBackup=false&amp;hashCustomerAndSessionId=4c2b8b7d251e031f867340dd748fdc203954406c&amp;languageCode=zh_CN&amp;environmentVFI=AmazonNavigationCards%2Fdevelopment-nov13patch%40B6165608796-AL2_x86_64&amp;secondLayerTreeName=apparel_shoes%2Bcomputer_office%2Bhome_kitchen%2Bbeauty_pca%2Bsports_outdoor%2Bgrocery%2Bbaby_toy%2Bphones_elec%2Bjewelry_watch%2Bhome_improvement%2Bvideo_game%2Bmusical_instrument%2Bcamera&amp;customerCountryCode=null&quot;</span><br><span class="line">        # 加其余验证参数主要是为了请求商品具体信息时使用</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">            &quot;Referer&quot;: &quot;https://www.amazon.cn&quot;,</span><br><span class="line">            &quot;X-Requested-With&quot;: &quot;XMLHttpRequest&quot;,</span><br><span class="line">            &quot;downlink&quot;: &quot;10&quot;,</span><br><span class="line">            &quot;ect&quot;: &quot;4g&quot;,</span><br><span class="line">            &quot;rtt&quot;: &quot;200&quot;,</span><br><span class="line">        &#125;</span><br><span class="line">        # 付费ip代理的api链接</span><br><span class="line">        self.ip_url = &quot;&quot;</span><br><span class="line">        # 付费ip地址队列</span><br><span class="line">        self.ip_queue = Queue()</span><br><span class="line"></span><br><span class="line">        # 商品分类对应所有商品数据的地址队列</span><br><span class="line">        self.classify_url_queue = Queue()</span><br><span class="line">        # 商品详情数据地址队列</span><br><span class="line">        self.detail_url_queue = Queue()</span><br><span class="line">        # 商品数据队列</span><br><span class="line">        self.shop_info_queue = Queue()</span><br><span class="line"></span><br><span class="line">    # 获取代理ip并上传到ip队列</span><br><span class="line">    def get_ip(self):</span><br><span class="line">        while True:</span><br><span class="line">            # 判断当前队列是否为空, 为空则获取ip</span><br><span class="line">            if self.ip_queue.empty():</span><br><span class="line">                ips = requests.get(self.ip_url).json()</span><br><span class="line">                for ip in ips:</span><br><span class="line">                    self.ip_queue.put(ip)</span><br><span class="line">            else:</span><br><span class="line">                continue</span><br><span class="line"></span><br><span class="line">    # 全局通用发起请求，使用随机UA和代理ip，抛出异常则重试3次</span><br><span class="line">    @retrying.retry(stop_max_attempt_number=3)</span><br><span class="line">    def request_data(self, url):</span><br><span class="line">        &quot;&quot;&quot;主要访问菜单api并返回api中的分类数据&quot;&quot;&quot;</span><br><span class="line">        ip = self.ip_queue.get()</span><br><span class="line">        proxies = &#123;</span><br><span class="line">            &#x27;http&#x27;: &#x27;http://&#x27; + ip,</span><br><span class="line">            &#x27;https&#x27;: &#x27;https://&#x27; + ip</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers[&#x27;User-Agent&#x27;] = get()</span><br><span class="line">        response = requests.get(url, headers=self.headers, proxies=proxies, verify=False, timeout=2)</span><br><span class="line">        # 代理ip重用</span><br><span class="line">        if response.status_code == 200:</span><br><span class="line">            self.ip_queue.put(ip)</span><br><span class="line">        else:</span><br><span class="line">            print(&#x27;状态码异常:&#x27;, ip)</span><br><span class="line">        return response</span><br><span class="line"></span><br><span class="line">    # 获取分类数据中的node参数，并构造商品分类对应所有商品数据的地址队列</span><br><span class="line">    def get_classify_url(self):</span><br><span class="line">        try:</span><br><span class="line">            response = self.request_data(self.menu_url).text</span><br><span class="line">        except Exception as e:</span><br><span class="line">            logger.error(f&quot;请求菜单 &#123;self.menu_url&#125; 异常：&#123;e&#125; &quot;)</span><br><span class="line">        else:</span><br><span class="line">            node_list = re.findall(r&#x27;&lt;a.*?node=(?!1478512071|1484186071|865184051)(.*?)&amp;.*?&lt;/a&gt;&#x27;, response, re.S)</span><br><span class="line">            for node in node_list:</span><br><span class="line">                self.classify_url_queue.put(f&quot;https://www.amazon.cn/s?rh=n%3A&#123;node&#125;&amp;fs=true&quot;)</span><br><span class="line"></span><br><span class="line">    # 重试获取最大页数，亚马逊有一定的反爬，导致部分链接的最大页数是获取不到的</span><br><span class="line">    @retrying.retry(stop_max_attempt_number=10)</span><br><span class="line">    def get_max_page(self, classify_url):</span><br><span class="line">        try:</span><br><span class="line">            response = self.request_data(classify_url).text</span><br><span class="line">        except Exception as e:</span><br><span class="line">            logger.error(f&quot;请求分类 &#123;classify_url&#125; 异常：&#123;e&#125; &quot;)</span><br><span class="line">        else:</span><br><span class="line">            max_page = re.findall(r&#x27;true&quot;&gt;(\d&#123;1,3&#125;)&lt;/span&gt;&lt;a.*?aria-label=&quot;转到下一页&#x27;, response, re.S)</span><br><span class="line">            if not max_page:</span><br><span class="line">                print(f&quot;。。。正在重试获取最大页数：&#123;classify_url&#125;&quot;)</span><br><span class="line">                raise Exception(classify_url)</span><br><span class="line">            return max_page[0]  # re.findall返回值为列表，重试过后</span><br><span class="line"></span><br><span class="line">    # 根据classify_url_queue地址，循环翻页，构造每个商品的详情数据地址队列</span><br><span class="line">    def get_detail_url(self):</span><br><span class="line">        while True:</span><br><span class="line">            classify_url = self.classify_url_queue.get()</span><br><span class="line">            try:</span><br><span class="line">                max_page = self.get_max_page(classify_url)</span><br><span class="line">                print(classify_url, max_page)</span><br><span class="line">            except Exception as e:</span><br><span class="line">                # 情况1：重试10次后，获取最大页数依然失败了，重试过后max_page值依然是空列表</span><br><span class="line">                # 情况2：所有商品数据只有一页的情况，也会重试并产生空列表</span><br><span class="line">                max_page = 1  # 如果失败抛出异常则将max_page赋值为1，否则只有成功的时候finally中的max_page才有值</span><br><span class="line">                logger.error(f&quot;获取最大页数失败：&#123;e&#125;&quot;)</span><br><span class="line">            finally:  # 如果使用else，那么获取最大页数成功的时候就不会走下面的逻辑</span><br><span class="line">                for page in range(1, int(max_page) + 1):</span><br><span class="line">                    # 这里不需要进行异常捕获，因为在获取这页数据时，被反爬则会返回一个没用的页面，不会抛出异常</span><br><span class="line">                    resp = self.request_data(classify_url + f&quot;&amp;page=&#123;page&#125;&quot;).text</span><br><span class="line">                    detail_url_list = re.findall(r&#x27;&lt;a.*?a-link-normal s-no-outline&quot;.*?href=&quot;(.*?)&quot;&gt;&lt;div&#x27;, resp, re.S)</span><br><span class="line">                    for detail in detail_url_list:</span><br><span class="line">                        detail_url = &quot;https://www.amazon.cn&quot; + detail</span><br><span class="line">                        self.detail_url_queue.put(detail_url)</span><br><span class="line">                self.classify_url_queue.task_done()</span><br><span class="line"></span><br><span class="line">    # 根据detail_url_queue商品的详情数据地址，拿到商品详情数据并进行数据清洗</span><br><span class="line">    def parse_shop_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            detail_url = self.detail_url_queue.get()</span><br><span class="line">            response = self.request_data(detail_url).text</span><br><span class="line">            try:</span><br><span class="line">                if re.search(r&quot;a-price-range&quot;, response):</span><br><span class="line">                    match = re.findall(r&#x27;a-offscreen&quot;&gt;(¥.*?)&lt;/span&gt;&#x27;, response)</span><br><span class="line">                    price = match[0] + &#x27;-&#x27; + match[1]</span><br><span class="line">                    title = re.findall(r&#x27;&lt;span id=&quot;productTitle&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;, response, re.S)[0]</span><br><span class="line">                else:</span><br><span class="line">                    price = re.search(r&#x27;a-offscreen&quot;&gt;(¥.*?)&lt;/span&gt;&#x27;, response).group(1)</span><br><span class="line">                    title = re.findall(r&#x27;&lt;span id=&quot;productTitle&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;, response, re.S)[0]</span><br><span class="line">            except Exception as e:</span><br><span class="line">                logger.error(f&quot;获取数据 &#123;detail_url&#125; 详情信息被反爬了：&#123;e&#125; &quot;)</span><br><span class="line">            else:</span><br><span class="line">                print(title.strip(), price)</span><br><span class="line">                # 将解析到的商品数据打包成一个元组提交到队列中</span><br><span class="line">                self.shop_info_queue.put((title.strip(), price, detail_url))</span><br><span class="line">            finally:</span><br><span class="line">                self.detail_url_queue.task_done()</span><br><span class="line"></span><br><span class="line">    # 创建mysql数据表用于保存数据</span><br><span class="line">    def create_table(self):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            create table if not exists amazon_shop(</span><br><span class="line">                id int primary key auto_increment,</span><br><span class="line">                price varchar(255) not null,</span><br><span class="line">                title varchar(255) not null,</span><br><span class="line">                goods_url varchar(255) not null</span><br><span class="line">            );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            self.cursor.execute(sql)</span><br><span class="line">            print(&#x27;数据表创建成功...&#x27;)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(f&#x27;数据表创建失败:&#x27;, e)</span><br><span class="line">            logger.error(&#x27;数据表创建失败:&#x27;, e)</span><br><span class="line"></span><br><span class="line">    # 将shop_info_queue中的详情数据信息保存到mysql数据库中，一次性保存30条</span><br><span class="line">    def save_shop_info(self):</span><br><span class="line">        while True:</span><br><span class="line">            info_list = list()  # 定义列表用于数据的批量存储</span><br><span class="line"></span><br><span class="line">            # 在队列中连续取到30条数据并添加到info列表中</span><br><span class="line">            for _ in range(30):</span><br><span class="line">                info = self.shop_info_queue.get()</span><br><span class="line">                info_list.append((0,) + info)  # 0占位id字段</span><br><span class="line">                self.shop_info_queue.task_done()</span><br><span class="line"></span><br><span class="line">            sql = &quot;&quot;&quot;</span><br><span class="line">                insert into amazon_shop values (%s, %s, %s, %s);</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">            try:</span><br><span class="line">                # 批量保存, executemany接收的是一个迭代对象</span><br><span class="line">                self.cursor.executemany(sql, info_list)</span><br><span class="line">                self.db.commit()</span><br><span class="line">                print(&#x27;数据插入成功:&#x27;, info_list)</span><br><span class="line">            except Exception as e:</span><br><span class="line">                print(&#x27;数据插入失败:&#x27;, e)</span><br><span class="line">                logger.error(&#x27;数据库保存失败:&#x27;, e)</span><br><span class="line">                self.db.rollback()</span><br><span class="line"></span><br><span class="line">    def main(self):</span><br><span class="line">        &quot;&quot;&quot;启动函数，使用多线程的方式并发爬取数据&quot;&quot;&quot;</span><br><span class="line">        self.create_table()</span><br><span class="line"></span><br><span class="line">        thread_list = list()  # 创建线程对象列表</span><br><span class="line"></span><br><span class="line">        # 获取代理ip线程</span><br><span class="line">        for _ in range(5):</span><br><span class="line">            thread_list.append(threading.Thread(target=self.get_ip))</span><br><span class="line">        # 获取分类地址线程</span><br><span class="line">        thread_list.append(threading.Thread(target=self.get_classify_url))</span><br><span class="line">        # 获取商品地址线程</span><br><span class="line">        for _ in range(10):</span><br><span class="line">            thread_list.append(threading.Thread(target=self.get_detail_url))</span><br><span class="line">        # 获取数据详细信息并清洗的线程</span><br><span class="line">        for _ in range(10):</span><br><span class="line">            thread_list.append(threading.Thread(target=self.parse_shop_info))</span><br><span class="line">        # 保存数据的线程，只能有一个，防止资源竞争</span><br><span class="line">        thread_list.append(threading.Thread(target=self.save_shop_info))</span><br><span class="line"></span><br><span class="line">        for thread in thread_list:</span><br><span class="line">            thread.daemon = True</span><br><span class="line">            thread.start()</span><br><span class="line"></span><br><span class="line">        # 延迟等待子线程对象启动</span><br><span class="line">        print(f&quot;正在等待子线程启动...&quot;)</span><br><span class="line">        time.sleep(4)</span><br><span class="line"></span><br><span class="line">        for q in [self.classify_url_queue, self.detail_url_queue, self.shop_info_queue]:</span><br><span class="line">            q.join()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    logger.add(&#x27;runtime_&#123;time&#125;.log&#x27;, rotation=&#x27;500 MB&#x27;)</span><br><span class="line">    amazon = Amazon()</span><br><span class="line">    amazon.main()</span><br></pre></td></tr></table></figure>
                    
                </div>

                
                        
<div class="post-copyright-info-container border-box">
    <div class="copyright-info-content border-box">
        <div class="copyright-info-top border-box">
            <div class="copyright-post-title border-box text-ellipsis">
                爬虫_1_基础知识
            </div>

            <div class="copyright-post-link border-box text-ellipsis">
                2023/09/01/爬虫_1_基础知识/
            </div>
        </div>

        <div class="copyright-info-bottom border-box">
            <div class="copyright-post-author bottom-item">
                <div class="type">
                    作者
                </div>
                <div class="content">xfblog</div>
            </div>

            <div class="post-time bottom-item">
                <div class="type">
                    发布于
                </div>
                <div class="content">2023-09-01 00:00</div>
            </div>


            <div class="post-license bottom-item">
                <div class="type">
                    许可
                </div>
                <div class="content tooltip" data-tooltip-content="CC BY-NC-SA 4.0">
                    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" target="_blank">
                        
                            <i class="fa-brands fa-creative-commons"></i>
                            <i class="fa-brands fa-creative-commons-by"></i>
                            <i class="fa-brands fa-creative-commons-nc"></i>
                            <i class="fa-brands fa-creative-commons-sa"></i>
                        
                    </a>
                </div>
            </div>
        </div>

        <i class="copyright-bg fa-solid fa-copyright"></i>
    </div>
    <div class="copy-copyright-info flex-center tooltip" data-tooltip-content="复制版权信息" data-tooltip-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Python/">Python</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                            <div class="post-share-container border-box">
    <ul class="share-list-wrap border-box">
        <li class="qq share-item border-box flex-center tooltip"
            data-tooltip-content="分享到 QQ"
        >
            <i class="fa-brands fa-qq"></i>
        </li>
        <li class="wechat share-item border-box flex-center tooltip tooltip-img"
            data-tooltip-content="分享到微信"
            data-tooltip-img-tip="微信扫一扫"
            data-tooltip-img-style="background-color: #fff; top: -10px; padding: 0.6rem 0.6rem 0.1rem 0.6rem;"
        >
            <i class="fa-brands fa-weixin"></i>
        </li>
        <li class="weibo share-item border-box flex-center tooltip"
            data-tooltip-content="分享到微博"
        >
            <i class="fa-brands fa-weibo"></i>
        </li>
    </ul>
</div>

                        
                    </div>
                </div>

                
                    

<div class="reward-author-container border-box flex-center">
    <div class="reward-btn border-box flex-center tooltip tooltip-img"
            data-tooltip-img-url="/images/zfb.webp"
            data-tooltip-img-trigger="click"
            data-tooltip-img-style="top: -6px;"
    >
        <i class="fa-solid fa-gift"></i>&nbsp;创作不易，打赏随意，您的支持是我更新的动力💪！
    </div>
</div>

                

                
                    <div class="post-nav border-box">
                        
                            <div class="prev-post">
                                <a class="prev"
                                   rel="prev"
                                   href="/2023/10/01/%E7%88%AC%E8%99%AB_2_Scrapy%E6%A1%86%E6%9E%B6/"
                                   title="爬虫_2_Scrapy框架"
                                >
                                    <span class="left arrow-icon flex-center">
                                        <i class="fas fa-chevron-left"></i>
                                    </span>
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">爬虫_2_Scrapy框架</span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/2023/08/01/Python_5_%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"
                                   title="Python_5_数据结构"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">Python_5_数据结构</span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
            <div class="pc-post-toc right-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C"><span class="nav-text">一、计算机网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-B-S%E5%92%8CC-S%E6%9E%B6%E6%9E%84"><span class="nav-text">1. B&#x2F;S和C&#x2F;S架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-OSI%E6%A8%A1%E5%9E%8B"><span class="nav-text">2. OSI模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-TCP-IP%E5%8D%8F%E8%AE%AE"><span class="nav-text">3. TCP&#x2F;IP协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Socket%E7%BC%96%E7%A8%8B"><span class="nav-text">4. Socket编程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-HTTP%E5%8D%8F%E8%AE%AE"><span class="nav-text">5. HTTP协议</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E4%BA%86%E8%A7%A3%E7%88%AC%E8%99%AB"><span class="nav-text">二、了解爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%B8%8E%E5%93%8D%E5%BA%94%E5%A4%B4"><span class="nav-text">1. 请求头与响应头</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Robots%E5%8D%8F%E8%AE%AE"><span class="nav-text">2. Robots协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-unicode%E5%92%8Cutf8%E7%BC%96%E7%A0%81"><span class="nav-text">3. unicode和utf8编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%BD%91%E7%AB%99%E5%88%86%E6%9E%90"><span class="nav-text">4. 网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-miniconda%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">5. miniconda的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Requests%E6%A8%A1%E5%9D%97"><span class="nav-text">三、Requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%93%8D%E5%BA%94%E5%B1%9E%E6%80%A7"><span class="nav-text">1. 响应属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AD%97%E8%8A%82%E6%B5%81%E4%B8%8B%E8%BD%BD"><span class="nav-text">2. 字节流下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%AF%B7%E6%B1%82%E9%87%8D%E5%AE%9A%E5%90%91"><span class="nav-text">3. 请求重定向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-SSL%E8%AF%81%E4%B9%A6%E9%94%99%E8%AF%AF"><span class="nav-text">4. SSL证书错误</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-retrying%E6%A8%A1%E5%9D%97"><span class="nav-text">5. retrying模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Session%E4%BC%9A%E8%AF%9D"><span class="nav-text">6. Session会话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E4%BB%A3%E7%90%86ip"><span class="nav-text">7. 代理ip</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96"><span class="nav-text">四、数据提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%B3%E4%BA%8EJSON%E6%95%B0%E6%8D%AE"><span class="nav-text">1. 关于JSON数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-xpath%E8%AF%AD%E6%B3%95"><span class="nav-text">2. xpath语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-jsonpath%E8%AF%AD%E6%B3%95"><span class="nav-text">3. jsonpath语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-bs4%E8%AF%AD%E6%B3%95"><span class="nav-text">4. bs4语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-text">5. 正则表达式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-text">五、数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-csv%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="nav-text">1. csv文件存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-json%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="nav-text">2. json文件存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8"><span class="nav-text">3. MySQL数据库存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Peewee%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">4. Peewee的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-MongDB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8"><span class="nav-text">5. MongDB数据库存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-md5%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D"><span class="nav-text">6. md5数据去重</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">六、并发爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%8D%8F%E7%A8%8B%E9%85%8D%E5%90%88requests"><span class="nav-text">1. 协程配合requests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-aiohttp%E4%BB%A3%E6%9B%BFrequests"><span class="nav-text">2. aiohttp代替requests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-aiomysql%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">3. aiomysql的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%B1%BD%E8%BD%A6%E4%B9%8B%E5%AE%B6%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">4. 汽车之家数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-JoinableQueue%E9%98%9F%E5%88%97"><span class="nav-text">5. JoinableQueue队列</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E7%BB%BC%E5%90%88%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B"><span class="nav-text">七、综合并发爬虫案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">1. 多线程实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">2. 线程池实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">3. 多进程实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%94%A8%E5%8D%8F%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">4. 用协程实现并发爬虫</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81selenium%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6"><span class="nav-text">八、selenium测试框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%9F%BA%E6%9C%AC%E7%BD%91%E9%A1%B5%E4%BD%BF%E7%94%A8"><span class="nav-text">1. 基本网页使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%8E%B7%E5%8F%96%E8%AF%B7%E6%B1%82%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF"><span class="nav-text">2. 获取请求相关信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%85%83%E7%B4%A0%E5%AE%9A%E4%BD%8D%E6%96%B9%E6%B3%95"><span class="nav-text">3. 元素定位方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-cookie%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86"><span class="nav-text">4. cookie信息处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-WebDriverWait%E7%AD%89%E5%BE%85"><span class="nav-text">5. WebDriverWait等待</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E7%BB%95%E8%BF%87%E6%A3%80%E6%B5%8B%E9%85%8D%E7%BD%AE"><span class="nav-text">6. 绕过检测配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-ActionChains%E5%8A%A8%E4%BD%9C%E9%93%BE"><span class="nav-text">7. ActionChains动作链</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-selenium%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86"><span class="nav-text">8. selenium异常处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-%E5%94%AF%E5%93%81%E4%BC%9A%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">9. 唯品会数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-pyppeteer%E6%A1%86%E6%9E%B6"><span class="nav-text">10. pyppeteer框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">九、IP代理池</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86%E9%87%87%E9%9B%86%E8%84%9A%E6%9C%AC"><span class="nav-text">1. 免费代理采集脚本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">2. 付费代理的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BA%9A%E9%A9%AC%E9%80%8A%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">3. 亚马逊数据采集</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>
        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="copyright-info info-item">
    Copyright&nbsp;&copy;&nbsp;<span>2021</span>&nbsp;-&nbsp;2025&nbsp;<a href="https://xfblog.cn/" target="_self">小傅博客</a>&nbsp;-&nbsp;All&nbsp;rights&nbsp;reserved.&nbsp;&nbsp;&nbsp;&nbsp;
    
            <!-- &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">xfblog</a> -->
        
    </div>

    <!-- <div class="theme-info info-item">
        由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
    </div> -->

    

    

        
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="post-tools-list border-box">
        <!-- PC encrypt again -->
        

        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        

        <!-- PC full screen -->
        <li class="tools-item flex-center full-screen">
            <i class="fa-solid fa-expand"></i>
        </li>
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <!-- toggle mode -->
        
            <li class="tools-item tool-toggle-theme-mode flex-center">
                <i class="fas fa-moon"></i>
            </li>
        

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        <!-- to bottom -->
        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C"><span class="nav-text">一、计算机网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-B-S%E5%92%8CC-S%E6%9E%B6%E6%9E%84"><span class="nav-text">1. B&#x2F;S和C&#x2F;S架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-OSI%E6%A8%A1%E5%9E%8B"><span class="nav-text">2. OSI模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-TCP-IP%E5%8D%8F%E8%AE%AE"><span class="nav-text">3. TCP&#x2F;IP协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Socket%E7%BC%96%E7%A8%8B"><span class="nav-text">4. Socket编程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-HTTP%E5%8D%8F%E8%AE%AE"><span class="nav-text">5. HTTP协议</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E4%BA%86%E8%A7%A3%E7%88%AC%E8%99%AB"><span class="nav-text">二、了解爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%AF%B7%E6%B1%82%E5%A4%B4%E4%B8%8E%E5%93%8D%E5%BA%94%E5%A4%B4"><span class="nav-text">1. 请求头与响应头</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Robots%E5%8D%8F%E8%AE%AE"><span class="nav-text">2. Robots协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-unicode%E5%92%8Cutf8%E7%BC%96%E7%A0%81"><span class="nav-text">3. unicode和utf8编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%BD%91%E7%AB%99%E5%88%86%E6%9E%90"><span class="nav-text">4. 网站分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-miniconda%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">5. miniconda的使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Requests%E6%A8%A1%E5%9D%97"><span class="nav-text">三、Requests模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%93%8D%E5%BA%94%E5%B1%9E%E6%80%A7"><span class="nav-text">1. 响应属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AD%97%E8%8A%82%E6%B5%81%E4%B8%8B%E8%BD%BD"><span class="nav-text">2. 字节流下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%AF%B7%E6%B1%82%E9%87%8D%E5%AE%9A%E5%90%91"><span class="nav-text">3. 请求重定向</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-SSL%E8%AF%81%E4%B9%A6%E9%94%99%E8%AF%AF"><span class="nav-text">4. SSL证书错误</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-retrying%E6%A8%A1%E5%9D%97"><span class="nav-text">5. retrying模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Session%E4%BC%9A%E8%AF%9D"><span class="nav-text">6. Session会话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E4%BB%A3%E7%90%86ip"><span class="nav-text">7. 代理ip</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96"><span class="nav-text">四、数据提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%B3%E4%BA%8EJSON%E6%95%B0%E6%8D%AE"><span class="nav-text">1. 关于JSON数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-xpath%E8%AF%AD%E6%B3%95"><span class="nav-text">2. xpath语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-jsonpath%E8%AF%AD%E6%B3%95"><span class="nav-text">3. jsonpath语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-bs4%E8%AF%AD%E6%B3%95"><span class="nav-text">4. bs4语法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-text">5. 正则表达式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-text">五、数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-csv%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="nav-text">1. csv文件存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-json%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="nav-text">2. json文件存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8"><span class="nav-text">3. MySQL数据库存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Peewee%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">4. Peewee的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-MongDB%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8"><span class="nav-text">5. MongDB数据库存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-md5%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D"><span class="nav-text">6. md5数据去重</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">六、并发爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%8D%8F%E7%A8%8B%E9%85%8D%E5%90%88requests"><span class="nav-text">1. 协程配合requests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-aiohttp%E4%BB%A3%E6%9B%BFrequests"><span class="nav-text">2. aiohttp代替requests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-aiomysql%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">3. aiomysql的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%B1%BD%E8%BD%A6%E4%B9%8B%E5%AE%B6%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">4. 汽车之家数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-JoinableQueue%E9%98%9F%E5%88%97"><span class="nav-text">5. JoinableQueue队列</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E7%BB%BC%E5%90%88%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB%E6%A1%88%E4%BE%8B"><span class="nav-text">七、综合并发爬虫案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">1. 多线程实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">2. 线程池实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">3. 多进程实现并发爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%94%A8%E5%8D%8F%E7%A8%8B%E5%AE%9E%E7%8E%B0%E5%B9%B6%E5%8F%91%E7%88%AC%E8%99%AB"><span class="nav-text">4. 用协程实现并发爬虫</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81selenium%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6"><span class="nav-text">八、selenium测试框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%9F%BA%E6%9C%AC%E7%BD%91%E9%A1%B5%E4%BD%BF%E7%94%A8"><span class="nav-text">1. 基本网页使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%8E%B7%E5%8F%96%E8%AF%B7%E6%B1%82%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF"><span class="nav-text">2. 获取请求相关信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%85%83%E7%B4%A0%E5%AE%9A%E4%BD%8D%E6%96%B9%E6%B3%95"><span class="nav-text">3. 元素定位方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-cookie%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86"><span class="nav-text">4. cookie信息处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-WebDriverWait%E7%AD%89%E5%BE%85"><span class="nav-text">5. WebDriverWait等待</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E7%BB%95%E8%BF%87%E6%A3%80%E6%B5%8B%E9%85%8D%E7%BD%AE"><span class="nav-text">6. 绕过检测配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-ActionChains%E5%8A%A8%E4%BD%9C%E9%93%BE"><span class="nav-text">7. ActionChains动作链</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-selenium%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86"><span class="nav-text">8. selenium异常处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-%E5%94%AF%E5%93%81%E4%BC%9A%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">9. 唯品会数据采集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-pyppeteer%E6%A1%86%E6%9E%B6"><span class="nav-text">10. pyppeteer框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81IP%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-text">九、IP代理池</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86%E9%87%87%E9%9B%86%E8%84%9A%E6%9C%AC"><span class="nav-text">1. 免费代理采集脚本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">2. 付费代理的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BA%9A%E9%A9%AC%E9%80%8A%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="nav-text">3. 亚马逊数据采集</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>





<!-- common js -->
<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/toggle-theme.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/libs/anime.min.js"></script>

<!-- local search -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/local-search.js"></script>


<!-- lazyload -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/lazyload.js"></script>


<div class="pjax">
    <!-- home page -->
    

    <!-- post page -->
    
        <!-- post-helper -->
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/post-helper.js"></script>

        <!-- toc -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/toc.js"></script>
        

        <!-- copyright-info -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/copyright-info.js"></script>
        

        <!-- share -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/share.js"></script>
        
    

    <!-- categories page -->
    

    <!-- links page -->
    

    <!-- photos page -->
    

    <!-- tools page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart()
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd()
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'))
            KEEP.initExecute()
        });
    });
</script>




    
        
            
<script class="custom-inject-js" src="/js/run_time.js" data-pjax></script>

        
    
        
            
<script class="custom-inject-js" src="/js/code_click_copy.js" data-pjax></script>

        
    
        
            
<script class="custom-inject-js" src="/js/copyrightpro.js" data-pjax></script>

        
    

</body>
</html>
