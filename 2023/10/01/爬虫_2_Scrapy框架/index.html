<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="Hexo Theme Keep">
    <meta name="description" content="Hexo Theme Keep">
    <meta name="author" content="xfblog">
    
    <title>
        
            爬虫_2_Scrapy框架 |
        
        XFBLOG.CN
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/favicon.webp">
    
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/fontawesome.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/regular.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/solid.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/font/css/brands.min.css">
    
        
            
                
<link rel="stylesheet" href="/css/custom-1.css">

            
        
    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"zh-CN","path":"search.xml"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"XFBLOG.CN","author":"xfblog","avatar":"/images/avatar.webp","logo":"/images/logo.webp","favicon":"/images/favicon.webp"},"menu":{"home":"/ || fa-solid fa-home","JumpTo":{"children":[{"JS逆向_3_扣算法专题":"/2024/05/01/JS逆向_3_扣算法专题/"},{"JS逆向_4_补环境专题":"/2024/07/01/JS逆向_4_补环境专题/"}]},"archives":"/archives || fa-solid fa-box-archive","tags":"/tags || fa-solid fa-tags","links":"/links || fa-solid fa-link","about":"/about || fa-solid fa-circle-info"},"first_screen":{"enable":true,"background_img":"/images/home_lights.webp","background_img_dark":"/images/home_dark.webp","description":null,"hitokoto":false},"social_contact":{"enable":true,"links":{"github":"https://github.com/xfblogcn","weixin":null,"qq":null,"weibo":null,"zhihu":null,"twitter":null,"x":null,"facebook":null,"email":null}},"scroll":{"progress_bar":false,"percent":true,"hide_header":true},"home":{"announcement":"版权声明：本站所有文章均为 xfblog.cn 小傅原创，请遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。","category":true,"tag":true,"post_datetime":"created","post_datetime_format":"YYYY-MM-DD"},"post":{"author_badge":{"enable":true,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":true,"min2read":true},"datetime_format":"YYYY-MM-DD","copyright_info":true,"share":true,"reward":{"enable":true,"img_link":"/images/zfb.webp","text":"创作不易，打赏随意，您的支持是我更新的动力💪！"}},"code_block":{"tools":{"enable":true,"style":"default"},"highlight_theme":"obsidian"},"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true,"layout":"right"},"website_count":{"busuanzi_count":{"enable":true,"site_uv":false,"site_pv":false,"page_pv":true}},"local_search":{"enable":true,"preload":true},"comment":{"enable":false,"use":"valine","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"gitalk":{"github_id":null,"github_admins":null,"repository":null,"client_id":null,"client_secret":null,"proxy":null},"twikoo":{"env_id":null,"region":null,"version":"1.6.36"},"waline":{"server_url":null,"reaction":false,"version":"3.2.1"},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":true},"lazyload":{"enable":true},"cdn":{"enable":true,"provider":"jsdelivr"},"pjax":{"enable":true},"footer":{"since":2021,"word_count":false,"site_deploy":{"enable":false,"provider":"github","url":null},"record":{"enable":true,"list":[{"code":"蜀ICP备2021021033号-1","link":"https://beian.miit.gov.cn"},{"code":"川公网安备 51138102000134号","link":"http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=51138102000134"}]}},"inject":{"enable":true,"css":["/css/custom-1.css"],"js":["/js/run_time.js","/js/code_click_copy.js","/js/copyrightpro.js"]},"root":"","source_data":{},"version":"4.2.5"}
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"}
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"}
    KEEP.language_copy_copyright = {"copy":"复制版权信息","copied":"已复制","title":"原文标题","author":"原文作者","link":"原文链接"}
  </script>
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Keep" type="application/atom+xml">
</head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>



<main class="page-container border-box">
    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="border-box header-content">
        <div class="left flex-start border-box">
            
                <a class="logo-image border-box" href="/">
                    <img src="/images/logo.webp">
                </a>
            
            <a class="site-name border-box" href="/">
               XFBLOG.CN
            </a>
        </div>

        <div class="right border-box">
            <div class="pc border-box">
                <ul class="menu-list border-box">
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-home"></i>
                                
                                首页
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box has-sub-menu">
                            <a class="menu-text-color border-box" href="javascript:void(0);">
                                
                                JUMPTO
                                
                                    <i class="menu-text-color collapse-icon fa-solid fa-angle-down"></i>
                                
                            </a>
                            
                                <ul class="sub-menu-list border-box">
                                    
                                        
                                        <li class="sub-menu-item border-box ">
                                            <a class="menu-text-color border-box flex-start" href="/2024/05/01/JS%E9%80%86%E5%90%91_3_%E6%89%A3%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98/">
                                                
                                                JS逆向_3_扣算法专题
                                            </a>
                                        </li>
                                    
                                        
                                        <li class="sub-menu-item border-box ">
                                            <a class="menu-text-color border-box flex-start" href="/2024/07/01/JS%E9%80%86%E5%90%91_4_%E8%A1%A5%E7%8E%AF%E5%A2%83%E4%B8%93%E9%A2%98/">
                                                
                                                JS逆向_4_补环境专题
                                            </a>
                                        </li>
                                    
                                </ul>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/archives">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                
                                归档
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/tags">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-tags"></i>
                                
                                标签
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/links">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-link"></i>
                                
                                友链
                                
                            </a>
                            
                        </li>
                    
                        
                        <li class="menu-item flex-start border-box">
                            <a class="menu-text-color border-box" href="/about">
                                
                                    <i class="menu-text-color menu-icon fa-solid fa-circle-info"></i>
                                
                                关于
                                
                            </a>
                            
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="menu-text-color fas search fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile border-box flex-start">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list border-box">
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-home"></i>
                                </span>
                            
                            首页
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box has-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="javascript:void(0);">
                            
                            JUMPTO
                        </a>
                        
                            <i class="right-side collapse-icon fa-solid fa-angle-left"></i>
                        
                    </label>
                    
                        <ul class="drawer-sub-menu-list border-box">
                            
                                
                                <li class="sub-menu-item border-box">
                                    <a class="drawer-menu-text-color border-box flex-start" href="/2024/05/01/js%E9%80%86%E5%90%91_3_%E6%89%A3%E7%AE%97%E6%B3%95%E4%B8%93%E9%A2%98/">
                                        
                                        JS逆向_3_扣算法专题
                                    </a>
                                </li>
                            
                                
                                <li class="sub-menu-item border-box">
                                    <a class="drawer-menu-text-color border-box flex-start" href="/2024/07/01/js%E9%80%86%E5%90%91_4_%E8%A1%A5%E7%8E%AF%E5%A2%83%E4%B8%93%E9%A2%98/">
                                        
                                        JS逆向_4_补环境专题
                                    </a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/archives">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-box-archive"></i>
                                </span>
                            
                            归档
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/tags">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-tags"></i>
                                </span>
                            
                            标签
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/links">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-link"></i>
                                </span>
                            
                            友链
                        </a>
                        
                    </label>
                    
                </li>
            
                
                <li class="drawer-menu-item border-box not-sub-menu">
                    <label class="drawer-menu-label border-box">
                        <a class="drawer-menu-text-color left-side flex-start border-box" href="/about">
                            
                                <span class="menu-icon-wrap border-box flex-center">
                                    <i class="drawer-menu-text-color menu-icon fa-solid fa-circle-info"></i>
                                </span>
                            
                            关于
                        </a>
                        
                    </label>
                    
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        爬虫_2_Scrapy框架
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.webp">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">xfblog</span>
                                
                                    <span class="author-badge">Lv3</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-calendar-plus"></i>&nbsp;
                <span class="datetime">2023-10-01</span>
            </span>

            
                <span class="meta-info-item post-update-date">
                    <i class="icon fa-solid fa-file-pen"></i>&nbsp;
                    <span class="datetime" data-updated="Mon Dec 09 2024 10:26:09 GMT+0800">2024-12-09</span>
                </span>
            
        

        
            <span class="meta-info-item post-category border-box"><i class="icon fas fa-folder"></i>&nbsp;
                <ul class="post-category-ul">
                    
                            <li class="category-item"><a href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></li>
                        
                    
                </ul>
            </span>
        

        
            <span class="post-tag meta-info-item border-box">
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Python/">Python</a></li>
                        
                    
                </ul>
            </span>
        

        
        
            <span class="meta-info-item post-wordcount">
                <i class="icon fas fa-file-word"></i>&nbsp;<span>16k 字</span>
            </span>
        
        
            <span class="meta-info-item post-min2read">
                <i class="icon fas fa-clock"></i>&nbsp;<span>69 分钟</span>
            </span>
        
        
            <span class="meta-info-item post-pv">
                <i class="icon fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
            </span>
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body ">
                    

                    
                         <h2 id="一、了解Scrapy框架"><a href="#一、了解Scrapy框架" class="headerlink" title="一、了解Scrapy框架"></a>一、了解Scrapy框架</h2><blockquote>
<p><strong>什么是 Scrapy ？</strong></p>
<p><strong>Scrapy是一个用于爬取网站内容，提取结构性数据而编写的 Python 开源爬虫应用框架。它提供了一套基础结构，使得用户能够轻松地定义如何从网站中提取数据，以及如何存储和处理这些数据，具有高度的可定制性和可扩展性（<a class="link"   href="https://docs.scrapy.org/en/latest/index.html"  target="_blank" rel="noopener">官方文档<i class="fas fa-external-link-alt"></i></a>、<a class="link"   href="https://www.osgeo.cn/scrapy/intro/install.html"  target="_blank" rel="noopener">中文官方文档<i class="fas fa-external-link-alt"></i></a>）</strong></p>
<p><strong>Event-driven networking（支持异步）：</strong>Scrapy 是异步爬虫框架，它使用 Twisted 库实现异步操作。虽然 <strong>Scrapy 的默认配置是单线程的，但它通过 Twisted 提供的事件循环机制实现了异步并发</strong>，这使得 Scrapy 能够在单个进程中高效地处理并发请求</p>
</blockquote>
<h3 id="1-scrapy的工作原理"><a href="#1-scrapy的工作原理" class="headerlink" title="1. scrapy的工作原理"></a>1. scrapy的工作原理</h3><ul>
<li><strong>引擎（Scrapy Engine）：</strong>Scrapy引擎是整个框架的核心，负责控制系统所有组件之间的数据流，并在发生特定动作时触发事件</li>
<li><strong>调度器（Scheduler）：</strong>调度器接收来自引擎的请求，并将其排队，同时去除重复的网址，以供稍后引擎请求它们时使用，用户可以根据自己的需求定制调度器</li>
<li><strong>下载器（Downloader）：</strong>下载器是所有组件中负担最大的，它用于高速地下载网络上的资源，负责获取网页并将其提供给引擎，引擎然后将其提供给Spiders爬虫</li>
<li><strong>爬虫（Spiders）：</strong>爬虫是由用户编写的自定义类，用于解析响应并从中提取数据信息（所谓的实体Item）或其他要跟踪的请求</li>
<li><strong>实体管道（Item Pipeline）：</strong>实体管道用于接收爬虫传过来的数据，负责在爬虫提取数据后对其进行处理，包括清理、验证和持久化存储</li>
<li><strong>中间件（Middleware）：</strong>中间件相当于过滤器，夹在不同部分之间（<strong>下载器中间件</strong>位于引擎和下载器之间，<strong>爬虫中间件</strong>位于引擎和爬虫之间）截获数据流，并进行特殊的加工处理</li>
</ul>
<img    
                       lazyload
                       alt="image"
                       data-src="https://xfblog.cn/images/1701832575-05e785c7a0438d6.webp"
                         style="width:80%;margin:0 auto;"
                 >

<ul>
<li><strong>scrapy的工作流程：</strong><ol>
<li>由爬虫将起始的 URL 构造成 Requests 对象 ⇒ 爬虫中间件 ⇒ 引擎 ⇒ 调度器</li>
<li>调度器把 Requests 对象 ⇒ 引擎 ⇒ 下载中间件 ⇒ 下载器</li>
<li>下载器发送请求，获取 Responses 响应 ⇒ 下载中间件 ⇒ 引擎 ⇒ 爬虫中间件 ⇒ 爬虫</li>
<li>爬虫提取 URL 地址，组装成 Requests 对象 ⇒ 爬虫中间件 ⇒ 引擎 ⇒ 调度器（重复步骤2）</li>
<li>爬虫提取数据 ⇒ 引擎 ⇒ 实体管道处理和保存数据</li>
</ol>
</li>
<li><strong>工作流程概述：</strong>首先由爬虫从 URL 起始队列中取出待请求 URL 并将它传递给调度器，然后引擎从调度器的待爬队列中取待爬 URL 并会经下载中间件交给下载器进行数据下载，随后将获取的数据经爬虫中间件交给爬虫模块进行分析并提取出目标数据和新 URL，最后将新 URL 传递给调度器的同时把解析的目标数据交给持久化模块进行处理。此外，在数据流动过程中能够配置各种中间件，进行必要的功能扩展。</li>
</ul>
<h3 id="2-scrapy的项目初识"><a href="#2-scrapy的项目初识" class="headerlink" title="2. scrapy的项目初识"></a>2. scrapy的项目初识</h3><blockquote>
<p><strong>Scrapy安装：<code>pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple</code></strong></p>
</blockquote>
<ul>
<li><strong>scrapy的使用流程：</strong><ol>
<li><strong>创建scrapy项目</strong>：scrapy startproject 项目名字</li>
<li><strong>进入scrapy项目文件夹</strong>：cd 项目名字</li>
<li><strong>创建爬虫脚本</strong>：scrapy genspider 爬虫名 允许爬取的域名</li>
<li><strong>运行scrapy爬虫</strong>：scrapy crawl 爬虫名字</li>
</ol>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline  # 运行指令的工具</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">创建scrapy项目：scrapy startproject &lt;project_name&gt;</span><br><span class="line">进入scrapy项目文件夹：cd &lt;project_name&gt;</span><br><span class="line">创建爬虫脚本：scrapy genspider &lt;spider_name&gt; &lt;spider_url&gt;</span><br><span class="line">运行scrapy项目（一定要在项目目录下）：scrapy crawl &lt;spider_name&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class BaiduSpider(scrapy.Spider):</span><br><span class="line">    # 爬虫脚本名称（在一个项目中，可能有多个爬虫，通过名称可以区分它们）</span><br><span class="line">    name = &quot;xfblog&quot;</span><br><span class="line">    # 允许爬取的域名列表（限制爬虫只能爬取指定域名下的页面，避免爬虫无意间爬取到其他域名的内容）</span><br><span class="line">    allowed_domains = [&quot;xfblog.cn&quot;]</span><br><span class="line">    # 定义爬虫启动时要访问的起始URL列表（爬虫将从这些URL开始请求页面并调用parse方法处理响应）</span><br><span class="line">    start_urls = [&quot;http://xfblog.cn/&quot;]</span><br><span class="line"></span><br><span class="line">    # 指定response参数的类型为HtmlResponse，可以使用使用HtmlResponse提供的方法来方便地进行HTML解析</span><br><span class="line">    def parse(self, response: HtmlResponse, **kwargs):</span><br><span class="line">        &quot;&quot;&quot;对start_urls中的URL爬取之后，回调的函数&quot;&quot;&quot;</span><br><span class="line">        print(f&quot;响应对象的url地址：&#123;response.url&#125;&quot;)</span><br><span class="line">        print(f&quot;响应对象的响应头：&#123;response.headers&#125;&quot;)</span><br><span class="line">        print(f&quot;响应对象对应的请求url地址：&#123;response.request.url&#125;&quot;)</span><br><span class="line">        print(f&quot;响应对象对应的请求头：&#123;response.request.headers&#125;&quot;)</span><br><span class="line">        print(f&quot;响应对象的状态码：&#123;response.status&#125;&quot;)</span><br><span class="line">        print(f&quot;响应对象的二进制数据（content）：&#123;response.body&#125;&quot;)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # 使用cmdline.execute()方法来执行启动命令，默认打印日志信息</span><br><span class="line">    cmdline.execute(&#x27;scrapy crawl xfblog&#x27;.split())</span><br><span class="line">    # cmdline.execute(&quot;scrapy crawl xfblog --nolog&quot;.split())  # 设置日志的最低级别</span><br></pre></td></tr></table></figure>

<h3 id="3-scrapy的断点续爬"><a href="#3-scrapy的断点续爬" class="headerlink" title="3. scrapy的断点续爬"></a>3. scrapy的断点续爬</h3><blockquote>
<p><strong>Scrapy 中的暂停和恢复爬取的功能通常被称为”断点续爬”（Resuming Crawls）</strong></p>
<p><strong>使用：</strong>断点续爬允许在爬取过程中暂停爬虫，然后在之后的某个时间点或在不同的机器上继续爬取。这对于大规模的爬取任务和长时间运行的爬虫来说非常有用，因为它们可能会因为各种原因中断，例如网络问题、程序崩溃等</p>
</blockquote>
<ul>
<li><p><strong>断点续爬过程：</strong></p>
<ul>
<li><strong>启动爬虫（支持暂停恢复的命令）：<code>scrapy crawl 爬虫名 -s JOBDIR=缓存scrapy信息的路径</code></strong></li>
<li><strong>暂停爬虫：在终端里按下一次 <code>ctrl+c</code>（只能一次）</strong></li>
<li><strong>恢复爬虫（与启动相同的命令）：<code>scrapy crawl 爬虫名 -s JOBDIR=缓存scrapy信息的路径</code></strong></li>
</ul>
</li>
<li><p><strong>注意：</strong>如果有多个爬虫脚本，那么一定要分别指定缓存路径，不能使用同一个缓存路径</p>
</li>
</ul>
<h3 id="4-scrapy的遍历算法"><a href="#4-scrapy的遍历算法" class="headerlink" title="4. scrapy的遍历算法"></a>4. scrapy的遍历算法</h3><blockquote>
<p>这里介绍了两种图遍历算法，在后续的使用中也可以看出scrapy所使用的是<strong>深度优先搜索（DFS）</strong></p>
</blockquote>
<ul>
<li><strong>深度优先搜索（DFS）</strong>：<ul>
<li><strong>策略：</strong>从起始节点开始，尽可能深入到图中的某一分支，直到不能再深入为止，然后回溯到上一层，再继续深入另一个分支。这个过程递归进行，直到遍历完整个图</li>
<li><strong>实现方式：</strong>通常使用递归或栈来实现。在递归实现中，每访问一个节点时，递归调用会不断深入到下一层</li>
<li><strong>特点：</strong>深度优先搜索常用于解决路径问题，例如查找图中的路径、解决迷宫等。在树的深度优先搜索中，它可以用来遍历所有节点</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个用于标记已访问节点的集合</span><br><span class="line">visited = set()</span><br><span class="line"></span><br><span class="line"># 节点图</span><br><span class="line">graph = &#123;</span><br><span class="line">    &#x27;A&#x27;: [&#x27;B&#x27;, &#x27;C&#x27;],</span><br><span class="line">    &#x27;B&#x27;: [&#x27;D&#x27;, &#x27;E&#x27;],</span><br><span class="line">    &#x27;C&#x27;: [&#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;],</span><br><span class="line">    &#x27;D&#x27;: [],</span><br><span class="line">    &#x27;E&#x27;: [&#x27;I&#x27;],</span><br><span class="line">    &#x27;F&#x27;: [],</span><br><span class="line">    &#x27;G&#x27;: [],</span><br><span class="line">    &#x27;H&#x27;: [],</span><br><span class="line">    &#x27;I&#x27;: []</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 通过递归的方式实现深度优先遍历函数</span><br><span class="line">def dfs_recursion(graph, node):</span><br><span class="line">    # 将当前节点标记为已访问，并输出节点值</span><br><span class="line">    visited.add(node)</span><br><span class="line">    print(node, end=&#x27; &#x27;)</span><br><span class="line"></span><br><span class="line">    # 遍历当前节点的所有邻居节点</span><br><span class="line">    for neighbor in graph[node]:</span><br><span class="line">        # 如果邻居节点未被访问，则递归调用自己遍历邻居的邻居节点</span><br><span class="line">        if neighbor not in visited:</span><br><span class="line">            dfs_recursion(graph, neighbor)</span><br><span class="line"></span><br><span class="line">dfs_recursion(graph, &#x27;A&#x27;)  # A B D E I C F G H</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>广度优先搜索（BFS）</strong>：<ul>
<li><strong>策略：</strong>从起始节点开始，先访问当前节点的所有相邻节点，然后依次访问这些相邻节点的相邻节点，依此类推（这个过程使用队列来实现）</li>
<li><strong>实现方式：</strong>使用队列来保存待访问的节点，每次从队列中取出一个节点访问，然后将其所有未访问的相邻节点加入队列</li>
<li><strong>特点：</strong>广度优先搜索常用于解决最短路径问题，例如查找两个节点之间的最短路径。在树的广度优先搜索中，它可以用来层序遍历所有节点</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from collections import deque</span><br><span class="line"></span><br><span class="line"># 节点图</span><br><span class="line">graph = &#123;</span><br><span class="line">    &#x27;A&#x27;: [&#x27;B&#x27;, &#x27;C&#x27;],</span><br><span class="line">    &#x27;B&#x27;: [&#x27;D&#x27;, &#x27;E&#x27;],</span><br><span class="line">    &#x27;C&#x27;: [&#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;],</span><br><span class="line">    &#x27;D&#x27;: [],</span><br><span class="line">    &#x27;E&#x27;: [&#x27;I&#x27;],</span><br><span class="line">    &#x27;F&#x27;: [],</span><br><span class="line">    &#x27;G&#x27;: [],</span><br><span class="line">    &#x27;H&#x27;: [],</span><br><span class="line">    &#x27;I&#x27;: []</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 使用deque队列实现的广度优先遍历函数</span><br><span class="line">def bfs(graph, start):</span><br><span class="line">    # 创建一个队列，用于存储需要遍历的节点</span><br><span class="line">    queue = deque([start])</span><br><span class="line">    # 创建一个列表，用于存储已经遍历过的节点</span><br><span class="line">    visited = list()</span><br><span class="line"></span><br><span class="line">    while queue:</span><br><span class="line">        # 取出队列的第一个节点，并将其标记为已访问</span><br><span class="line">        node = queue.popleft()</span><br><span class="line">        visited.append(node)</span><br><span class="line"></span><br><span class="line">        # 遍历当前节点的所有邻居节点</span><br><span class="line">        for neighbor in graph[node]:</span><br><span class="line">            # 如果该邻居节点还未被访问，则将其加入队列</span><br><span class="line">            if neighbor not in visited:</span><br><span class="line">                queue.append(neighbor)</span><br><span class="line"></span><br><span class="line">    return visited</span><br><span class="line"></span><br><span class="line">print(bfs(graph, &#x27;A&#x27;))  # [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;, &#x27;G&#x27;, &#x27;H&#x27;, &#x27;I&#x27;]</span><br></pre></td></tr></table></figure>

<h3 id="5-scrapy的日志级别"><a href="#5-scrapy的日志级别" class="headerlink" title="5. scrapy的日志级别"></a>5. scrapy的日志级别</h3><blockquote>
<p>Scrapy框架提供了强大的日志功能，基于Python的内置 logging 模块，用于记录和处理在爬取过程中发生的错误，并且有不同的日志级别（<strong>默认为DEBUG级别</strong>）：</p>
<ul>
<li><strong>日志配置：在配置文件settings.py中可以通过LOG_LEVEL参数调整日志级别</strong></li>
<li>**日志使用：由 scrapy.Spider 类提供了用于记录日志的属性 <code>self.logger.日志级别(&quot;&quot;)</code> **</li>
<li><strong>日志级别（按照严重程度排序）： DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL</strong></li>
</ul>
</blockquote>
<ul>
<li><strong>DEBUG（调试）:</strong><ul>
<li><strong>含义：</strong> 用于详细和调试信息。通常在开发和调试阶段使用，记录程序内部状态、变量值等详细信息。</li>
<li><strong>示例：</strong> <code>self.logger.debug(&quot;这是调试信息&quot;)</code></li>
</ul>
</li>
<li><strong>INFO（信息）:</strong><ul>
<li><strong>含义：</strong> 用于一般性的信息记录，用于表明应用程序的正常运行情况。通常是在生产环境中记录的级别。</li>
<li><strong>示例：</strong> <code>self.logger.info(&quot;应用程序成功启动&quot;)</code></li>
</ul>
</li>
<li><strong>WARNING（警告）:</strong><ul>
<li><strong>含义：</strong> 用于表示潜在的问题，不影响程序正常运行，但可能需要注意。通常用于警告可能会导致错误的情况。</li>
<li><strong>示例：</strong> <code>self.logger.warning(&quot;文件未找到，使用默认设置&quot;)</code></li>
</ul>
</li>
<li><strong>ERROR（错误）:</strong><ul>
<li><strong>含义：</strong> 用于表示错误情况，但程序仍然可以继续运行。通常用于记录捕获到的异常或其他导致程序不能正常工作的错误。</li>
<li><strong>示例：</strong> <code>self.logger.error(&quot;处理请求时发生错误&quot;)</code></li>
</ul>
</li>
<li><strong>CRITICAL（严重错误）:</strong><ul>
<li><strong>含义：</strong> 用于表示严重错误，可能导致程序无法继续运行。通常用于记录不可恢复的错误情况。</li>
<li><strong>示例：</strong> <code>self.logger.critical(&quot;严重错误：无法连接到数据库&quot;)</code></li>
</ul>
</li>
</ul>
<h2 id="二、Spider爬虫模块"><a href="#二、Spider爬虫模块" class="headerlink" title="二、Spider爬虫模块"></a>二、Spider爬虫模块</h2><blockquote>
<p><strong>dont_filter参数：</strong></p>
<ul>
<li><strong>作用：</strong>用于告诉框架在进行请求去重时是否忽略对该请求的去重检查</li>
<li><strong>用法：</strong>可以通过设置 <code>dont_filter=True</code> 来禁用请求 url 过滤</li>
<li><strong>默认值：</strong>在 scrapy.Request 请求方法中，默认 dont_filter&#x3D;False（<strong>默认开启过滤</strong>）</li>
</ul>
</blockquote>
<h3 id="1-重要函数"><a href="#1-重要函数" class="headerlink" title="1. 重要函数"></a>1. 重要函数</h3><blockquote>
<p><strong>需要重写 start_requests 方法的场景：</strong></p>
<ol>
<li>如果 start_urls 中的 url 是需要登录后才能访问的网页，则需重写此方法重新发起请求并手动添加cookie</li>
<li>如果需要开启请求 url 过滤，则需要重写此方法重新发起 scrapy.Request 请求，此请求默认开启过滤（因为 start_urls 中的 url 在被生成 Request 对象时，默认设置为关闭过滤，即 <code>dont_filter=True</code>）</li>
<li>如果 start_urls 中的 url 需要使用 POST 请求，则需要重写此方法使用 scrapy.FormRequest 发起请求</li>
</ol>
</blockquote>
<ul>
<li><p><strong>start_requests 函数：</strong></p>
<ul>
<li><p><strong>作用：</strong>用于生成初始请求，若不重写此函数，则默认从 start_urls 中生成初始请求</p>
</li>
<li><p><strong>回调函数：</strong>生成的请求默认使用 parse 作为回调函数，也可以指定回调函数，根据不同的条件，执行不同的回调解析函数</p>
</li>
</ul>
</li>
<li><p><strong>parse 函数：</strong></p>
<ul>
<li><strong>作用：</strong>是 Scrapy 爬虫中用于处理响应数据的默认回调函数，可以提取所需的信息，或跟进其他链接生成更多请求</li>
</ul>
</li>
<li><p><strong>注意：parse函数中可以使用yield语句生成多个请求或数据项，但只能是 BaseItem、Request、dict 和None 四种类型对象</strong></p>
<ul>
<li><strong>BaseItem：</strong>通过 yield 返回的 BaseItem 对象将被传递给 Item Pipeline 进行处理</li>
<li><strong>Request：</strong>通过 yield Request(…) 的方式，爬虫可以在解析函数中发起新的请求，继续爬取其他页面</li>
<li><strong>dict：</strong>通过 yield 返回字典对象，可以将提取到的数据传递给后续的解析函数或 Item Pipeline 进行处理</li>
<li><strong>None：</strong>yield None 通常用于结束当前解析函数的执行</li>
</ul>
</li>
<li><p><strong>response.xpath() 方法：</strong>接受一个 XPath 表达式作为参数，返回一个包含选定节点的 SelectorList 对象</p>
<ul>
<li><strong>extract()：</strong>用于获取 SelectorList 中所有匹配的节点的文本内容，返回一个包含所有文本内容的列表</li>
<li><strong>extract_first()：</strong>用于获取 SelectorList 中第一个匹配的节点的文本内容，如果没有匹配的节点，则返回 None</li>
</ul>
</li>
<li><p><strong>response.urljoin() 方法：</strong>作用是将相对URL转换为绝对URL</p>
</li>
</ul>
<h3 id="2-数据提取、跟进链接"><a href="#2-数据提取、跟进链接" class="headerlink" title="2. 数据提取、跟进链接"></a>2. 数据提取、跟进链接</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line">import redis</span><br><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">class Top250Spider(scrapy.Spider):</span><br><span class="line">    name = &quot;top250&quot;</span><br><span class="line">    allowed_domains = [&quot;movie.douban.com&quot;, &quot;doubanio.com&quot;]</span><br><span class="line"></span><br><span class="line">    # start_urls = [&quot;https://movie.douban.com/top250&quot;]</span><br><span class="line"></span><br><span class="line">    def __init__(self, *args, **kwargs):</span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line">        self.redis_cli = redis.Redis()</span><br><span class="line"></span><br><span class="line">    # 程序退出时关闭redis连接</span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.redis_cli.close()</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        url = &quot;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&quot;</span><br><span class="line">        for page in range(0, 226, 25):</span><br><span class="line">            url_md5 = hashlib.md5(str(url.format(page)).encode()).hexdigest()</span><br><span class="line">            if self.redis_cli.sismember(&#x27;douban:tx_work_url_filter&#x27;, url_md5):</span><br><span class="line">                print(&quot;url重复...&quot;)</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                self.redis_cli.sadd(&#x27;douban:tx_work_url_filter&#x27;, url_md5)</span><br><span class="line">            yield scrapy.Request(url=url.format(page))</span><br><span class="line"></span><br><span class="line">    def parse(self, response: HtmlResponse, **kwargs):</span><br><span class="line">        li_list = response.xpath(&quot;//div[@id=&#x27;content&#x27;]//ol[@class=&#x27;grid_view&#x27;]/li&quot;)</span><br><span class="line">        for item in li_list:</span><br><span class="line">            info = dict()</span><br><span class="line">            info[&#x27;type_&#x27;] = &#x27;info&#x27;</span><br><span class="line">            info[&#x27;img_url&#x27;] = item.xpath(&quot;.//img/@src&quot;).extract_first()</span><br><span class="line">            info[&#x27;title&#x27;] = item.xpath(&quot;.//div[@class=&#x27;hd&#x27;]/a/span/text()&quot;).extract_first()</span><br><span class="line">            info[&#x27;star&#x27;] = item.xpath(&quot;.//span[@class=&#x27;rating_num&#x27;]/text()&quot;).extract_first()</span><br><span class="line">            # info[&#x27;comment&#x27;] = item.xpath(&quot;.//div[@class=&#x27;star&#x27;]/span[last()]/text()&quot;).extract_first()</span><br><span class="line">            yield info</span><br><span class="line">            yield scrapy.Request(url=info[&#x27;img_url&#x27;], callback=self.img_parse, cb_kwargs=&#123;&quot;title&quot;: info[&#x27;title&#x27;]&#125;)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def img_parse(response: HtmlResponse, title):</span><br><span class="line">        yield &#123;</span><br><span class="line">            &quot;type_&quot;: &#x27;image&#x27;,</span><br><span class="line">            &quot;img_name&quot;: title + &#x27;.png&#x27;,</span><br><span class="line">            &quot;img_content&quot;: response.body</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    cmdline.execute(&#x27;scrapy crawl top250&#x27;.split())</span><br></pre></td></tr></table></figure>

<h3 id="3-请求post携带表单"><a href="#3-请求post携带表单" class="headerlink" title="3. 请求post携带表单"></a>3. 请求post携带表单</h3><blockquote>
<p>案例：巨潮资讯网（<code>http://www.cninfo.com.cn/new/commonUrl?url=disclosure/list/notice#szse</code>）</p>
</blockquote>
<ul>
<li>对于携带表单数据的post请求，通过 <strong>scrapy.FormRequest()</strong> 发起</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline</span><br><span class="line"></span><br><span class="line">class CninfoSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;cninfo&quot;</span><br><span class="line">    allowed_domains = [&quot;www.cninfo.com.cn&quot;]</span><br><span class="line"></span><br><span class="line">    # start_urls = [&quot;http://www.cninfo.com.cn/new/index&quot;]</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        url = &quot;http://www.cninfo.com.cn/new/disclosure&quot;</span><br><span class="line">        for page in range(1, 26):</span><br><span class="line">            data = &#123;</span><br><span class="line">                &quot;column&quot;: &quot;szse_latest&quot;,</span><br><span class="line">                &quot;pageNum&quot;: str(page),</span><br><span class="line">                &quot;pageSize&quot;: &quot;30&quot;,</span><br><span class="line">                &quot;sortName&quot;: &quot;&quot;,</span><br><span class="line">                &quot;sortType&quot;: &quot;&quot;,</span><br><span class="line">                &quot;clusterFlag&quot;: &quot;true&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            yield scrapy.FormRequest(url=url, formdata=data)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        print(response)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    cmdline.execute(&quot;scrapy crawl cninfo&quot;.split())</span><br></pre></td></tr></table></figure>

<h3 id="4-请求post携带json"><a href="#4-请求post携带json" class="headerlink" title="4. 请求post携带json"></a>4. 请求post携带json</h3><blockquote>
<p>案例：网易招聘（<code>https://hr.163.com/job-list.html</code>）</p>
</blockquote>
<ul>
<li>对于携带json数据的post请求，通过 <strong>scrapy.http.JsonRequest()</strong> 发起</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline</span><br><span class="line">from scrapy.http import JsonRequest, HtmlResponse</span><br><span class="line"></span><br><span class="line">class WorkinfoSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;workinfo&quot;</span><br><span class="line">    allowed_domains = [&quot;hr.163.com&quot;]</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        url = &quot;https://hr.163.com/api/hr163/position/queryPage&quot;</span><br><span class="line">        json_params = &#123;</span><br><span class="line">            &quot;currentPage&quot;: 1,</span><br><span class="line">            &quot;pageSize&quot;: 10</span><br><span class="line">        &#125;</span><br><span class="line">        yield JsonRequest(url=url, data=json_params)</span><br><span class="line"></span><br><span class="line">    def parse(self, response: HtmlResponse, **kwargs):</span><br><span class="line">        print(response.json())</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    cmdline.execute(&quot;scrapy crawl workinfo --nolog&quot;.split())</span><br></pre></td></tr></table></figure>

<h2 id="三、Middleware和Extension"><a href="#三、Middleware和Extension" class="headerlink" title="三、Middleware和Extension"></a>三、Middleware和Extension</h2><blockquote>
<p><strong>Middleware：</strong>中间件主要有<strong>下载器中间件</strong>和<strong>爬虫中间件</strong>，在Scrapy默认的情况下，两种中间件都在 middlewares.py 一个文件中，但由于两种中间件使用方法相同和功能重复，所以<strong>常使用下载器中间件</strong></p>
<p><strong>付费IP代理：</strong><a class="link"   href="https://www.kuaidaili.com/doc/dev/sdk_http/#proxy_python-scrapy"  target="_blank" rel="noopener">快代理开发配置文档<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>扩展（Extension）：</strong>是一种机制，允许开发者通过添加自定义功能来扩展或修改Scrapy框架在运行过程中的行为，以满足特定需求或执行额外的任务</p>
<p><strong>注意：有一个思维误区，就是将”通过selenium获取到登录后的cookies”的行为使用扩展实现，保证全局只执行一次，但是扩展一般是根据爬虫的启动而启动的，如果要在start_reqests中使用此扩展，那么此时爬虫还没有启动，扩展也就自然无法运行，拿不到cookies；而如果要在parse函数中使用此扩展，虽然确实可以拿到登录后的cookies，但是这意味着每一次请求的每一次响应都将获取一次cookies，这是没有必要的</strong></p>
<p><strong>解决：将”通过selenium获取到登录后的cookies”的逻辑在spider爬虫模块的start_requests方法中实现</strong>（即保证了在爬虫启动前执行，拿到登录后的cookies，又保证了只执行一次）</p>
</blockquote>
<h3 id="1-重要函数-1"><a href="#1-重要函数-1" class="headerlink" title="1. 重要函数"></a>1. 重要函数</h3><ul>
<li><p><strong>from_crawler 函数：</strong></p>
<ul>
<li><strong>作用：</strong>是一个特殊的类方法，用于在中间件实例化之前设置一些配置</li>
<li><strong>参数crawler：</strong>是一个Scrapy的 Crawler 对象，包含了当前爬虫的配置信息</li>
<li><strong>用法：</strong>通常被用来从配置中获取一些值，并将它们传递给中间件的实例。这使得中间件可以根据爬虫的配置动态地调整其行为</li>
</ul>
</li>
<li><p><strong>process_request 函数：</strong></p>
<ul>
<li><strong>返回None：</strong>将该Request对象传递给下载器，或通过引擎传递给其他权重更低的下载中间件的process_request方法（函数没有return也是返回None）</li>
<li><strong>返回Response对象：</strong>不再请求，把response返回给引擎</li>
<li><strong>返回Request对象：</strong>把request对象通过引擎再次交给调度器，此时将不通过其他权重低的process_request方法</li>
</ul>
</li>
<li><p><strong>process_response 函数：</strong></p>
<ul>
<li><strong>返回Resposne对象：</strong>将该Resposne对象传递给爬虫处理，或通过引擎传递给其他权重更低的下载中间件的process_response方法</li>
<li><strong>返回Request对象：</strong>通过引擎再次交给调取器进行请求，再次通过当前下载中间件的process_request方法，而不会通过其他权重低的process_request方法</li>
</ul>
</li>
</ul>
<h3 id="2-指纹去重中间件"><a href="#2-指纹去重中间件" class="headerlink" title="2. 指纹去重中间件"></a>2. 指纹去重中间件</h3><ul>
<li><strong>Js 逆向结合 Scrapy 框架</strong>：如果 POST 请求携带的数据是经过 js 加密且每次生成不一样，则在分布式中就无法通过请求对象的指纹完成去重，而需要自定义一个可以用于去重的参数生成指纹（例如加密前的明文参数）</li>
<li><strong>注意配置 scrapy_redis 去重器：</strong><code>DUPEFILTER_CLASS = &quot;myproject.middlewares.MyDupeFilter&quot;</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 自定义的 Scrapy 去重中间件</span><br><span class="line">from scrapy_redis.dupefilter import RFPDupeFilter</span><br><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line"># 定义自定义的去重过滤器类 MyDupeFilter，继承自 RFPDupeFilter</span><br><span class="line">class MyDupeFilter(RFPDupeFilter):</span><br><span class="line">    def __init__(self, server, key, debug=False):</span><br><span class="line">        # 调用父类的构造方法，传递参数 server, key, debug</span><br><span class="line">        super(MyDupeFilter, self).__init__(server, key, debug)</span><br><span class="line">        # 初始化一个集合用于存储指纹信息，确保请求的唯一性</span><br><span class="line">        self.fingerprints = set()</span><br><span class="line">        # 将传递的 key 作为指纹的键值</span><br><span class="line">        self.fingerprint_key = key</span><br><span class="line"></span><br><span class="line">    def request_seen(self, request):</span><br><span class="line">        # 从请求的 meta 中提取关键信息，例如 &#x27;unique_key&#x27;（例如加密前的明文参数）</span><br><span class="line">        unique_key = request.meta.get(&#x27;unique_key&#x27;)</span><br><span class="line">        # 使用 SHA-1 哈希算法对关键信息进行哈希，生成指纹</span><br><span class="line">        fp = hashlib.sha1(unique_key.encode()).hexdigest()</span><br><span class="line"></span><br><span class="line">        # 如果指纹已存在于集合中，表示请求重复</span><br><span class="line">        if fp in self.fingerprints:</span><br><span class="line">            return True</span><br><span class="line">        # 将新的指纹添加到集合中</span><br><span class="line">        self.fingerprints.add(fp)</span><br><span class="line"></span><br><span class="line">        # 如果设置了 fingerprint_key，将指纹添加到 Redis 中，以确保分布式去重</span><br><span class="line">        if self.fingerprint_key:</span><br><span class="line">            self.server.sadd(self.fingerprint_key, fp)</span><br><span class="line"></span><br><span class="line">        # 返回 False 表示请求不重复</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure>

<h3 id="2-付费代理ip扩展"><a href="#2-付费代理ip扩展" class="headerlink" title="2. 付费代理ip扩展"></a>2. 付费代理ip扩展</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<ul>
<li>自定义快代理的scrapy扩展（<strong>extend_ip.py</strong>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import threading</span><br><span class="line">import requests</span><br><span class="line">from scrapy import signals</span><br><span class="line"></span><br><span class="line"># 提取代理IP的api，一次提取10个（生成链接：https://www.kuaidaili.com/dps/genapiurl/）</span><br><span class="line">api_url = &#x27;https://dps.kdlapi.com/api/getdps/?secret_id=o88cwu0sh49ls75kntwt&amp;num=1&amp;signature=dtb4yamicu504fub6llgx7mjabckit83&amp;pt=1&amp;format=json&amp;sep=1&#x27;</span><br><span class="line">foo = True</span><br><span class="line"></span><br><span class="line">class Proxy:</span><br><span class="line">    def __init__(self, ):</span><br><span class="line">        self._proxy_list = requests.get(api_url).json().get(&#x27;data&#x27;).get(&#x27;proxy_list&#x27;)</span><br><span class="line"></span><br><span class="line">    @property  # p.proxy_list：自动执行此方法，并获取方法的返回值</span><br><span class="line">    def proxy_list(self):</span><br><span class="line">        return self._proxy_list</span><br><span class="line"></span><br><span class="line">    @proxy_list.setter  # p.proxy_list=&#x27;abc&#x27;：自动执行此方法，并将&#x27;abc&#x27;赋值给list参数</span><br><span class="line">    def proxy_list(self, list):</span><br><span class="line">        self._proxy_list = list</span><br><span class="line"></span><br><span class="line">pro = Proxy()</span><br><span class="line"></span><br><span class="line">class MyExtend:</span><br><span class="line">    def __init__(self, crawler):</span><br><span class="line">        self.crawler = crawler</span><br><span class="line">        # 将自定义方法绑定到scrapy信号上，使程序与spider引擎同步启动与关闭</span><br><span class="line">        crawler.signals.connect(self.start, signals.engine_started)</span><br><span class="line">        crawler.signals.connect(self.close, signals.spider_closed)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(crawler)</span><br><span class="line"></span><br><span class="line">    def start(self):</span><br><span class="line">        t = threading.Thread(target=self.extract_proxy)</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    def extract_proxy(self):</span><br><span class="line">        while foo:</span><br><span class="line">            pro.proxy_list = requests.get(api_url).json().get(&#x27;data&#x27;).get(&#x27;proxy_list&#x27;)</span><br><span class="line">            # 设置每一分钟提取一次ip，更新proxy_list列表中的ip，保证都是有效ip</span><br><span class="line">            time.sleep(60)</span><br><span class="line"></span><br><span class="line">    def close(self):</span><br><span class="line">        global foo</span><br><span class="line">        foo = False</span><br></pre></td></tr></table></figure>

<h3 id="3-付费代理中间件"><a href="#3-付费代理中间件" class="headerlink" title="3. 付费代理中间件"></a>3. 付费代理中间件</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<ul>
<li>自定义HeadersDownloaderMiddleware中间件（<strong>middlewares.py</strong>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line">from .extend_ip import pro</span><br><span class="line"></span><br><span class="line">class HeadersDownloaderMiddleware:</span><br><span class="line">    def __init__(self, user_agent_list=None):</span><br><span class="line">        self.user_agent_list = user_agent_list</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        # 从settings配置中获取user_agent值</span><br><span class="line">        user_agent_list = crawler.settings.get(&#x27;USER_AGENTS_LIST&#x27;)</span><br><span class="line">        # 创建中间件实例并传递配置值</span><br><span class="line">        return cls(user_agent_list)</span><br><span class="line"></span><br><span class="line">    def process_request(self, request: HtmlResponse, spider):</span><br><span class="line">        &quot;&quot;&quot;每一个request通过下载器中间件，此方法都会运行一次，方法process_response同理&quot;&quot;&quot;</span><br><span class="line">        # print(&quot;下载中间件---------process_request---------&quot;)</span><br><span class="line">        request.headers[&#x27;User-Agent&#x27;] = random.choice(self.user_agent_list)</span><br><span class="line"></span><br><span class="line">        # 设置免费代理</span><br><span class="line">        # request.meta[&#x27;proxy&#x27;] = &#x27;http://127.0.0.1:7890&#x27;</span><br><span class="line"></span><br><span class="line">        # 设置付费代理，需要在settings.py中激活extend_ip扩展</span><br><span class="line">        username = &quot;d4550779926&quot;</span><br><span class="line">        password = &quot;kzsel6pa&quot;</span><br><span class="line">        # 如果当前ip不可用，那么process_response重试，这里会在代理池中重新选择一个代理ip</span><br><span class="line">        proxy = random.choice(pro.proxy_list)</span><br><span class="line">        # print(f&quot;当前请求使用的代理ip：&#123;proxy&#125;&quot;)</span><br><span class="line">        request.meta[&#x27;proxy&#x27;] = f&quot;http://&#123;username&#125;:&#123;password&#125;@&#123;proxy&#125;/&quot;</span><br><span class="line"></span><br><span class="line">        # 不写returen：默认返回None</span><br><span class="line"></span><br><span class="line">    def process_response(self, request, response: HtmlResponse, spider):</span><br><span class="line">        # print(&quot;下载中间件---------process_response---------&quot;)</span><br><span class="line">        # print(request.headers)  # 通过request.headers直接拿到请求头</span><br><span class="line">        if not response.status == 200:</span><br><span class="line">            request.dont_filter = True  # 使重复发送的请求对象能够再次进入调度器的队列</span><br><span class="line">            return request</span><br><span class="line">        return response  # 最后必须将处理后的响应返回给引擎</span><br></pre></td></tr></table></figure>

<h3 id="4-中间件selenium配置"><a href="#4-中间件selenium配置" class="headerlink" title="4. 中间件selenium配置"></a>4. 中间件selenium配置</h3><blockquote>
<p>案例2：腾讯招聘（<code>https://careers.tencent.com/search.html?keyword=python</code>）</p>
<p><strong>注意：这里选择在下载中间件的process_request方法中配置selenium，是因为这里的爬取逻辑</strong></p>
<ul>
<li><strong>爬取逻辑：先在spider模块的start_requests方法中构造循环发起多个翻页请求，然后每一个scrapy.Request()都在经过下载中间件时被开启selenium获取了网页源代码，并且将响应返回给了引擎，这样其实每一个请求都没有走到downloader下载器，就已经获取到响应返回了</strong></li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">from scrapy import signals</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line"></span><br><span class="line">class SeleniumDownloaderMiddleware:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.browser = webdriver.Chrome()</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        s = cls()</span><br><span class="line">        # 检测爬虫状态，如果爬虫关闭则调用spider_closed方法</span><br><span class="line">        crawler.signals.connect(s.spider_closed, signal=signals.spider_closed)</span><br><span class="line">        return s</span><br><span class="line"></span><br><span class="line">    def spider_closed(self):</span><br><span class="line">        self.browser.quit()</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        self.browser.get(request.url)</span><br><span class="line">        wait_ob = WebDriverWait(self.browser, timeout=10)</span><br><span class="line">        # 等待所有工作信息存在（加载完毕）</span><br><span class="line">        wait_ob.until(EC.presence_of_all_elements_located((By.XPATH, &#x27;//div[@class=&quot;recruit-list&quot;]&#x27;)))</span><br><span class="line">        body = self.browser.page_source  # 获取网页源代码</span><br><span class="line"></span><br><span class="line">        # 这种做法将不会经过下载器进行请求，通过selenium直接将获取到的body打包成响应返回给引擎</span><br><span class="line">        return HtmlResponse(body=body, url=request.url, request=request, encoding=&#x27;utf-8&#x27;)</span><br></pre></td></tr></table></figure>

<h2 id="四、Item实体模块"><a href="#四、Item实体模块" class="headerlink" title="四、Item实体模块"></a>四、Item实体模块</h2><h3 id="1-重要知识点"><a href="#1-重要知识点" class="headerlink" title="1. 重要知识点"></a>1. 重要知识点</h3><ul>
<li><p><strong>Item模块：</strong>用于定义数据模型，即在爬取过程中需要提取的数据的结构。模块中定义了 Scrapy Item 类，描述了爬取到的数据应该包含哪些字段以及它们的数据类型，可以在多个爬虫中重复使用</p>
</li>
<li><p><strong>ItemLoader工具：</strong>用于加载和处理爬取到的数据并填充到 Item 对象中，在Item的填充过程中可以应用多个处理器，进行数据清洗、转换、预处理等操作</p>
<ul>
<li><strong>input_processor：</strong>应用于从<strong>网页中提取的原始数据</strong>的处理器，用于在将数据填充到 <strong>ItemLoader 之前</strong>对其进行清理、转换或其他操作（<strong>清理文本、转换格式等</strong>）</li>
<li><strong>output_processor：</strong>应用于从 <strong>ItemLoader 加载的数据</strong>的处理器，用于在数据填充到 <strong>Item 之前</strong>对其进行清理、转换或其他操作（<strong>取第一个非空值、拼接字符串等</strong>）</li>
</ul>
</li>
<li><p><strong>itemloaders.processors 模块中提供了一系列用于数据处理的类：</strong></p>
<ul>
<li><strong>TakeFirst：</strong>从处理器返回的结果中取第一个非空值，常用于获取单一数据项</li>
<li><strong>MapCompose：</strong>对一个可迭代对象中的每个元素应用一个或多个处理器</li>
<li><strong>Compose：</strong>将多个处理器组合在一起，按照顺序应用</li>
<li><strong>Join：</strong>将列表中的字符串连接成单个字符串，默认连接符为””（空字符串）</li>
<li><strong>Identity：</strong>返回输入的原始值，即不做任何处理直接返回输入数据本身</li>
</ul>
</li>
<li><p><strong>注意：这些处理器类可以单独使用，也可以组合在一起形成处理器链，通常用在 ItemLoader 的 input_processor 和 output_processor 参数中</strong></p>
</li>
<li><p><strong>add_xpath() 方法：</strong></p>
<ul>
<li><strong>作用：</strong>在spider爬虫中创建ItemLoader实例，并使用 add_xpath 方法将数据填充到 ItemLoader 中，最后yield生成item对象</li>
</ul>
<ul>
<li><strong>返回值：</strong>在填充数据时，xpath若匹配成功则返回一个列表，匹配失败则返回None</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">    # 创建一个 ItemLoader 实例，并指定要加载的 Item 类</span><br><span class="line">    loader = ItemLoader(item=MyItem(), response=response)</span><br><span class="line"></span><br><span class="line">    # 使用 add_xpath 方法添加数据，第一个参数是Item的字段名，第二个参数是XPath表达式</span><br><span class="line">    loader.add_xpath(&#x27;title&#x27;, &#x27;//h1/text()&#x27;)</span><br><span class="line">    loader.add_xpath(&#x27;description&#x27;, &#x27;//div[@class=&quot;description&quot;]/text()&#x27;)</span><br><span class="line"></span><br><span class="line">    # 完成加载并生成 Item 对象</span><br><span class="line">    yield loader.load_item()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="五、Pipline管道模块"><a href="#五、Pipline管道模块" class="headerlink" title="五、Pipline管道模块"></a>五、Pipline管道模块</h2><h3 id="1-重要函数-2"><a href="#1-重要函数-2" class="headerlink" title="1. 重要函数"></a>1. 重要函数</h3><ul>
<li><p><strong>from_settings(cls, settings) 类方法：</strong></p>
<ul>
<li><strong>作用：</strong>是 Scrapy 中的一个约定，用于从配置文件中读取设置并初始化相关对象，通常在 Pipeline 中使用，但在其他模块组件中也可以使用</li>
<li><strong>参数settings：</strong>是一个字典，可以直接通过 key 获取到 setting.py 配置文件中的配置项</li>
</ul>
</li>
<li><p><strong>process_item(self, item, spider)函数：</strong></p>
<ul>
<li><strong>作用：</strong>用于处理从爬虫中传递过来的 Item 对象，如数据清洗、持久化存储等（<strong>此函数不可省略</strong>）</li>
<li><strong>参数item：</strong>爬虫传递过来的 Item 对象，是一个字典或类似字典的对象，包含了爬虫抓取到的数据</li>
<li><strong>参数spider：</strong>爬虫实例，可以通过 spider.name 获取爬虫名称</li>
</ul>
</li>
<li><p><strong>process_item返回值：</strong></p>
<ul>
<li><strong>返回 Item 对象：</strong>则该对象将继续在后续的 Pipeline 中进行处理</li>
<li><strong>返回 None 对象：</strong>表示该 Item 的处理过程将停止，不再传递给其他 Pipeline，<strong>但是后续权重更低的 pipeline 的 process_item 方法接受到的 item 将会是None</strong></li>
<li><strong>返回 DropItem 异常：</strong>表示丢弃该 Item（**<code>raise DropItem(&quot;&quot;)</code>**）</li>
</ul>
</li>
<li><p><strong>open_spider函数：</strong></p>
<ul>
<li><strong>作用：</strong>在爬虫启动时被调用，用于执行一些初始化操作，如链接数据库、准备文件句柄等（<strong>只执行一次</strong>）</li>
</ul>
</li>
<li><p><strong>close_spider函数：</strong></p>
<ul>
<li><strong>作用：</strong>在爬虫关闭时被调用，用于执行一些清理工作，如关闭数据库连接、关闭文件等（<strong>只执行一次</strong>）</li>
</ul>
</li>
</ul>
<h3 id="2-多个管道item传递"><a href="#2-多个管道item传递" class="headerlink" title="2. 多个管道item传递"></a>2. 多个管道item传递</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<ul>
<li><strong>注意：虽然当一个 Item 被返回 None 时，该 Item 将不再被传递给后续的 Pipeline，但在 Scrapy 中，由于异步处理，某些情况下可能会出现 None 被传递到下一个 Pipeline 的情况，此时则需添加判断处理</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line">import os</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class DoubanImgPipeline:</span><br><span class="line">    &quot;&quot;&quot;保存图片数据&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 可能存在多个爬虫脚本，可以通过爬虫名称指定目标脚本</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            if item[&#x27;type_&#x27;] == &#x27;image&#x27;:</span><br><span class="line">                images_path = os.path.dirname(os.getcwd()) + &#x27;/download/&#x27;</span><br><span class="line">                if not os.path.exists(images_path):</span><br><span class="line">                    os.mkdir(images_path)</span><br><span class="line">                with open(images_path + item[&#x27;img_name&#x27;], &#x27;wb&#x27;) as fp:</span><br><span class="line">                    fp.write(item[&#x27;img_content&#x27;])</span><br><span class="line">                    print(f&quot;下载图片&#123;item[&#x27;img_name&#x27;]&#125;成功。。。&quot;)</span><br><span class="line">                # 返回None，表示该Item不再传递给后续Pipeline</span><br><span class="line">                return None</span><br><span class="line">            else:</span><br><span class="line">                # 将图片以外的数据（Item）传递给其它pipline管道，进行其余保存处理</span><br><span class="line">                return item</span><br><span class="line"></span><br><span class="line">class DoubanFilePipeline:</span><br><span class="line">    &quot;&quot;&quot;保存电影信息为文件&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            # 检查item是否为 None，如果是则不处理</span><br><span class="line">            if item is not None:</span><br><span class="line">                self.fp.write(json.dumps(item, ensure_ascii=False, indent=4))</span><br><span class="line">                print(f&quot;保存文件&#123;item[&#x27;title&#x27;]&#125;成功。。。&quot;)</span><br><span class="line">                return item</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.fp = open(&#x27;json.text&#x27;, &#x27;a&#x27;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.fp.close()</span><br><span class="line"></span><br><span class="line">class DoubanMongoPipeline:</span><br><span class="line">    &quot;&quot;&quot;保存电影信息到mongo数据库中&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            # 检查item是否为 None，如果是则不处理</span><br><span class="line">            if item is not None:</span><br><span class="line">                self.collection.insert_one(item)</span><br><span class="line">                print(f&quot;存储数据库&#123;item[&#x27;title&#x27;]&#125;成功。。。&quot;)</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.mongo_cli = pymongo.MongoClient()</span><br><span class="line">            self.collection = self.mongo_cli[&#x27;py_spider&#x27;][&#x27;douban_top250&#x27;]</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.mongo_cli.close()</span><br></pre></td></tr></table></figure>

<h3 id="3-数据存储去重"><a href="#3-数据存储去重" class="headerlink" title="3. 数据存储去重"></a>3. 数据存储去重</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<ul>
<li>在管道pipline中<strong>判断数据是否存储过</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class DoubanCheckPipeline:</span><br><span class="line">    &quot;&quot;&quot;对数据进行去重&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            # 创建一个新的字典，将 img_content 字段剔除，转为字符串并加密成 md5 值</span><br><span class="line">            # 原因：bytes对象不是可 JSON 序列化的对象，会序列化失败抛出异常</span><br><span class="line">            item_copy = &#123;key: value for key, value in item.items() if key != &#x27;img_content&#x27;&#125;</span><br><span class="line">            item_md5 = hashlib.md5(json.dumps(item_copy).encode()).hexdigest()</span><br><span class="line"></span><br><span class="line">            # 使用redis中set集合的 sismember 方法检查成员是否存在</span><br><span class="line">            if self.redis_cli.sismember(&#x27;douban:tx_work_item_filter&#x27;, item_md5):</span><br><span class="line">                # 抛出 DropItem 异常，表示丢弃该Item</span><br><span class="line">                raise DropItem(&#x27;数据已存在。。。&#x27;)</span><br><span class="line">            else:</span><br><span class="line">                self.redis_cli.sadd(&#x27;douban:tx_work_item_filter&#x27;, item_md5)</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.redis_cli = redis.Redis()</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        if spider.name == &#x27;top250&#x27;:</span><br><span class="line">            self.redis_cli.close()</span><br></pre></td></tr></table></figure>

<h3 id="4-请求url地址去重"><a href="#4-请求url地址去重" class="headerlink" title="4. 请求url地址去重"></a>4. 请求url地址去重</h3><blockquote>
<p>案例1：豆瓣电影top250（<code>https://movie.douban.com/top250</code>）</p>
</blockquote>
<ul>
<li>在爬虫spider或爬虫中间件中<strong>判断URL是否请求过</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line">import redis</span><br><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">class Top250Spider(scrapy.Spider):</span><br><span class="line">    name = &quot;top250&quot;</span><br><span class="line">    allowed_domains = [&quot;movie.douban.com&quot;, &quot;doubanio.com&quot;]</span><br><span class="line"></span><br><span class="line">    # start_urls = [&quot;https://movie.douban.com/top250&quot;]</span><br><span class="line"></span><br><span class="line">    def __init__(self, *args, **kwargs):</span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line">        self.redis_cli = redis.Redis()</span><br><span class="line"></span><br><span class="line">    # 程序退出时关闭redis连接</span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.redis_cli.close()</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        url = &quot;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&quot;</span><br><span class="line">        for page in range(0, 226, 25):</span><br><span class="line">            url_md5 = hashlib.md5(str(url.format(page)).encode()).hexdigest()</span><br><span class="line">            if self.redis_cli.sismember(&#x27;douban:tx_work_url_filter&#x27;, url_md5):</span><br><span class="line">                print(&quot;url重复...&quot;)</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                self.redis_cli.sadd(&#x27;douban:tx_work_url_filter&#x27;, url_md5)</span><br><span class="line">            yield scrapy.Request(url=url.format(page))</span><br></pre></td></tr></table></figure>

<h3 id="5-adbapi异步数据库接口"><a href="#5-adbapi异步数据库接口" class="headerlink" title="5. adbapi异步数据库接口"></a>5. adbapi异步数据库接口</h3><blockquote>
<p><strong>twisted中的adbapi：</strong></p>
<p>数据库pymysql的commit()和execute()在提交数据时，都是同步提交至数据库，由于scrapy框架数据的解析和异步多线程的，所以scrapy的数据解析速度要远高于数据的写入数据库的速度。如果数据写入过慢，会造成数据库写入的阻塞，影响数据库写入的效率。 使用<code>twisted</code>异步IO框架，实现数据的异步写入，通过多线程异步的形式对数据进行写入，可以提高数据的写入速度</p>
</blockquote>
<ul>
<li><strong>adbapi.ConnectionPool 类：</strong>创建一个数据库连接池对象，其中包括多个连接对象，每个连接对象在独立的线程中工作。adbapi只是提供了异步访问数据库的编程框架，再其内部依然使用<code>pymysql</code>这类库访问数据库</li>
<li><strong>dbpool.runInteraction(do_insert, item) 方法：</strong>异步调用do_insert函数，<code>dbpool</code>会选择连接池中的一个连接对象在独立线程中调用insert_db，其中参数item会被传给do_insert的第二个参数，传给do_insert的第一个参数是一个<code>Transaction</code>对象，其接口与<code>cursor</code>对象类似，可以调用<code>execute</code>方法执行SQL语句，do_insert执行后，不需要调用commit方法提交事务，<code>adbapi</code>会自动处理<strong>所有</strong>数据库事务</li>
<li><strong>query.addErrback(self.handle_error) 方法：</strong>addErrback方法是<code>Twisted</code>框架中用于处理<code>Deferred</code>对象中发生错误的回调方法。当在<code>runInteraction</code>中执行的操作发生异常时，该异常会被包装为一个<code>Failure</code>对象，然后通过addErrback指定的回调函数进行处理。<code>handle_error</code>方法中的参数<code>failure</code>就是包含了异常信息的<code>Failure</code>对象</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">class MysqlTwistedPipeline:</span><br><span class="line">    def __init__(self, dbpool):</span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_settings(cls, settings):</span><br><span class="line">        dbparms = dict(</span><br><span class="line">            host=settings[&quot;MYSQL_HOST&quot;],</span><br><span class="line">            db=settings[&quot;MYSQL_DBNAME&quot;],</span><br><span class="line">            user=settings[&quot;MYSQL_USER&quot;],</span><br><span class="line">            passwd=settings[&quot;MYSQL_PASSWORD&quot;],</span><br><span class="line">            charset=&quot;utf8&quot;,  # 字符集必须是utf8更规范，utf-8会报错</span><br><span class="line">            cursorclass=pymysql.cursors.DictCursor,  # 指定了在执行查询时返回的结果集的游标类型</span><br><span class="line">            use_unicode=True,  # 使用Unicode编码，更好地支持包含非ASCII字符的数据</span><br><span class="line">        )</span><br><span class="line">        dbpool = adbapi.ConnectionPool(&#x27;pymysql&#x27;, **dbparms)</span><br><span class="line"></span><br><span class="line">        # 在创建连接池的时候异步执行创建表的逻辑</span><br><span class="line">        cls.create_table(dbpool)</span><br><span class="line"></span><br><span class="line">        return cls(dbpool)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def create_table(cls, dbpool):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            create table if not exists table_name(</span><br><span class="line">                </span><br><span class="line">            );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 异步方式执行创建表的sql语句</span><br><span class="line">        dbpool.runInteraction(cls._create_table, sql)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def _create_table(cursor, sql):</span><br><span class="line">        cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 异步调用do_insert函数，执行sql语句，并传入item参数</span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        # 在runInteraction中执行的操作发生异常时，回调handle_error函数处理异常</span><br><span class="line">        query.addErrback(self.handle_error)</span><br><span class="line"></span><br><span class="line">    def do_insert(self, cursor, item):</span><br><span class="line">        &quot;&quot;&quot;注意：在构造sql语句时，最好使用参数化占位符来解决这个问题，</span><br><span class="line">        而不是直接在sql查询中插入值，这种方法可以避免sql注入的问题，并更容易处理特殊字符</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # `ON DUPLICATE KEY UPDATE`语法在插入数据时如果主键冲突，则可以指定字段进行更新的操作</span><br><span class="line">        sql = f&quot;&quot;&quot;</span><br><span class="line">            insert into cnblogs_news values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)</span><br><span class="line">            ON DUPLICATE KEY UPDATE totalview=VALUES(totalview);</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # 需要保证进入数据库的每个值都是str或者int类型的</span><br><span class="line">        params = []</span><br><span class="line">        cursor.execute(sql, params)  # 参数2接受列表或元组</span><br><span class="line"></span><br><span class="line">    def handle_error(self, failure):</span><br><span class="line">        print(f&quot;mysql处理错误：&#123;failure&#125;&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="六、Settings配置模块"><a href="#六、Settings配置模块" class="headerlink" title="六、Settings配置模块"></a>六、Settings配置模块</h2><h3 id="1-配置下载延迟"><a href="#1-配置下载延迟" class="headerlink" title="1. 配置下载延迟"></a>1. 配置下载延迟</h3><ul>
<li><strong>DOWNLOAD_DELAY：限制的是每个请求之间的延迟，而不是每组并发请求之前</strong></li>
<li><strong>注意：</strong>限制并发请求的数量是为了控制爬虫对目标网站的访问频率，防止对服务器造成过大的负担。即使设置了 DOWNLOAD_DELAY，如果并发请求数量过高，仍然可能在短时间内向服务器发送大量请求，这可能导致服务器拒绝服务、IP被封禁等问题</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 下载延迟，每个请求间的下载延迟，单位为秒(default: 0，默认是针对同一域名的每个请求延迟)</span><br><span class="line">DOWNLOAD_DELAY = 2</span><br><span class="line"># 随机下载延迟，范围是0.5到1.5倍DOWNLOAD_DELAY(default: True)</span><br><span class="line">DOWNLOAD_DELAY_RANDOMIZE = True</span><br><span class="line"></span><br><span class="line"># 全局限制，配置执行的最大并发请求数(default: 16)</span><br><span class="line"># CONCURRENT_REQUESTS = 4</span><br><span class="line"># 注意：CONCURRENT_REQUESTS &gt;= CONCURRENT_REQUESTS_PER_DOMAIN + CONCURRENT_REQUESTS_PER_IP</span><br><span class="line"></span><br><span class="line"># 下载延迟设置将仅支持以下其中一项：</span><br><span class="line"># CONCURRENT_REQUESTS_PER_DOMAIN = 2  # 同一域名下的并发请求数量(default: 8)</span><br><span class="line"># CONCURRENT_REQUESTS_PER_IP = 2  # 同一IP下的并发请求数量(default: 0，则优先级高于域名限制)</span><br></pre></td></tr></table></figure>

<h3 id="2-全部配置"><a href="#2-全部配置" class="headerlink" title="2. 全部配置"></a>2. 全部配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">BOT_NAME = &quot;project&quot;  # 表示Scrapy项目的名称</span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [&quot;project.spiders&quot;]  # 表示Scrapy项目中包含爬虫代码的Python模块，是一个列表</span><br><span class="line">NEWSPIDER_MODULE = &quot;project.spiders&quot;  # 表示创建新爬虫时的脚本文件放置的默认模块</span><br><span class="line"></span><br><span class="line"># Obey robots.txt rules</span><br><span class="line">ROBOTSTXT_OBEY = True  # 用于指定是否遵循网站的 robots.txt 规则</span><br><span class="line"></span><br><span class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="line">CONCURRENT_REQUESTS = 32  # 用于指定在任何给定时间点同时执行的最大请求数量</span><br><span class="line"></span><br><span class="line"># 默认情况下，RANDOMIZE_DOWNLOAD_DELAY = True，用于控制是否随机化DOWNLOAD_DELAY</span><br><span class="line"># 实际延迟时间：[0.5 * DOWNLOAD_DELAY, 1.5 * DOWNLOAD_DELAY]，即0.5到1.5倍之间</span><br><span class="line">DOWNLOAD_DELAY = 1</span><br><span class="line"># 下载延迟设置只会受到CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP中的一个影响</span><br><span class="line"># CONCURRENT_REQUESTS_PER_DOMAIN = 16  # 控制对于同一域名的并发请求数量</span><br><span class="line"># CONCURRENT_REQUESTS_PER_IP = 16  # 控制对于同一 IP 地址的并发请求数量</span><br><span class="line"></span><br><span class="line"># Disable cookies (enabled by default)</span><br><span class="line"># COOKIES_ENABLED = False  # 用于控制是否启用cookies（默认 cookies 是启用的）</span><br><span class="line"></span><br><span class="line"># 用于设置默认的请求头信息</span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;  </span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 用于启用或禁用爬虫中间件（数字越小，优先级越高）</span><br><span class="line"># SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">#    &quot;project.middlewares.ProjectSpiderMiddleware&quot;: 543,</span><br><span class="line"># &#125;</span><br><span class="line"></span><br><span class="line"># 用于启用或禁用下载器中间件（数字越小，优先级越高）</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &quot;project.middlewares.IpDownloaderMiddleware&quot;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 用于启用或禁用扩展（数字越小，优先级越高）</span><br><span class="line">EXTENSIONS = &#123;</span><br><span class="line">    &quot;project.extend_ip.MyExtend&quot;: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 用于指定数据处理的管道的设置，按照优先级顺序，依次将 Item 经过这些 Pipeline 进行处理</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &quot;project.pipelines.JsonExporterPipeline&quot;: 300,</span><br><span class="line">    &quot;project.pipelines.ScrapyJsonExporterPipeline&quot;: 301,</span><br><span class="line">    # &quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1,  # 使用scrapy默认的下载图片pipeline，优先级为1</span><br><span class="line">    &quot;project.pipelines.MyImagesPipeline&quot;: 1,</span><br><span class="line">    &quot;project.pipelines.MysqlTwistedPipeline&quot;: 302,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Enable and configure the AutoThrottle extension (disabled by default)</span><br><span class="line"># See https://docs.scrapy.org/en/latest/topics/autothrottle.html</span><br><span class="line"># AUTOTHROTTLE_ENABLED = True  # 启用或禁用自动限速扩展，默认是禁用的</span><br><span class="line"># The initial download delay</span><br><span class="line"># AUTOTHROTTLE_START_DELAY = 5  # 初始下载延迟，表示在 Scrapy 发送第一个请求之前等待的时间，默认为5秒</span><br><span class="line"># The maximum download delay to be set in case of high latencies</span><br><span class="line"># AUTOTHROTTLE_MAX_DELAY = 60  # 最大下载延迟，表示自动限速的最大等待时间</span><br><span class="line"># The average number of requests Scrapy should be sending in parallel to</span><br><span class="line"># each remote server</span><br><span class="line"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0  # 每个远程服务器平均并行发送的请求数量，默认为 1.0</span><br><span class="line"># Enable showing throttling stats for every response received:</span><br><span class="line"># AUTOTHROTTLE_DEBUG = False  # 启用或禁用自动限速的调试模式，默认是禁用的，开启调试会记录更多有关信息</span><br><span class="line"></span><br><span class="line"># Enable and configure HTTP caching (disabled by default)</span><br><span class="line"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span><br><span class="line"># HTTPCACHE_ENABLED = True  # 启用或禁用HTTP缓存，默认是禁用的</span><br><span class="line"># HTTPCACHE_EXPIRATION_SECS = 0  # 指定缓存响应的过期时间（秒），过期后将发送新请求到服务器</span><br><span class="line"># HTTPCACHE_DIR = &quot;httpcache&quot;  # 定义缓存响应的存储目录</span><br><span class="line"># HTTPCACHE_IGNORE_HTTP_CODES = []  # 定义在缓存时应该忽略的HTTP状态码列表</span><br><span class="line"># HTTPCACHE_STORAGE = &quot;scrapy.extensions.httpcache.FilesystemCacheStorage&quot;  # 存储缓存的引擎</span><br><span class="line"></span><br><span class="line"># Set settings whose default value is deprecated to a future-proof value</span><br><span class="line"># 这些设置的作用是确保在将来的Scrapy版本中仍然能够正常运行，因为某些默认值已被弃用</span><br><span class="line">REQUEST_FINGERPRINTER_IMPLEMENTATION = &quot;2.7&quot;</span><br><span class="line">TWISTED_REACTOR = &quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;</span><br><span class="line">FEED_EXPORT_ENCODING = &quot;utf-8&quot;</span><br><span class="line"></span><br><span class="line"># 处理图像下载的设置字段</span><br><span class="line">IMAGES_URLS_FIELD = &#x27;front_image_url&#x27;  # 用于指定 Item 对象中包含图像 URL 的字段名称</span><br><span class="line"># 如果路径以斜杠/开头，就是绝对路径；没有以斜杠开头，就是相对于当前工作目录的相对路径</span><br><span class="line">IMAGES_STORE = os.path.dirname(os.getcwd()) + &#x27;/images/&#x27;</span><br><span class="line"></span><br><span class="line"># MySQL相关配置</span><br><span class="line">MYSQL_HOST = &quot;127.0.0.1&quot;</span><br><span class="line">MYSQL_USER = &quot;root&quot;</span><br><span class="line">MYSQL_PASSWORD = &quot;123456&quot;</span><br><span class="line">MYSQL_DBNAME = &quot;py_spider&quot;</span><br></pre></td></tr></table></figure>

<h2 id="七、cnblogs项目实战"><a href="#七、cnblogs项目实战" class="headerlink" title="七、cnblogs项目实战"></a>七、cnblogs项目实战</h2><h3 id="1-spiders-cnblogs-py"><a href="#1-spiders-cnblogs-py" class="headerlink" title="1. spiders&#x2F;cnblogs.py"></a>1. spiders&#x2F;cnblogs.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import cmdline</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from selenium.webdriver.support.ui import WebDriverWait</span><br><span class="line">from selenium.webdriver.support import expected_conditions as EC</span><br><span class="line">from selenium.webdriver.common.by import By</span><br><span class="line">import time</span><br><span class="line">import re</span><br><span class="line">import hashlib</span><br><span class="line">from urllib import parse</span><br><span class="line"></span><br><span class="line">from scrapy.loader import ItemLoader</span><br><span class="line">from bkyuan.items import DefaultItemLoader</span><br><span class="line">from bkyuan.items import CnblogsArticleItem</span><br><span class="line"></span><br><span class="line">class CnblogsSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;cnblogs&quot;</span><br><span class="line">    allowed_domains = [&quot;news.cnblogs.com&quot;, &quot;cnblogs.com&quot;]</span><br><span class="line">    start_urls = [&quot;https://news.cnblogs.com/&quot;]</span><br><span class="line"></span><br><span class="line">    options = webdriver.ChromeOptions()</span><br><span class="line">    # 禁用Chrome的Blink渲染引擎中相关的自动化特性，目的是减少被自动化工具检测到的概率</span><br><span class="line">    options.add_argument(&#x27;--disable-blink-features=AutomationControlled&#x27;)</span><br><span class="line">    options.add_argument(&#x27;--headless&#x27;)  # 无头模式</span><br><span class="line">    options.page_load_strategy = &quot;eager&quot;  # 更改页面加载策略为eager</span><br><span class="line"></span><br><span class="line">    def __init__(self, *args, **kwargs):</span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line">        self.browser = webdriver.Chrome(options=self.options)</span><br><span class="line">        self.wait_bro = WebDriverWait(self.browser, 5)</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        self.browser.quit()</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        self.browser.get(&quot;https://account.cnblogs.com/signin&quot;)</span><br><span class="line">        username = self.wait_bro.until(</span><br><span class="line">            EC.presence_of_element_located((By.XPATH, &#x27;//input[@formcontrolname=&quot;username&quot;]&#x27;)))</span><br><span class="line">        password = self.wait_bro.until(</span><br><span class="line">            EC.presence_of_element_located((By.XPATH, &#x27;//input[@formcontrolname=&quot;password&quot;]&#x27;)))</span><br><span class="line">        login_btn = self.wait_bro.until(</span><br><span class="line">            EC.element_to_be_clickable((By.XPATH, &#x27;//button[contains(@class,&quot;mat-flat-button&quot;)]&#x27;)))</span><br><span class="line">        username.send_keys(&quot;小傅xfblog&quot;)</span><br><span class="line">        password.send_keys(&quot;zhangjie789&quot;)</span><br><span class="line">        login_btn.click()</span><br><span class="line">        check_btn = self.wait_bro.until(EC.element_to_be_clickable((By.XPATH, &#x27;//div[@id=&quot;rectMask&quot;]&#x27;)))</span><br><span class="line">        check_btn.click()</span><br><span class="line">        # 登录成功后，首先跳转新闻页，然后随机访问一篇文章，获取其cookies以供后续爬取文章详情页携带</span><br><span class="line">        time.sleep(2)  # 登录成功后不能点击太快，否则会又跳转至登录页面</span><br><span class="line">        self.browser.get(&quot;https://news.cnblogs.com/&quot;)</span><br><span class="line">        article_a = self.wait_bro.until(</span><br><span class="line">            EC.element_to_be_clickable((By.XPATH, &#x27;//div[@class=&quot;news_block&quot;][1]//h2/a&#x27;)))</span><br><span class="line">        article_a.click()</span><br><span class="line">        cookies = self.browser.get_cookies()</span><br><span class="line">        cookie_dict = &#123;item[&#x27;name&#x27;]: item[&#x27;value&#x27;] for item in cookies&#125;</span><br><span class="line"></span><br><span class="line">        url = &quot;https://news.cnblogs.com/n/page/&#123;&#125;/&quot;</span><br><span class="line">        for page in range(1, 101):</span><br><span class="line">            yield scrapy.Request(url=url.format(page), cookies=cookie_dict, meta=&#123;&#x27;cookies&#x27;: cookie_dict&#125;)</span><br><span class="line"></span><br><span class="line">    def parse(self, response: HtmlResponse, **kwargs):</span><br><span class="line">        &quot;&quot;&quot;parse函数主要完成两个功能：</span><br><span class="line">        1. 获取这页中的每条新闻的封面图片，存为元数据</span><br><span class="line">        2. 获取新闻详情页的url，并交给scrapy进行下载后调用相应的解析方法</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        news_list = response.xpath(&quot;//div[@id=&#x27;news_list&#x27;]/div[@class=&#x27;news_block&#x27;]&quot;)</span><br><span class="line">        for news in news_list:</span><br><span class="line">            image_url = news.xpath(&quot;.//img/@src&quot;).extract_first()</span><br><span class="line">            # 这里有些文章没有封面图片。有些以//开头，所以需要处理为完整url</span><br><span class="line">            if image_url and image_url.startswith(&quot;//&quot;):</span><br><span class="line">                image_url = &quot;https:&quot; + image_url</span><br><span class="line">            article_url = news.xpath(&quot;.//h2[@class=&#x27;news_entry&#x27;]/a/@href&quot;).extract_first()</span><br><span class="line"></span><br><span class="line">            # 使用response.urljoin()方法将相对url转换为绝对url</span><br><span class="line">            article_url = response.urljoin(article_url)</span><br><span class="line">            yield scrapy.Request(url=article_url, cookies=response.meta.get(&quot;cookies&quot;, &quot;&quot;),</span><br><span class="line">                                 meta=&#123;&#x27;front_image_url&#x27;: image_url&#125;, callback=self.parse_article)</span><br><span class="line"></span><br><span class="line">    def parse_article(self, response: HtmlResponse):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        分析得知：后续文章详情的评论数、阅读量等数据，是通过数据api返回的，所以需要得到文章id，</span><br><span class="line">        构造请求url：https://news.cnblogs.com/NewsAjax/GetAjaxNewsInfo?contentId=&#123;&#125;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        article_id = re.findall(r&#x27;.*?(\d&#123;6,&#125;?)&#x27;, response.url)[0]</span><br><span class="line">        url_md5 = hashlib.md5(str(response.url).encode()).hexdigest()</span><br><span class="line">        # 日志打印请求元数据</span><br><span class="line">        self.logger.warning(response.request.meta)</span><br><span class="line">        if article_id:</span><br><span class="line">            # 创建一个 ItemLoader 实例，并指定要加载的 Item 类</span><br><span class="line">            item_info = DefaultItemLoader(item=CnblogsArticleItem(), response=response)</span><br><span class="line">            item_info.add_xpath(&quot;title&quot;, &#x27;//*[@id=&quot;news_title&quot;]/a/text()&#x27;)</span><br><span class="line">            item_info.add_xpath(&quot;create_date&quot;, &#x27;//*[@id=&quot;news_info&quot;]//*[@class=&quot;time&quot;]/text()&#x27;)</span><br><span class="line">            item_info.add_xpath(&quot;tags&quot;, &#x27;//*[@id=&quot;news_more_info&quot;]//a/text()&#x27;)</span><br><span class="line">            item_info.add_value(&quot;url&quot;, response.url)</span><br><span class="line">            item_info.add_value(&quot;url_md5_id&quot;, url_md5)</span><br><span class="line">            # 下载图片需要在settings中配置 IMAGES_URLS_FIELD 和 IMAGES_STORE 字段</span><br><span class="line">            # 一定要确保图片 url 值是有效的，否则会抛出异常 ValueError：Missing scheme in request url</span><br><span class="line">            if response.meta.get(&quot;front_image_url&quot;, &quot;&quot;):</span><br><span class="line">                item_info.add_value(&quot;front_image_url&quot;, response.meta.get(&quot;front_image_url&quot;, &quot;&quot;))</span><br><span class="line"></span><br><span class="line">            # 由于是带着cookies在进行详情页请求，此时response.url已经变了，所以不再使用response.urljoin()</span><br><span class="line">            # 可以使用 urllib.parse 包中的 urljoin() 方法，手动控制 url 前缀</span><br><span class="line">            api_url = parse.urljoin(&quot;https://news.cnblogs.com/&quot;, f&quot;/NewsAjax/GetAjaxNewsInfo?contentId=&#123;article_id&#125;&quot;)</span><br><span class="line">            yield scrapy.Request(url=api_url, meta=&#123;&#x27;item_info&#x27;: item_info&#125;, callback=self.parse_api)</span><br><span class="line"></span><br><span class="line">    def parse_api(self, response: HtmlResponse):</span><br><span class="line">        data = response.json()</span><br><span class="line">        item_info = response.meta.get(&quot;item_info&quot;, &quot;&quot;)</span><br><span class="line">        # 注意 item 中和这里定义字段的大小写，区分大小写！</span><br><span class="line">        item_info.add_value(&quot;commentcount&quot;, data.get(&quot;CommentCount&quot;, 0))  # 评论数</span><br><span class="line">        item_info.add_value(&quot;totalview&quot;, data.get(&quot;TotalView&quot;, 0))  # 阅读量</span><br><span class="line">        item_info.add_value(&quot;diggcount&quot;, data.get(&quot;DiggCount&quot;, 0))  # 推荐数量</span><br><span class="line">        item_info.add_value(&quot;burycount&quot;, data.get(&quot;BuryCount&quot;, 0))  # 不推荐数量</span><br><span class="line"></span><br><span class="line">        # 完成加载并生成 Item 对象</span><br><span class="line">        yield item_info.load_item()</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    cmdline.execute(&quot;scrapy crawl cnblogs&quot;.split())</span><br></pre></td></tr></table></figure>

<h3 id="2-extend-ip-py"><a href="#2-extend-ip-py" class="headerlink" title="2. extend_ip.py"></a>2. extend_ip.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import threading</span><br><span class="line">import requests</span><br><span class="line">from scrapy import signals</span><br><span class="line"></span><br><span class="line"># 提取代理IP的api，一次提取10个（生成链接：https://www.kuaidaili.com/dps/genapiurl/）</span><br><span class="line">api_url = &#x27;https://dps.kdlapi.com/api/getdps/?secret_id=o88cwu0sh49ls75kntwt&amp;num=1&amp;signature=dtb4yamicu504fub6llgx7mjabckit83&amp;pt=1&amp;format=json&amp;sep=1&#x27;</span><br><span class="line">foo = True</span><br><span class="line"></span><br><span class="line">class Proxy:</span><br><span class="line">    def __init__(self, ):</span><br><span class="line">        self._proxy_list = requests.get(api_url).json().get(&#x27;data&#x27;).get(&#x27;proxy_list&#x27;)</span><br><span class="line"></span><br><span class="line">    @property  # p.proxy_list：自动执行此方法，并获取方法的返回值</span><br><span class="line">    def proxy_list(self):</span><br><span class="line">        return self._proxy_list</span><br><span class="line"></span><br><span class="line">    @proxy_list.setter  # p.proxy_list=&#x27;abc&#x27;：自动执行此方法，并将&#x27;abc&#x27;赋值给list参数</span><br><span class="line">    def proxy_list(self, list):</span><br><span class="line">        self._proxy_list = list</span><br><span class="line"></span><br><span class="line">pro = Proxy()</span><br><span class="line"></span><br><span class="line">class MyExtend:</span><br><span class="line">    def __init__(self, crawler):</span><br><span class="line">        self.crawler = crawler</span><br><span class="line">        # 将自定义方法绑定到scrapy信号上，使程序与spider引擎同步启动与关闭</span><br><span class="line">        crawler.signals.connect(self.start, signals.engine_started)</span><br><span class="line">        crawler.signals.connect(self.close, signals.spider_closed)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(crawler)</span><br><span class="line"></span><br><span class="line">    def start(self):</span><br><span class="line">        t = threading.Thread(target=self.extract_proxy)</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">    def extract_proxy(self):</span><br><span class="line">        while foo:</span><br><span class="line">            pro.proxy_list = requests.get(api_url).json().get(&#x27;data&#x27;).get(&#x27;proxy_list&#x27;)</span><br><span class="line">            # 设置每一分钟提取一次ip，更新proxy_list列表中的ip，保证都是有效ip</span><br><span class="line">            time.sleep(60)</span><br><span class="line"></span><br><span class="line">    def close(self):</span><br><span class="line">        global foo</span><br><span class="line">        foo = False</span><br></pre></td></tr></table></figure>

<h3 id="3-middlewares-py"><a href="#3-middlewares-py" class="headerlink" title="3. middlewares.py"></a>3. middlewares.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from scrapy import signals</span><br><span class="line">import random</span><br><span class="line">from .extend_ip import pro</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line"></span><br><span class="line">class IpDownloaderMiddleware:</span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        username = &quot;d4550779926&quot;</span><br><span class="line">        password = &quot;kzsel6pa&quot;</span><br><span class="line">        # 如果当前ip不可用，那么process_response重试，这里会在代理池中重新选择一个代理ip</span><br><span class="line">        proxy = random.choice(pro.proxy_list)</span><br><span class="line">        request.meta[&#x27;proxy&#x27;] = f&quot;http://&#123;username&#125;:&#123;password&#125;@&#123;proxy&#125;/&quot;</span><br><span class="line">        return None</span><br><span class="line"></span><br><span class="line">    def process_response(self, request, response: HtmlResponse, spider):</span><br><span class="line">        if not response.status == 200:</span><br><span class="line">            request.dont_filter = True</span><br><span class="line">            return request</span><br><span class="line">        return response</span><br></pre></td></tr></table></figure>

<h3 id="4-items-py"><a href="#4-items-py" class="headerlink" title="4. items.py"></a>4. items.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from itemloaders.processors import TakeFirst, Identity, Join, Compose, MapCompose</span><br><span class="line">from scrapy.loader import ItemLoader</span><br><span class="line"></span><br><span class="line">def date_convert(date):</span><br><span class="line">    # 这里处理字段一定不能只处理不对的，要将正确的也return回去</span><br><span class="line">    return &quot;1970-07-01&quot; if not date else date</span><br><span class="line">    </span><br><span class="line">class DefaultItemLoader(ItemLoader):</span><br><span class="line">    # 配置默认的输出字段格式，因为在获取数据时，add_xpath方法获取的都是列表</span><br><span class="line">    default_output_processor = TakeFirst()  # 取出列表第一个非空值</span><br><span class="line"></span><br><span class="line">class CnblogsArticleItem(scrapy.Item):</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        # 常配合spider中的add_value进行处理器调用，用于在将数据填充到ItemLoader之前对其进行处理</span><br><span class="line">        input_processor=Compose(date_convert)  # 如果获取的日期为空，则赋值&quot;1970-07-01&quot;</span><br><span class="line">    )</span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        # 由于标签tags不止一个，而TakeFirst()只取第一个，所以可以使用Join将多个拼接为字符串</span><br><span class="line">        output_processor=Join(&quot;,&quot;)</span><br><span class="line">    )</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_md5_id = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    front_image_url = scrapy.Field(</span><br><span class="line">        # 图片下载字段的值必须是一个列表，列表中包含正确的图片url地址</span><br><span class="line">        # DefaultItemLoader将图片字段输出为了字符串，所以这里需要输出原始数据，即列表</span><br><span class="line">        output_processor=Identity()</span><br><span class="line">    )</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    commentcount = scrapy.Field()</span><br><span class="line">    totalview = scrapy.Field()</span><br><span class="line">    diggcount = scrapy.Field()</span><br><span class="line">    burycount = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h3 id="5-piplines-py"><a href="#5-piplines-py" class="headerlink" title="5. piplines.py"></a>5. piplines.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">import json</span><br><span class="line">from scrapy.exporters import JsonLinesItemExporter</span><br><span class="line">from scrapy.pipelines.images import ImagesPipeline</span><br><span class="line">from twisted.enterprise import adbapi</span><br><span class="line">import pymysql.cursors</span><br><span class="line"></span><br><span class="line"># 自定义json数据的导出模式，所有的item都会在内存中被处理并最终写入文件，可能导致内存消耗较高</span><br><span class="line">class JsonExporterPipeline:</span><br><span class="line">    &quot;&quot;&quot;关于初始化文件对象的问题：</span><br><span class="line">    1、如果想要在爬虫启动时创建文件，在关闭时关闭文件，使用 open_spider 和 close_spider 是很好的选择</span><br><span class="line">    2、如果想要在整个爬虫运行期间保持文件打开状态，那么使用 __init__ 和 spider_closed 的写法也是可以的</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if spider.name == &quot;cnblogs&quot;:</span><br><span class="line">            # 为了确保使整个文件成为一个 JSON 数组，方便后续调用，只需加上换行即可自动生成</span><br><span class="line">            self.fp.write(json.dumps(dict(item), ensure_ascii=False) + &quot;,\n&quot;)</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        if spider.name == &quot;cnblogs&quot;:</span><br><span class="line">            # 使用默认的 JSON 编码器无法处理自定义对象，无法将 Item 对象转换为字典</span><br><span class="line">            self.fp = open(&quot;article1.json&quot;, &quot;w&quot;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line">            self.fp.write(&quot;[\n&quot;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        if spider.name == &quot;cnblogs&quot;:</span><br><span class="line">            self.fp.write(&quot;]\n&quot;)</span><br><span class="line">            self.fp.close()</span><br><span class="line"></span><br><span class="line"># Scrapy官方的json数据导出模式，每次parse函数yield的item经过处理直接写入json文件，不占内存</span><br><span class="line">class ScrapyJsonExporterPipeline:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 以二进制模式打开文件时不应该指定编码参数，因为文件是以字节形式读取和写入的，而不是字符</span><br><span class="line">        self.fp = open(&quot;article2.json&quot;, &quot;wb&quot;)</span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp, encoding=&quot;utf-8&quot;, ensure_ascii=False)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line">        self.fp.write(b&quot;[&quot;)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 注意：export_item方法中只接受字节数据，所以上面文件应该以二进制模式wb打开</span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        self.fp.write(b&quot;,&quot;)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.fp.write(b&quot;]&quot;)</span><br><span class="line">        self.fp.close()</span><br><span class="line"></span><br><span class="line"># 自定义下载图片管道，遍历下载结果，获取图片保存的路径并添加到item中，最后返回处理后的item</span><br><span class="line">class MyImagesPipeline(ImagesPipeline):</span><br><span class="line">    def item_completed(self, results, item, info):</span><br><span class="line">        if &#x27;front_image_url&#x27; in item:</span><br><span class="line">            for ok, value in results:</span><br><span class="line">                front_image_path = value[&#x27;path&#x27;]</span><br><span class="line">                item[&#x27;front_image_path&#x27;] = front_image_path</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line"># 自定义异步mysql存储管道</span><br><span class="line">class MysqlTwistedPipeline:</span><br><span class="line">    def __init__(self, dbpool):</span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_settings(cls, settings):</span><br><span class="line">        dbparms = dict(</span><br><span class="line">            host=settings[&quot;MYSQL_HOST&quot;],</span><br><span class="line">            db=settings[&quot;MYSQL_DBNAME&quot;],</span><br><span class="line">            user=settings[&quot;MYSQL_USER&quot;],</span><br><span class="line">            passwd=settings[&quot;MYSQL_PASSWORD&quot;],</span><br><span class="line">            charset=&quot;utf8&quot;,  # 字符集必须是utf8更规范，utf-8会报错</span><br><span class="line">            cursorclass=pymysql.cursors.DictCursor,  # 指定了在执行查询时返回的结果集的游标类型</span><br><span class="line">            use_unicode=True,  # 使用Unicode编码，更好地支持包含非ASCII字符的数据</span><br><span class="line">        )</span><br><span class="line">        dbpool = adbapi.ConnectionPool(&#x27;pymysql&#x27;, **dbparms)</span><br><span class="line"></span><br><span class="line">        # 在创建连接池的时候异步执行创建表的逻辑</span><br><span class="line">        cls.create_table(dbpool)</span><br><span class="line"></span><br><span class="line">        return cls(dbpool)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def create_table(cls, dbpool):</span><br><span class="line">        sql = &quot;&quot;&quot;</span><br><span class="line">            create table if not exists cnblogs_news(</span><br><span class="line">                url_md5_id char(32) primary key,</span><br><span class="line">                title varchar(255),</span><br><span class="line">                tags varchar(50),</span><br><span class="line">                create_date timestamp,</span><br><span class="line">                url varchar(255),</span><br><span class="line">                front_image_url varchar(255),</span><br><span class="line">                front_image_path varchar(255),</span><br><span class="line">                commentcount int,</span><br><span class="line">                totalview int,</span><br><span class="line">                diggcount int,</span><br><span class="line">                burycount int</span><br><span class="line">            );</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 异步方式执行创建表的sql语句</span><br><span class="line">        dbpool.runInteraction(cls._create_table, sql)</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def _create_table(cursor, sql):</span><br><span class="line">        cursor.execute(sql)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # 异步调用do_insert函数，执行sql语句，并传入item参数</span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        # 在runInteraction中执行的操作发生异常时，回调handle_error函数处理异常</span><br><span class="line">        query.addErrback(self.handle_error)</span><br><span class="line"></span><br><span class="line">    def do_insert(self, cursor, item):</span><br><span class="line">        &quot;&quot;&quot;注意：在构造sql语句时，最好使用参数化占位符来解决这个问题，</span><br><span class="line">        而不是直接在sql查询中插入值，这种方法可以避免sql注入的问题，并更容易处理特殊字符</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # `ON DUPLICATE KEY UPDATE`语法在插入数据时如果主键冲突，则指定一个字段进行更新的操作</span><br><span class="line">        sql = f&quot;&quot;&quot;</span><br><span class="line">            insert into cnblogs_news values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)</span><br><span class="line">            ON DUPLICATE KEY UPDATE totalview=VALUES(totalview);</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        params = [</span><br><span class="line">            item.get(&quot;url_md5_id&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;title&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;tags&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;create_date&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;url&quot;, &quot;&quot;),</span><br><span class="line">            # 保证进入数据库的每个值都是str或者int类型的</span><br><span class="line">            &quot;,&quot;.join(item.get(&quot;front_image_url&quot;, [])),</span><br><span class="line">            item.get(&quot;front_image_path&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;commentcount&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;totalview&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;diggcount&quot;, &quot;&quot;),</span><br><span class="line">            item.get(&quot;burycount&quot;, &quot;&quot;)</span><br><span class="line">        ]</span><br><span class="line">        cursor.execute(sql, params)  # 参数2接受列表或元组</span><br><span class="line"></span><br><span class="line">    def handle_error(self, failure):</span><br><span class="line">        print(f&quot;mysql处理错误：&#123;failure&#125;&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="6-settings-py"><a href="#6-settings-py" class="headerlink" title="6. settings.py"></a>6. settings.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">BOT_NAME = &quot;bkyuan&quot;</span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [&quot;bkyuan.spiders&quot;]</span><br><span class="line">NEWSPIDER_MODULE = &quot;bkyuan.spiders&quot;</span><br><span class="line"></span><br><span class="line"># Obey robots.txt rules</span><br><span class="line">ROBOTSTXT_OBEY = False</span><br><span class="line"></span><br><span class="line"># Override the default request headers:</span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36&#x27;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Enable or disable downloader middlewares</span><br><span class="line"># See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &quot;bkyuan.middlewares.IpDownloaderMiddleware&quot;: 543,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Enable or disable extensions</span><br><span class="line"># See https://docs.scrapy.org/en/latest/topics/extensions.html</span><br><span class="line">EXTENSIONS = &#123;</span><br><span class="line">    &quot;bkyuan.extend_ip.MyExtend&quot;: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Configure item pipelines</span><br><span class="line"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &quot;bkyuan.pipelines.JsonExporterPipeline&quot;: 300,</span><br><span class="line">    &quot;bkyuan.pipelines.ScrapyJsonExporterPipeline&quot;: 301,</span><br><span class="line">    # &quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1,  # 使用scrapy默认的下载图片pipeline，优先级为1</span><br><span class="line">    &quot;bkyuan.pipelines.MyImagesPipeline&quot;: 1,</span><br><span class="line">    &quot;bkyuan.pipelines.MysqlTwistedPipeline&quot;: 302,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Set settings whose default value is deprecated to a future-proof value</span><br><span class="line">REQUEST_FINGERPRINTER_IMPLEMENTATION = &quot;2.7&quot;</span><br><span class="line">TWISTED_REACTOR = &quot;twisted.internet.asyncioreactor.AsyncioSelectorReactor&quot;</span><br><span class="line">FEED_EXPORT_ENCODING = &quot;utf-8&quot;</span><br><span class="line"></span><br><span class="line"># 处理图像下载的设置字段</span><br><span class="line">IMAGES_URLS_FIELD = &#x27;front_image_url&#x27;  # 用于指定 Item 对象中包含图像 URL 的字段名称</span><br><span class="line"># 如果路径以斜杠/开头，就是绝对路径；没有以斜杠开头，就是相对于当前工作目录的相对路径</span><br><span class="line">IMAGES_STORE = os.path.dirname(os.getcwd()) + &#x27;/images/&#x27;</span><br><span class="line"></span><br><span class="line"># MySQL相关配置</span><br><span class="line">MYSQL_HOST = &quot;127.0.0.1&quot;</span><br><span class="line">MYSQL_USER = &quot;root&quot;</span><br><span class="line">MYSQL_PASSWORD = &quot;123456&quot;</span><br><span class="line">MYSQL_DBNAME = &quot;py_spider&quot;</span><br></pre></td></tr></table></figure>

<h2 id="八、Scrapy-Redis增量爬虫"><a href="#八、Scrapy-Redis增量爬虫" class="headerlink" title="八、Scrapy_Redis增量爬虫"></a>八、Scrapy_Redis增量爬虫</h2><blockquote>
<p><strong>引入：</strong>前面所提到的<code>scrapy的断点续爬</code>和<code>请求url地址去重</code>就是在原生实现增量爬虫，虽然有效，但却无法从<code>JOBDIR</code>指定的路径中看到爬取的情况，而 <strong>scrapy_redis</strong> 则是将两者融合并完美实现，不仅可以以同样的方式 <strong>ctrl+c</strong> 实现暂停，以启动命令实现断点续爬，还可以在redis数据库中通过 <strong>dupfilter</strong> 和 <strong>requests</strong> 字段得知爬取情况，其中<strong>dupfilter对应已爬取的指纹</strong>，而<strong>requests对应还没爬取的</strong>，以实现增量爬虫</p>
<p><strong>什么是增量爬虫？</strong></p>
<p>在Scrapy中，增量爬虫（Incremental Crawling）是指只爬取自上次爬取以来发生变化的数据，而不是重新爬取整个网站或数据集。这样可以节省时间和资源，特别是在处理大规模的网站或数据集时</p>
<p><strong>scrapy_redis 扩展：</strong>scrapy_redis允许在分布式环境中使用Scrapy，并且可以单独利用此插件实现增量爬虫，通过在Redis中存储爬取状态来避免重复爬取相同的数据</p>
<p><strong>注意：使用scrapy-redis之前最好将scrapy版本保持在2.8.0版本，以免有兼容性问题</strong></p>
<ul>
<li><strong><code>pip install scrapy==2.8.0</code></strong></li>
<li><strong><code>pip install scrapy-redis</code></strong></li>
</ul>
</blockquote>
<ul>
<li><strong>增量爬虫 settings.py 文件新增配置：</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot; scrapy-redis配置 &quot;&quot;&quot;</span><br><span class="line"># （使用 scrapy_redis 调度器）指定用于管理请求调度的类</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># （使用 scrapy_redis 去重器）指定用于管理重复请求的类</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line">&quot;&quot;&quot;可以替换成布隆过滤器：</span><br><span class="line">pip install scrapy-redis-bloomfilter</span><br><span class="line">from scrapy_redis_bloomfilter.dupefilter import RFPDupeFilter</span><br><span class="line">DUPEFILTER_CLASS = &#x27;scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter&#x27;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># （调度器持久化）是否在关闭时候保留原来的调度器和去重记录，True=保留，False=清空</span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"># （设置 redis 连接信息）这里表示 Redis 运行在本地机器上，使用默认端口 6379 和数据库0</span><br><span class="line">REDIS_URL = &quot;redis://127.0.0.1:6379/0&quot;</span><br><span class="line"></span><br><span class="line"># （调度器队列类）指定用于管理优先级队列的类</span><br><span class="line">SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.PriorityQueue&#x27;  # （优先级队列）</span><br><span class="line"># SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.FifoQueue&#x27;  # （先进先出队列）</span><br><span class="line"># SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.LifoQueue&#x27;  # （先进后出/后进先出队列）</span><br><span class="line"></span><br><span class="line"># （启动时清空调度器）默认为False，True表示调度器将在启动时清空所有待处理的请求和去重过滤器，一般不配置</span><br><span class="line"># SCHEDULER_FLUSH_ON_START = True</span><br></pre></td></tr></table></figure>

<h2 id="九、Scrapy-Redis分布式爬虫"><a href="#九、Scrapy-Redis分布式爬虫" class="headerlink" title="九、Scrapy_Redis分布式爬虫"></a>九、Scrapy_Redis分布式爬虫</h2><blockquote>
<p>**注意：学习使用 ubuntu-18.04.6 版本虚拟机模拟实现分布式爬虫（<a class="link"   href="https://xfblog.cn/2023/11/01/%E7%88%AC%E8%99%AB_3_Ubuntu%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"  target="_blank" rel="noopener">ubuntu-18.04.6安装配置<i class="fas fa-external-link-alt"></i></a>**）</p>
<p><strong>引入：为什么要使用 scrapy_redis 做分布式爬虫？</strong></p>
<ul>
<li>在单机爬虫中，所有请求对象 requests 都是直接存放在内存中的，除主机外的其他电脑是不可能访问到主机的内存的，所以借助 scrapy_redis 将原本存储在内存中的数据放到了 Redis 队列中，利用 Redis 高效和支持网络访问的特性，便可以实现多台电脑读取同一个 Redis 队列中的数据，从而实现分布式爬虫</li>
</ul>
</blockquote>
<h3 id="1-scrapy-redis工作流程"><a href="#1-scrapy-redis工作流程" class="headerlink" title="1. scrapy_redis工作流程"></a>1. scrapy_redis工作流程</h3><ul>
<li><strong>scrapy_redis的实现原理：</strong><ul>
<li><strong>替换调度器和去重器：</strong>scrapy_redis 替换了 Scrapy 框架中的默认调度器和去重器，将 Redis 作为中心化的任务调度和去重的数据存储</li>
<li><strong>任务队列的使用：</strong>scrapy_redis 使用 Redis 数据库作为任务队列，将爬虫需要爬取的 URL 存储在 Redis 的列表或者集合中。每个爬虫节点都可以从这个 Redis 队列中获取待爬取的 URL，实现了任务的分发</li>
<li><strong>分布式去重：</strong>scrapy_redis 通过使用 Redis 数据结构的去重特性，即 <code>RFPDupeFilter</code>，来实现分布式环境下的 URL 去重。当一个 URL 被加入到 Redis 中时，系统会检查是否已经存在于 Redis 中，以实现去重</li>
<li><strong>任务调度和分配：</strong>scrapy_redis 的调度器负责从 Redis 队列中获取待爬取的 URL，并将其分配给不同的爬虫节点。这样，不同的节点可以并行地爬取不同的 URL，提高了整个系统的效率</li>
</ul>
</li>
<li><strong>scrapy_redis的工作流程：</strong><ol>
<li>在scrapy_redis中，所有待抓取的 request 对象和去重的 request 对象指纹都存在共用的 redis 队列中</li>
<li>所有的服务器中的 scrapy 进程共用同一个 redis 中的 request 对象的队列</li>
<li>所有的 request 对象存入 redis 前，都会通过 redis 中的 request 指纹集合进行判断是否存入过（去重）</li>
</ol>
</li>
</ul>
<img    
                       lazyload
                       alt="image"
                       data-src="https://xfblog.cn/images/1703165509-14b9d3416aa3a54.png"
                         style="width:80%;margin:0 auto;"
                 >

<h3 id="2-RedisPipeline的使用"><a href="#2-RedisPipeline的使用" class="headerlink" title="2. RedisPipeline的使用"></a>2. RedisPipeline的使用</h3><blockquote>
<p><strong>开启RedisPipeline管道：<code>&quot;scrapy_redis.pipelines.RedisPipeline&quot;: 301</code>（将所有数据存入redis中）</strong></p>
</blockquote>
<ul>
<li><p><strong>关于数据存储问题：</strong>分布式爬虫中，每一台电脑上只存储这台电脑所爬取的数据，与其他设备所爬取的数据互不相通也不重复，所以解决数据统一存储便成为问题。<strong>开启RedisPipeline管道</strong>，那么则可以实现所有数据都存储在Redis数据库中，但前提是所有设备都开启，否则设备之间数据也互不相通</p>
</li>
<li><p><strong>关于Redis中数据的问题：</strong>在RedisPipeline管道存储数据时，不支持存储二进制数据，需要将二进制数据进行 Base64 编码，并将结果解码为字符串后再存储（<strong>注意：由于在传递item时将图片数据进行了 Base64 编码，那么在 pipline 其他管道中保存数据时，就一定要将数据进行 Base64 解码，才能正确保存</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># spider.py：将二进制数据进行 Base64 编码，并将结果解码为字符串，才能正确传递</span><br><span class="line">img_data = base64.b64encode(response.body).decode()</span><br><span class="line">yield &#123;</span><br><span class="line">    &quot;type_&quot;: &#x27;image&#x27;,</span><br><span class="line">    &quot;img_name&quot;: title + &#x27;.png&#x27;,</span><br><span class="line">    &quot;img_content&quot;: img_data</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># piplines.py：将数据进行 Base64 解码，才能正确保存</span><br><span class="line">with open(images_path + item[&#x27;img_name&#x27;], &#x27;wb&#x27;) as fp:</span><br><span class="line">    img_data = base64.b64decode(item[&#x27;img_content&#x27;])</span><br><span class="line">    fp.write(img_data)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-实现GET请求"><a href="#3-实现GET请求" class="headerlink" title="3. 实现GET请求"></a>3. 实现GET请求</h3><blockquote>
<p><strong>scrapy_redis 的使用注意：</strong></p>
<ol>
<li><strong>不能使用 start_urls 属性：</strong>RedisSpider 类是为分布式爬虫设计的，它使用 Redis 作为任务队列，所以如果爬虫类继承了 RedisSpider 类，便不能使用此属性了</li>
<li><strong>不能重写 start_requests 方法：</strong>RedisSpider 类与 scrapy.Spider 类中的 start_requests 方法底层实现不一样，所以如果爬虫类继承了 RedisSpider 类，便不能重写此方法了</li>
</ol>
</blockquote>
<ul>
<li><p><strong>scrapy_redis项目配置：</strong></p>
<ol>
<li><p><strong>将爬虫类继承自 RedisSpider 类：</strong>并指定 <strong><code>redis_key</code></strong> 属性，即从 Redis 队列中获取起始 URL 的键名</p>
</li>
<li><p><strong>添加 scrapy_redis 配置（settings文件）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot; scrapy-redis配置 &quot;&quot;&quot;</span><br><span class="line"># 配置scrapy_redis调度器</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># 配置scrapy_redis去重器</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># 调度器持久化，True=保留，False=清空</span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"># 配置redis连接信息</span><br><span class="line">REDIS_URL = &quot;redis://127.0.0.1:6379/0&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>执行保存 start_url 的脚本：</strong>由于 scrapy_redis 不再通过 start_urls 属性读取起始url，而是通过 <strong><code>redis_key</code></strong> 属性从 Redis 队列中获取起始 URL 的键名，所以需要先将起始 start_url 保存到 Redis 中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import redis</span><br><span class="line"></span><br><span class="line">res = redis.Redis(host=&quot;192.168.64.12&quot;, port=6379)</span><br><span class="line">res.lpush(&#x27;dd_book:start_url&#x27;, &#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;)</span><br><span class="line">res.close()</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h3 id="4-实现POST请求"><a href="#4-实现POST请求" class="headerlink" title="4. 实现POST请求"></a>4. 实现POST请求</h3><blockquote>
<p><strong>使用 scrapy_redis 实现 POST 请求携带表单数据：</strong></p>
<ul>
<li>进行 post 请求时，通常通过携带的表单数据进行翻页等操作，此时 url 固定，所以 Redis 中存储的应该是表单数据，只需携带不同表单数据便可发起不同 post 请求</li>
</ul>
</blockquote>
<ul>
<li><p><strong>scrapy_redis项目配置：</strong></p>
<ol>
<li><p><strong>将爬虫类继承自 RedisSpider 类：</strong>并指定 <strong><code>redis_key</code></strong> 属性，即从 Redis 队列中获取起始 URL 的键名</p>
</li>
<li><p><strong>添加 scrapy_redis 配置（settings文件）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot; scrapy-redis配置 &quot;&quot;&quot;</span><br><span class="line"># 配置scrapy_redis调度器</span><br><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line"># 配置scrapy_redis去重器</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line"># 调度器持久化，True=保留，False=清空</span><br><span class="line">SCHEDULER_PERSIST = True</span><br><span class="line"># 配置redis连接信息</span><br><span class="line">REDIS_URL = &quot;redis://127.0.0.1:6379/0&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>重写 make_request_from_data 方法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def make_request_from_data(self, data):</span><br><span class="line">    # 参数data：&#123;&#x27;url&#x27;: url, &#x27;from_data&#x27;: fromdata, &#x27;meta&#x27;: meta&#125;</span><br><span class="line">    # 即存储在redis中的起始form_data表单数据，一般通过手动执行脚本插入</span><br><span class="line">    data = json.loads(data)</span><br><span class="line">    form_data = data.get(&quot;from_data&quot;)</span><br><span class="line"></span><br><span class="line">    # 由于post请求只需通过改变表单数据控制翻页，所以url可以写死</span><br><span class="line">    url = &quot;http://www.cninfo.com.cn/new/disclosure&quot;</span><br><span class="line">    return FormRequest(url=url, formdata=form_data)  # 必须使用return返回请求</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>执行保存 form_data 的脚本：</strong>与get请求不同的是，get请求中直接通过请求不同的 url 地址控制翻页，而 post 请求中通过携带不同的 form_data 表单数据进行控制翻页，所以 post 请求中存储在 Redis 队列中的是 form_data 表单数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import redis</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">def push_start_url(db, request_obj):</span><br><span class="line">    # 参数request_obj：&#123;&#x27;url&#x27;: url, &#x27;from_data&#x27;: fromdata, &#x27;meta&#x27;: meta&#125;</span><br><span class="line">    db.lpush(&#x27;jc:start_urls&#x27;, json.dumps(request_obj))</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    redis_cli = redis.Redis()</span><br><span class="line">    for page in range(1, 21):</span><br><span class="line">        from_data = &#123;</span><br><span class="line">            &quot;column&quot;: &quot;szse_latest&quot;,</span><br><span class="line">            &quot;pageNum&quot;: str(page),</span><br><span class="line">            &quot;pageSize&quot;: &quot;30&quot;,</span><br><span class="line">            &quot;sortName&quot;: &quot;&quot;,</span><br><span class="line">            &quot;sortType&quot;: &quot;&quot;,</span><br><span class="line">            &quot;clusterFlag&quot;: &quot;true&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        push_start_url(redis_cli, &#123;&#x27;from_data&#x27;: from_data&#125;)</span><br><span class="line">    redis_cli.close()</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="十、scrapy项目部署"><a href="#十、scrapy项目部署" class="headerlink" title="十、scrapy项目部署"></a>十、scrapy项目部署</h2><h3 id="1-项目部署-scrapyd"><a href="#1-项目部署-scrapyd" class="headerlink" title="1. 项目部署_scrapyd"></a>1. 项目部署_scrapyd</h3><ul>
<li><p><strong>【服务器端】安装scrapyd：<code>pip install scrapyd</code></strong></p>
<ul>
<li><p><strong>在存储所有爬虫项目的文件夹根路径下创建并配置 scrapyd.conf 文件：<code>vim scrapyd.conf</code></strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[scrapyd]</span><br><span class="line"># 监听的IP地址，默认为127.0.0.1（修改成0.0.0.0才能在别的电脑上访问scrapyd运行之后的服务器）</span><br><span class="line">bind_address = 0.0.0.0</span><br><span class="line"># 监听的端口，默认为6800</span><br><span class="line">http_port = 6800</span><br><span class="line"># 是否打开debug模式，默认为off</span><br><span class="line">debug = off</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>在当前路径下开启 scrapy 服务：<code>scrapyd</code>（服务器地址更改为 <a class="link"   target="_blank" rel="noopener" href="http://0.0.0.0:6800/" >http://0.0.0.0:6800/<i class="fas fa-external-link-alt"></i></a> 即可）</strong></p>
</li>
</ul>
</li>
<li><p><strong>【客户端】安装scrapyd-client：<code>pip install scrapyd-client</code></strong></p>
<ul>
<li><p><strong>配置爬虫项目的 scrapy.cfg 文件：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = douban.settings</span><br><span class="line"></span><br><span class="line">[deploy:ubuntu-1]  # 标记要发布道哪个服务器</span><br><span class="line">url = http://192.168.64.12:6800/</span><br><span class="line">project = douban</span><br><span class="line"></span><br><span class="line">;如果有多台服务器可以编写多个deploy</span><br><span class="line">;[deploy:ubuntu-2]</span><br><span class="line">;url = http://192.168.64.13:6800/</span><br><span class="line">;project = douban</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>检查 scrapy.cfg 配置是否生效：<code>scrapyd-deploy -l</code>（小写L）</strong></p>
</li>
</ul>
</li>
<li><p><strong>将客户端 scrapy 项目发布到服务器端：<code>scrapyd-deploy &lt;target&gt; -p &lt;project&gt; --version &lt;version&gt;</code></strong></p>
<ul>
<li>target：配置文件 scrapy.cfg 里 deploy 后面的 target 名字，例如 <code>ubuntu-1</code></li>
<li>project：可以随意定义，跟爬虫的工程名字无关，一般与scrapy爬虫项目名相同</li>
<li>version：自定义版本号，不写的话默认为当前时间戳，<strong>一般不写</strong></li>
<li><strong>注意：发布时爬虫是未运行状态，且爬虫目录下不要放无关的py文件，否则可能会导致发布失败</strong></li>
</ul>
</li>
<li><p><strong>运行爬虫：<code>curl http://服务器ip:6800/schedule.json -d project=项目名 -d spider=脚本名</code></strong></p>
</li>
<li><p><strong>停止爬虫：<code>curl http://服务器ip:6800/cancel.json -d project=项目名 -d job=任务的id值</code></strong></p>
</li>
</ul>
<h3 id="2-项目部署-scrapydweb"><a href="#2-项目部署-scrapydweb" class="headerlink" title="2. 项目部署_scrapydweb"></a>2. 项目部署_scrapydweb</h3><blockquote>
<p><strong>注意：scrapydweb的兼容问题很大，需要创建 requirements.txt 文件统一控制包的版本，通过 <code>pip install -r requirements.txt</code> 安装所有兼容版本的包</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pip&gt;=19.1.1</span><br><span class="line">APScheduler==3.5.3</span><br><span class="line">click==7.0</span><br><span class="line">colorama==0.4.0</span><br><span class="line">Flask==1.0.2</span><br><span class="line">Flask-Compress==1.4.0</span><br><span class="line">Flask-SQLAlchemy==2.4.0</span><br><span class="line">idna==2.7</span><br><span class="line">itsdangerous==1.1.0</span><br><span class="line">Jinja2==2.10</span><br><span class="line">logparser==0.8.2</span><br><span class="line">MarkupSafe==1.1.1</span><br><span class="line">pexpect==4.7.0</span><br><span class="line">ptyprocess==0.6.0</span><br><span class="line">pytz==2018.9</span><br><span class="line">requests&gt;=2.21.0</span><br><span class="line">setuptools&gt;=40.6.3</span><br><span class="line">six==1.12.0</span><br><span class="line">SQLAlchemy==1.3.24</span><br><span class="line">tzlocal==1.5.1</span><br><span class="line">w3lib==1.19.0</span><br><span class="line">Werkzeug==0.14.1</span><br></pre></td></tr></table></figure>
</blockquote>
<ul>
<li><p><strong>【服务器端】安装scrapydweb：<code>pip install scrapydweb</code></strong></p>
</li>
<li><p><strong>开启 scrapydweb 服务（必须先开启 scrapyd 服务）：<code>scrapydweb</code></strong></p>
<ul>
<li>scrapydweb 服务可以在任意路径下开启</li>
<li>scrapydweb 服务第一次运行不会成功，只会创建一个文件，第二次运行即可成功</li>
</ul>
</li>
<li><p><strong>基本使用：</strong></p>
<ol>
<li>通过 5000 端口访问 scrapydweb 服务</li>
<li>在 Deploy Project –&gt; Upload file 中上传压缩后的项目</li>
<li>在 Run Spider 中运行爬虫</li>
<li>在 Jobs 中可以进行爬虫的暂停和启动</li>
</ol>
</li>
</ul>
<h3 id="3-项目部署-Gerapy"><a href="#3-项目部署-Gerapy" class="headerlink" title="3. 项目部署_Gerapy"></a>3. 项目部署_Gerapy</h3><blockquote>
<p><strong>注意：Gerapy 是一个基于Web的客户端，安装在客户端，用于管理和监控分布式爬虫，基于 django 开发</strong></p>
<p><strong>安装：<code>pip install gerapy==0.9.12</code>（0.9.12是稳定版本）</strong></p>
</blockquote>
<ul>
<li><p><strong>Gerapy 基本使用：</strong></p>
<ol>
<li><strong><code>gerapy init</code>：</strong>指定一个单独用于部署 Gerapy 的文件夹，通过此命令创建一个 Gerapy 工作目录</li>
<li><strong><code>gerapy migrate</code>：</strong>执行数据库迁移操作，将数据库模型的变更应用到数据库中</li>
<li><strong><code>gerapy createsuperuser</code>：</strong>创建一个具有系统中最高级别的权限的超级用户</li>
<li><strong><code>gerapy runserver</code>：</strong>在开发环境中启动 Gerapy 的 Web 服务器</li>
</ol>
</li>
<li><p><strong>Gerapy 的 Web 服务器基本使用：</strong></p>
<ol>
<li><strong>主机管理：</strong>用于连接 scrapyd 服务，所以需要确保 ubuntu 中的 scrapyd 服务是开启的（名称：ubuntu-1，ip：ubuntu的ip地址，端口：6800，认证：scrapyd也有账号密码则需要认证）</li>
<li><strong>项目管理：</strong>用于上传 scrapy 项目，上传 –&gt; 部署 –&gt; 打包项目 –&gt; 选取节点 –&gt; 点击部署 –&gt; 主机管理</li>
<li><strong>任务管理：</strong>用于开启不同爬虫任务（Date：指定时间运行爬虫，Interval：间隔指定时间后 - 重复调用爬虫，Crontab：定时爬虫）</li>
</ol>
</li>
</ul>

                    
                </div>

                
                        
<div class="post-copyright-info-container border-box">
    <div class="copyright-info-content border-box">
        <div class="copyright-info-top border-box">
            <div class="copyright-post-title border-box text-ellipsis">
                爬虫_2_Scrapy框架
            </div>

            <div class="copyright-post-link border-box text-ellipsis">
                2023/10/01/爬虫_2_Scrapy框架/
            </div>
        </div>

        <div class="copyright-info-bottom border-box">
            <div class="copyright-post-author bottom-item">
                <div class="type">
                    作者
                </div>
                <div class="content">xfblog</div>
            </div>

            <div class="post-time bottom-item">
                <div class="type">
                    发布于
                </div>
                <div class="content">2023-10-01 00:00</div>
            </div>


            <div class="post-license bottom-item">
                <div class="type">
                    许可
                </div>
                <div class="content tooltip" data-tooltip-content="CC BY-NC-SA 4.0">
                    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans" target="_blank">
                        
                            <i class="fa-brands fa-creative-commons"></i>
                            <i class="fa-brands fa-creative-commons-by"></i>
                            <i class="fa-brands fa-creative-commons-nc"></i>
                            <i class="fa-brands fa-creative-commons-sa"></i>
                        
                    </a>
                </div>
            </div>
        </div>

        <i class="copyright-bg fa-solid fa-copyright"></i>
    </div>
    <div class="copy-copyright-info flex-center tooltip" data-tooltip-content="复制版权信息" data-tooltip-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Python/">Python</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                            <div class="post-share-container border-box">
    <ul class="share-list-wrap border-box">
        <li class="qq share-item border-box flex-center tooltip"
            data-tooltip-content="分享到 QQ"
        >
            <i class="fa-brands fa-qq"></i>
        </li>
        <li class="wechat share-item border-box flex-center tooltip tooltip-img"
            data-tooltip-content="分享到微信"
            data-tooltip-img-tip="微信扫一扫"
            data-tooltip-img-style="background-color: #fff; top: -10px; padding: 0.6rem 0.6rem 0.1rem 0.6rem;"
        >
            <i class="fa-brands fa-weixin"></i>
        </li>
        <li class="weibo share-item border-box flex-center tooltip"
            data-tooltip-content="分享到微博"
        >
            <i class="fa-brands fa-weibo"></i>
        </li>
    </ul>
</div>

                        
                    </div>
                </div>

                
                    

<div class="reward-author-container border-box flex-center">
    <div class="reward-btn border-box flex-center tooltip tooltip-img"
            data-tooltip-img-url="/images/zfb.webp"
            data-tooltip-img-trigger="click"
            data-tooltip-img-style="top: -6px;"
    >
        <i class="fa-solid fa-gift"></i>&nbsp;创作不易，打赏随意，您的支持是我更新的动力💪！
    </div>
</div>

                

                
                    <div class="post-nav border-box">
                        
                            <div class="prev-post">
                                <a class="prev"
                                   rel="prev"
                                   href="/2023/11/01/%E7%88%AC%E8%99%AB_3_Ubuntu%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"
                                   title="爬虫_3_Ubuntu安装配置"
                                >
                                    <span class="left arrow-icon flex-center">
                                        <i class="fas fa-chevron-left"></i>
                                    </span>
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">爬虫_3_Ubuntu安装配置</span>
                                        <span class="post-nav-item">上一篇</span>
                                    </span>
                                </a>
                            </div>
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/2023/09/01/%E7%88%AC%E8%99%AB_1_%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"
                                   title="爬虫_1_基础知识"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">爬虫_1_基础知识</span>
                                        <span class="post-nav-item">下一篇</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    






                
            </div>
        </div>

        
            <div class="pc-post-toc right-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BA%86%E8%A7%A3Scrapy%E6%A1%86%E6%9E%B6"><span class="nav-text">一、了解Scrapy框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">1. scrapy的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-scrapy%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%88%9D%E8%AF%86"><span class="nav-text">2. scrapy的项目初识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-scrapy%E7%9A%84%E6%96%AD%E7%82%B9%E7%BB%AD%E7%88%AC"><span class="nav-text">3. scrapy的断点续爬</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-scrapy%E7%9A%84%E9%81%8D%E5%8E%86%E7%AE%97%E6%B3%95"><span class="nav-text">4. scrapy的遍历算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-scrapy%E7%9A%84%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB"><span class="nav-text">5. scrapy的日志级别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Spider%E7%88%AC%E8%99%AB%E6%A8%A1%E5%9D%97"><span class="nav-text">二、Spider爬虫模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E3%80%81%E8%B7%9F%E8%BF%9B%E9%93%BE%E6%8E%A5"><span class="nav-text">2. 数据提取、跟进链接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%AF%B7%E6%B1%82post%E6%90%BA%E5%B8%A6%E8%A1%A8%E5%8D%95"><span class="nav-text">3. 请求post携带表单</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%AF%B7%E6%B1%82post%E6%90%BA%E5%B8%A6json"><span class="nav-text">4. 请求post携带json</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Middleware%E5%92%8CExtension"><span class="nav-text">三、Middleware和Extension</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0-1"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%8C%87%E7%BA%B9%E5%8E%BB%E9%87%8D%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">2. 指纹去重中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86ip%E6%89%A9%E5%B1%95"><span class="nav-text">2. 付费代理ip扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">3. 付费代理中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%B8%AD%E9%97%B4%E4%BB%B6selenium%E9%85%8D%E7%BD%AE"><span class="nav-text">4. 中间件selenium配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Item%E5%AE%9E%E4%BD%93%E6%A8%A1%E5%9D%97"><span class="nav-text">四、Item实体模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-text">1. 重要知识点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81Pipline%E7%AE%A1%E9%81%93%E6%A8%A1%E5%9D%97"><span class="nav-text">五、Pipline管道模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0-2"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%A4%9A%E4%B8%AA%E7%AE%A1%E9%81%93item%E4%BC%A0%E9%80%92"><span class="nav-text">2. 多个管道item传递</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%BB%E9%87%8D"><span class="nav-text">3. 数据存储去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%AF%B7%E6%B1%82url%E5%9C%B0%E5%9D%80%E5%8E%BB%E9%87%8D"><span class="nav-text">4. 请求url地址去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-adbapi%E5%BC%82%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8E%A5%E5%8F%A3"><span class="nav-text">5. adbapi异步数据库接口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81Settings%E9%85%8D%E7%BD%AE%E6%A8%A1%E5%9D%97"><span class="nav-text">六、Settings配置模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%85%8D%E7%BD%AE%E4%B8%8B%E8%BD%BD%E5%BB%B6%E8%BF%9F"><span class="nav-text">1. 配置下载延迟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%A8%E9%83%A8%E9%85%8D%E7%BD%AE"><span class="nav-text">2. 全部配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81cnblogs%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98"><span class="nav-text">七、cnblogs项目实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-spiders-cnblogs-py"><span class="nav-text">1. spiders&#x2F;cnblogs.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-extend-ip-py"><span class="nav-text">2. extend_ip.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-middlewares-py"><span class="nav-text">3. middlewares.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-items-py"><span class="nav-text">4. items.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-piplines-py"><span class="nav-text">5. piplines.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-settings-py"><span class="nav-text">6. settings.py</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81Scrapy-Redis%E5%A2%9E%E9%87%8F%E7%88%AC%E8%99%AB"><span class="nav-text">八、Scrapy_Redis增量爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81Scrapy-Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-text">九、Scrapy_Redis分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy-redis%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-text">1. scrapy_redis工作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-RedisPipeline%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">2. RedisPipeline的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9E%E7%8E%B0GET%E8%AF%B7%E6%B1%82"><span class="nav-text">3. 实现GET请求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AE%9E%E7%8E%B0POST%E8%AF%B7%E6%B1%82"><span class="nav-text">4. 实现POST请求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E3%80%81scrapy%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2"><span class="nav-text">十、scrapy项目部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-scrapyd"><span class="nav-text">1. 项目部署_scrapyd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-scrapydweb"><span class="nav-text">2. 项目部署_scrapydweb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-Gerapy"><span class="nav-text">3. 项目部署_Gerapy</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>
        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="copyright-info info-item">
    Copyright&nbsp;&copy;&nbsp;<span>2021</span>&nbsp;-&nbsp;2025&nbsp;<a href="https://xfblog.cn/" target="_self">小傅博客</a>&nbsp;-&nbsp;All&nbsp;rights&nbsp;reserved.&nbsp;&nbsp;&nbsp;&nbsp;
    
            <!-- &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">xfblog</a> -->
        
    </div>

    <!-- <div class="theme-info info-item">
        由&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;驱动&nbsp;&&nbsp;主题&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
    </div> -->

    

    

    
        <div class="record-info info-item">
            
                
                    <div class="record-item border-box">
                        <a class=""
                           target="_blank"
                           href="https://beian.miit.gov.cn"
                        >
                        
                            蜀ICP备2021021033号-1
                        </a>
                    </div>
                
            
                
                    <div class="record-item border-box">
                        <a class=""
                           target="_blank"
                           href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=51138102000134"
                        >
                        
                            <!-- 在第二个 <a> 标签前插入图片 -->
                            <img src="http://xfblog.cn/images/icon.webp" style="vertical-align: top;">
                        
                            川公网安备 51138102000134号
                        </a>
                    </div>
                
            
        </div>&nbsp;&nbsp;&nbsp;&nbsp;
        
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="post-tools-list border-box">
        <!-- PC encrypt again -->
        

        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        

        <!-- PC full screen -->
        <li class="tools-item flex-center full-screen">
            <i class="fa-solid fa-expand"></i>
        </li>
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <!-- toggle mode -->
        
            <li class="tools-item tool-toggle-theme-mode flex-center">
                <i class="fas fa-moon"></i>
            </li>
        

        <!-- rss -->
        
            <li class="tools-item rss flex-center">
                <a class="flex-center"
                   href="/atom.xml"
                   target="_blank"
                >
                    <i class="fas fa-rss"></i>
                </a>
            </li>
        

        <!-- to bottom -->
        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%BA%86%E8%A7%A3Scrapy%E6%A1%86%E6%9E%B6"><span class="nav-text">一、了解Scrapy框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">1. scrapy的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-scrapy%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%88%9D%E8%AF%86"><span class="nav-text">2. scrapy的项目初识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-scrapy%E7%9A%84%E6%96%AD%E7%82%B9%E7%BB%AD%E7%88%AC"><span class="nav-text">3. scrapy的断点续爬</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-scrapy%E7%9A%84%E9%81%8D%E5%8E%86%E7%AE%97%E6%B3%95"><span class="nav-text">4. scrapy的遍历算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-scrapy%E7%9A%84%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB"><span class="nav-text">5. scrapy的日志级别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Spider%E7%88%AC%E8%99%AB%E6%A8%A1%E5%9D%97"><span class="nav-text">二、Spider爬虫模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96%E3%80%81%E8%B7%9F%E8%BF%9B%E9%93%BE%E6%8E%A5"><span class="nav-text">2. 数据提取、跟进链接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E8%AF%B7%E6%B1%82post%E6%90%BA%E5%B8%A6%E8%A1%A8%E5%8D%95"><span class="nav-text">3. 请求post携带表单</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%AF%B7%E6%B1%82post%E6%90%BA%E5%B8%A6json"><span class="nav-text">4. 请求post携带json</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Middleware%E5%92%8CExtension"><span class="nav-text">三、Middleware和Extension</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0-1"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%8C%87%E7%BA%B9%E5%8E%BB%E9%87%8D%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">2. 指纹去重中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86ip%E6%89%A9%E5%B1%95"><span class="nav-text">2. 付费代理ip扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E4%BB%98%E8%B4%B9%E4%BB%A3%E7%90%86%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="nav-text">3. 付费代理中间件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%B8%AD%E9%97%B4%E4%BB%B6selenium%E9%85%8D%E7%BD%AE"><span class="nav-text">4. 中间件selenium配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Item%E5%AE%9E%E4%BD%93%E6%A8%A1%E5%9D%97"><span class="nav-text">四、Item实体模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-text">1. 重要知识点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81Pipline%E7%AE%A1%E9%81%93%E6%A8%A1%E5%9D%97"><span class="nav-text">五、Pipline管道模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%87%8D%E8%A6%81%E5%87%BD%E6%95%B0-2"><span class="nav-text">1. 重要函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%A4%9A%E4%B8%AA%E7%AE%A1%E9%81%93item%E4%BC%A0%E9%80%92"><span class="nav-text">2. 多个管道item传递</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%BB%E9%87%8D"><span class="nav-text">3. 数据存储去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E8%AF%B7%E6%B1%82url%E5%9C%B0%E5%9D%80%E5%8E%BB%E9%87%8D"><span class="nav-text">4. 请求url地址去重</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-adbapi%E5%BC%82%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8E%A5%E5%8F%A3"><span class="nav-text">5. adbapi异步数据库接口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81Settings%E9%85%8D%E7%BD%AE%E6%A8%A1%E5%9D%97"><span class="nav-text">六、Settings配置模块</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%85%8D%E7%BD%AE%E4%B8%8B%E8%BD%BD%E5%BB%B6%E8%BF%9F"><span class="nav-text">1. 配置下载延迟</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%A8%E9%83%A8%E9%85%8D%E7%BD%AE"><span class="nav-text">2. 全部配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81cnblogs%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98"><span class="nav-text">七、cnblogs项目实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-spiders-cnblogs-py"><span class="nav-text">1. spiders&#x2F;cnblogs.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-extend-ip-py"><span class="nav-text">2. extend_ip.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-middlewares-py"><span class="nav-text">3. middlewares.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-items-py"><span class="nav-text">4. items.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-piplines-py"><span class="nav-text">5. piplines.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-settings-py"><span class="nav-text">6. settings.py</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81Scrapy-Redis%E5%A2%9E%E9%87%8F%E7%88%AC%E8%99%AB"><span class="nav-text">八、Scrapy_Redis增量爬虫</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81Scrapy-Redis%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="nav-text">九、Scrapy_Redis分布式爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-scrapy-redis%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-text">1. scrapy_redis工作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-RedisPipeline%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">2. RedisPipeline的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%AE%9E%E7%8E%B0GET%E8%AF%B7%E6%B1%82"><span class="nav-text">3. 实现GET请求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AE%9E%E7%8E%B0POST%E8%AF%B7%E6%B1%82"><span class="nav-text">4. 实现POST请求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E3%80%81scrapy%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2"><span class="nav-text">十、scrapy项目部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-scrapyd"><span class="nav-text">1. 项目部署_scrapyd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-scrapydweb"><span class="nav-text">2. 项目部署_scrapydweb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2-Gerapy"><span class="nav-text">3. 项目部署_Gerapy</span></a></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>





<!-- common js -->
<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/toggle-theme.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/libs/anime.min.js"></script>

<!-- local search -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/local-search.js"></script>


<!-- lazyload -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/lazyload.js"></script>


<div class="pjax">
    <!-- home page -->
    

    <!-- post page -->
    
        <!-- post-helper -->
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/post-helper.js"></script>

        <!-- toc -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/toc.js"></script>
        

        <!-- copyright-info -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/copyright-info.js"></script>
        

        <!-- share -->
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/post/share.js"></script>
        
    

    <!-- categories page -->
    

    <!-- links page -->
    

    <!-- photos page -->
    

    <!-- tools page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->

    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@4.2.5/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart()
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd()
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'))
            KEEP.initExecute()
        });
    });
</script>




    
        
            
<script class="custom-inject-js" src="/js/run_time.js" data-pjax></script>

        
    
        
            
<script class="custom-inject-js" src="/js/code_click_copy.js" data-pjax></script>

        
    
        
            
<script class="custom-inject-js" src="/js/copyrightpro.js" data-pjax></script>

        
    

</body>
</html>
